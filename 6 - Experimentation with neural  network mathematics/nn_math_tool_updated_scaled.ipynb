{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, experimentation tool, version 2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This version uses min/max-scaling also for the imaginary x1/x2/y -data. It seems our model works much better now!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just copy/paste -the needed activation functions, \n",
    "# we're going to need these again\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness\n",
    "# np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        # add the row to result (y is target)\n",
    "        # basically the order of variables: x1, x2, y \n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override above data with our generation function\n",
    "data = generate_train_data()\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.240000</td>\n",
       "      <td>4.620000</td>\n",
       "      <td>13.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.450322</td>\n",
       "      <td>1.178768</td>\n",
       "      <td>6.744216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2           y\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     2.240000    4.620000   13.520000\n",
       "std      1.450322    1.178768    6.744216\n",
       "min      0.000000    3.000000    3.000000\n",
       "25%      1.000000    4.000000    7.000000\n",
       "50%      2.000000    5.000000   13.000000\n",
       "75%      4.000000    6.000000   20.250000\n",
       "max      4.000000    6.000000   26.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The neural network training code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 0.12321003728725537\n",
      "Epoch: 2, loss 0.07399774292767143\n",
      "Epoch: 3, loss 0.057584760213442845\n",
      "Epoch: 4, loss 0.048641620015049424\n",
      "Epoch: 5, loss 0.04644034075813614\n",
      "Epoch: 6, loss 0.0460039445389316\n",
      "Epoch: 7, loss 0.04563328793047945\n",
      "Epoch: 8, loss 0.0452308597328483\n",
      "Epoch: 9, loss 0.04480377228842645\n",
      "Epoch: 10, loss 0.044436126432241645\n",
      "Epoch: 11, loss 0.043871119315068235\n",
      "Epoch: 12, loss 0.04296847477132355\n",
      "Epoch: 13, loss 0.0430451979044455\n",
      "Epoch: 14, loss 0.04217960885583978\n",
      "Epoch: 15, loss 0.04224068579733181\n",
      "Epoch: 16, loss 0.04186154562608783\n",
      "Epoch: 17, loss 0.04098714123717291\n",
      "Epoch: 18, loss 0.041025624582447504\n",
      "Epoch: 19, loss 0.0401782475166654\n",
      "Epoch: 20, loss 0.04020104974202396\n",
      "Epoch: 21, loss 0.039809561418360576\n",
      "Epoch: 22, loss 0.03896060053660754\n",
      "Epoch: 23, loss 0.03896202992258924\n",
      "Epoch: 24, loss 0.03813972596929955\n",
      "Epoch: 25, loss 0.038126177078678956\n",
      "Epoch: 26, loss 0.03772717707638625\n",
      "Epoch: 27, loss 0.036910191186081576\n",
      "Epoch: 28, loss 0.03687682846013818\n",
      "Epoch: 29, loss 0.03647575998556977\n",
      "Epoch: 30, loss 0.03567981772383682\n",
      "Epoch: 31, loss 0.035628031517279495\n",
      "Epoch: 32, loss 0.035226733736792866\n",
      "Epoch: 33, loss 0.03445404215872994\n",
      "Epoch: 34, loss 0.03438490264693812\n",
      "Epoch: 35, loss 0.03398518287228139\n",
      "Epoch: 36, loss 0.03323779942494713\n",
      "Epoch: 37, loss 0.033152467174156985\n",
      "Epoch: 38, loss 0.03275608552448301\n",
      "Epoch: 39, loss 0.03203588625274587\n",
      "Epoch: 40, loss 0.03193559260692231\n",
      "Epoch: 41, loss 0.03154423460582731\n",
      "Epoch: 42, loss 0.030852886765326815\n",
      "Epoch: 43, loss 0.03073891256370367\n",
      "Epoch: 44, loss 0.03035416423317026\n",
      "Epoch: 45, loss 0.029693104450222725\n",
      "Epoch: 46, loss 0.02956675795872505\n",
      "Epoch: 47, loss 0.02919008414296647\n",
      "Epoch: 48, loss 0.028812172196283015\n",
      "Epoch: 49, loss 0.02819179503589524\n",
      "Epoch: 50, loss 0.028052357257180815\n",
      "Epoch: 51, loss 0.02768847945445902\n",
      "Epoch: 52, loss 0.027324732529910134\n",
      "Epoch: 53, loss 0.02696474442919367\n",
      "Epoch: 54, loss 0.026396710666680714\n",
      "Epoch: 55, loss 0.026244739997658858\n",
      "Epoch: 56, loss 0.02589959080480487\n",
      "Epoch: 57, loss 0.025555945920645067\n",
      "Epoch: 58, loss 0.025216537745167116\n",
      "Epoch: 59, loss 0.02488147552337438\n",
      "Epoch: 60, loss 0.0243777008843979\n",
      "Epoch: 61, loss 0.024215057732195417\n",
      "Epoch: 62, loss 0.023895385826195967\n",
      "Epoch: 63, loss 0.02357842843500436\n",
      "Epoch: 64, loss 0.023266080651234745\n",
      "Epoch: 65, loss 0.02295840310118653\n",
      "Epoch: 66, loss 0.02261744690422176\n",
      "Epoch: 67, loss 0.022350069480768654\n",
      "Epoch: 68, loss 0.02205696145419225\n",
      "Epoch: 69, loss 0.021768327888807178\n",
      "Epoch: 70, loss 0.02148448135065441\n",
      "Epoch: 71, loss 0.02120543350168125\n",
      "Epoch: 72, loss 0.020931188449324022\n",
      "Epoch: 73, loss 0.02066174573151046\n",
      "Epoch: 74, loss 0.020397100501132977\n",
      "Epoch: 75, loss 0.020137243688389636\n",
      "Epoch: 76, loss 0.019882162166437834\n",
      "Epoch: 77, loss 0.019631838919868742\n",
      "Epoch: 78, loss 0.019386253215297456\n",
      "Epoch: 79, loss 0.019145380773389505\n",
      "Epoch: 80, loss 0.01890919394167607\n",
      "Epoch: 81, loss 0.01867766186754056\n",
      "Epoch: 82, loss 0.01845075067079542\n",
      "Epoch: 83, loss 0.018228423615301907\n",
      "Epoch: 84, loss 0.01801064127912177\n",
      "Epoch: 85, loss 0.017797361722726376\n",
      "Epoch: 86, loss 0.017588540654826998\n",
      "Epoch: 87, loss 0.017384131595423007\n",
      "Epoch: 88, loss 0.017196273658733323\n",
      "Epoch: 89, loss 0.01700174521374907\n",
      "Epoch: 90, loss 0.0168110706332619\n",
      "Epoch: 91, loss 0.0166242774522966\n",
      "Epoch: 92, loss 0.01644132237819352\n",
      "Epoch: 93, loss 0.016262160127476295\n",
      "Epoch: 94, loss 0.016086744014383036\n",
      "Epoch: 95, loss 0.015915026092320168\n",
      "Epoch: 96, loss 0.01574695728663604\n",
      "Epoch: 97, loss 0.015582487521532181\n",
      "Epoch: 98, loss 0.015421565841184993\n",
      "Epoch: 99, loss 0.015264140525150672\n",
      "Epoch: 100, loss 0.015110159198140663\n",
      "Epoch: 101, loss 0.014962701672286725\n",
      "Epoch: 102, loss 0.014817609098727322\n",
      "Epoch: 103, loss 0.014675626581185043\n",
      "Epoch: 104, loss 0.014536710338369445\n",
      "Epoch: 105, loss 0.014400811913332897\n",
      "Epoch: 106, loss 0.014267882531935255\n",
      "Epoch: 107, loss 0.014137873212899714\n",
      "Epoch: 108, loss 0.014010734848735622\n",
      "Epoch: 109, loss 0.013883227017367\n",
      "Epoch: 110, loss 0.013759082867975659\n",
      "Epoch: 111, loss 0.013637589974199664\n",
      "Epoch: 112, loss 0.013518700007067766\n",
      "Epoch: 113, loss 0.013402368073334643\n",
      "Epoch: 114, loss 0.01328854945112373\n",
      "Epoch: 115, loss 0.013177199616651435\n",
      "Epoch: 116, loss 0.013068274283816978\n",
      "Epoch: 117, loss 0.012961729440665731\n",
      "Epoch: 118, loss 0.012857521382852536\n",
      "Epoch: 119, loss 0.012755606744297805\n",
      "Epoch: 120, loss 0.012655942525213654\n",
      "Epoch: 121, loss 0.012558486117666193\n",
      "Epoch: 122, loss 0.012463195328827423\n",
      "Epoch: 123, loss 0.01237002840206071\n",
      "Epoch: 124, loss 0.012229097393396421\n",
      "Epoch: 125, loss 0.012131992967548313\n",
      "Epoch: 126, loss 0.0119145416418788\n",
      "Epoch: 127, loss 0.011733408777449337\n",
      "Epoch: 128, loss 0.01156732934164109\n",
      "Epoch: 129, loss 0.011364158748355092\n",
      "Epoch: 130, loss 0.01115430543906992\n",
      "Epoch: 131, loss 0.010968291406967394\n",
      "Epoch: 132, loss 0.01079162060722962\n",
      "Epoch: 133, loss 0.010921600539411947\n",
      "Epoch: 134, loss 0.0111503898343108\n",
      "Epoch: 135, loss 0.011203441221207828\n",
      "Epoch: 136, loss 0.011255870673577115\n",
      "Epoch: 137, loss 0.011309695928596177\n",
      "Epoch: 138, loss 0.01136443027483976\n",
      "Epoch: 139, loss 0.011419692723534946\n",
      "Epoch: 140, loss 0.01147519579195067\n",
      "Epoch: 141, loss 0.011530722744969916\n",
      "Epoch: 142, loss 0.01158611029797966\n",
      "Epoch: 143, loss 0.011641235541001478\n",
      "Epoch: 144, loss 0.011696006057074685\n",
      "Epoch: 145, loss 0.011750352457370631\n",
      "Epoch: 146, loss 0.01180422274445675\n",
      "Epoch: 147, loss 0.011857578058034777\n",
      "Epoch: 148, loss 0.01191038946567831\n",
      "Epoch: 149, loss 0.011962635543075065\n",
      "Epoch: 150, loss 0.012014300550409519\n",
      "Epoch: 151, loss 0.012065373058606381\n",
      "Epoch: 152, loss 0.012009306146919444\n",
      "Epoch: 153, loss 0.011878424928957352\n",
      "Epoch: 154, loss 0.01179274931214284\n",
      "Epoch: 155, loss 0.011740458814199106\n",
      "Epoch: 156, loss 0.011615524039151509\n",
      "Epoch: 157, loss 0.01152212999034727\n",
      "Epoch: 158, loss 0.011464078193872728\n",
      "Epoch: 159, loss 0.011431447289130921\n",
      "Epoch: 160, loss 0.011417040243778364\n",
      "Epoch: 161, loss 0.011415674306584966\n",
      "Epoch: 162, loss 0.011423609566869908\n",
      "Epoch: 163, loss 0.011438143543684774\n",
      "Epoch: 164, loss 0.011457321844622528\n",
      "Epoch: 165, loss 0.011479730593744221\n",
      "Epoch: 166, loss 0.011504346965439699\n",
      "Epoch: 167, loss 0.011530431278070245\n",
      "Epoch: 168, loss 0.011557448954181303\n",
      "Epoch: 169, loss 0.011405248931916101\n",
      "Epoch: 170, loss 0.011302902447353099\n",
      "Epoch: 171, loss 0.01123758802494213\n",
      "Epoch: 172, loss 0.01119842647577226\n",
      "Epoch: 173, loss 0.011177728455488147\n",
      "Epoch: 174, loss 0.011170048340996884\n",
      "Epoch: 175, loss 0.011171520770169014\n",
      "Epoch: 176, loss 0.011179398042525766\n",
      "Epoch: 177, loss 0.011191725283955664\n",
      "Epoch: 178, loss 0.01120711115613066\n",
      "Epoch: 179, loss 0.011224565408034222\n",
      "Epoch: 180, loss 0.011243383489191697\n",
      "Epoch: 181, loss 0.011263064450070623\n",
      "Epoch: 182, loss 0.011283252462651187\n",
      "Epoch: 183, loss 0.011303695138402903\n",
      "Epoch: 184, loss 0.011324213808946658\n",
      "Epoch: 185, loss 0.011344682333892431\n",
      "Epoch: 186, loss 0.011365011990044247\n",
      "Epoch: 187, loss 0.011385140698652832\n",
      "Epoch: 188, loss 0.011405025347219842\n",
      "Epoch: 189, loss 0.01142463631854214\n",
      "Epoch: 190, loss 0.011443953593764183\n",
      "Epoch: 191, loss 0.011462963977552457\n",
      "Epoch: 192, loss 0.01167843925732856\n",
      "Epoch: 193, loss 0.01190144044596775\n",
      "Epoch: 194, loss 0.012064879585690675\n",
      "Epoch: 195, loss 0.012185625018923263\n",
      "Epoch: 196, loss 0.012276108794260688\n",
      "Epoch: 197, loss 0.012345179614224738\n",
      "Epoch: 198, loss 0.012399102835173436\n",
      "Epoch: 199, loss 0.012442298966995146\n",
      "Epoch: 200, loss 0.012346024129468827\n",
      "Epoch: 201, loss 0.012284044469105472\n",
      "Epoch: 202, loss 0.012246221539785239\n",
      "Epoch: 203, loss 0.01222493108624058\n",
      "Epoch: 204, loss 0.012214936206411927\n",
      "Epoch: 205, loss 0.012212635710567696\n",
      "Epoch: 206, loss 0.012215550554566567\n",
      "Epoch: 207, loss 0.012221973252691587\n",
      "Epoch: 208, loss 0.012230727596585307\n",
      "Epoch: 209, loss 0.012241003527904644\n",
      "Epoch: 210, loss 0.012110909959051126\n",
      "Epoch: 211, loss 0.012023947027721076\n",
      "Epoch: 212, loss 0.012107413086782495\n",
      "Epoch: 213, loss 0.012031583498627497\n",
      "Epoch: 214, loss 0.011981246832370165\n",
      "Epoch: 215, loss 0.01195045310390469\n",
      "Epoch: 216, loss 0.01193286881803018\n",
      "Epoch: 217, loss 0.011924190079699637\n",
      "Epoch: 218, loss 0.011921495906251604\n",
      "Epoch: 219, loss 0.011922802288873892\n",
      "Epoch: 220, loss 0.011926761155056831\n",
      "Epoch: 221, loss 0.011932456534842751\n",
      "Epoch: 222, loss 0.011939266253133676\n",
      "Epoch: 223, loss 0.011946767957898424\n",
      "Epoch: 224, loss 0.011954675220578718\n",
      "Epoch: 225, loss 0.011962794066159507\n",
      "Epoch: 226, loss 0.011970993395683008\n",
      "Epoch: 227, loss 0.011979184861008628\n",
      "Epoch: 228, loss 0.011987309172454073\n",
      "Epoch: 229, loss 0.011995326784733319\n",
      "Epoch: 230, loss 0.01200321156261974\n",
      "Epoch: 231, loss 0.012010946474198737\n",
      "Epoch: 232, loss 0.012018520663520249\n",
      "Epoch: 233, loss 0.012025927461438228\n",
      "Epoch: 234, loss 0.012033163034371686\n",
      "Epoch: 235, loss 0.012040225466690651\n",
      "Epoch: 236, loss 0.012047114137767536\n",
      "Epoch: 237, loss 0.012053829299201815\n",
      "Epoch: 238, loss 0.012060371787984747\n",
      "Epoch: 239, loss 0.012066742831953725\n",
      "Epoch: 240, loss 0.012072943917881437\n",
      "Epoch: 241, loss 0.012078976702061915\n",
      "Epoch: 242, loss 0.012286406296841771\n",
      "Epoch: 243, loss 0.012475813317629168\n",
      "Epoch: 244, loss 0.012518657721337426\n",
      "Epoch: 245, loss 0.012520417719271055\n",
      "Epoch: 246, loss 0.012522693650831767\n",
      "Epoch: 247, loss 0.01252540707512659\n",
      "Epoch: 248, loss 0.012528365014105567\n",
      "Epoch: 249, loss 0.01253144068469056\n",
      "Epoch: 250, loss 0.012534551441742736\n",
      "Epoch: 251, loss 0.012719067210195969\n",
      "Epoch: 252, loss 0.012870323083853257\n",
      "Epoch: 253, loss 0.012970720422455704\n",
      "Epoch: 254, loss 0.01303771926731842\n",
      "Epoch: 255, loss 0.01308293658197927\n",
      "Epoch: 256, loss 0.013113962774963253\n",
      "Epoch: 257, loss 0.013135740702727734\n",
      "Epoch: 258, loss 0.013233469149601584\n",
      "Epoch: 259, loss 0.013301108714558372\n",
      "Epoch: 260, loss 0.013346698582744202\n",
      "Epoch: 261, loss 0.013378051778784261\n",
      "Epoch: 262, loss 0.013400219890424735\n",
      "Epoch: 263, loss 0.013416453019613054\n",
      "Epoch: 264, loss 0.013428839049733491\n",
      "Epoch: 265, loss 0.01343871784995242\n",
      "Epoch: 266, loss 0.013446948638689657\n",
      "Epoch: 267, loss 0.013454082144402895\n",
      "Epoch: 268, loss 0.01346047127669197\n",
      "Epoch: 269, loss 0.01346634218319504\n",
      "Epoch: 270, loss 0.01347183983018771\n",
      "Epoch: 271, loss 0.013477057219776753\n",
      "Epoch: 272, loss 0.013482054106832026\n",
      "Epoch: 273, loss 0.013486868983008557\n",
      "Epoch: 274, loss 0.01349152674619607\n",
      "Epoch: 275, loss 0.013496043606588176\n",
      "Epoch: 276, loss 0.013500430223724196\n",
      "Epoch: 277, loss 0.013504693711561058\n",
      "Epoch: 278, loss 0.013508838919524695\n",
      "Epoch: 279, loss 0.013512869250660657\n",
      "Epoch: 280, loss 0.013516787183946291\n",
      "Epoch: 281, loss 0.013520594607609309\n",
      "Epoch: 282, loss 0.013524293031755954\n",
      "Epoch: 283, loss 0.0135278837239564\n",
      "Epoch: 284, loss 0.013531367795668875\n",
      "Epoch: 285, loss 0.013534746257305455\n",
      "Epoch: 286, loss 0.013538020053303882\n",
      "Epoch: 287, loss 0.013541190084455045\n",
      "Epoch: 288, loss 0.013544257222111148\n",
      "Epoch: 289, loss 0.013547222317222904\n",
      "Epoch: 290, loss 0.013550086206084593\n",
      "Epoch: 291, loss 0.013552849713984327\n",
      "Epoch: 292, loss 0.013555513657521728\n",
      "Epoch: 293, loss 0.013558078846078868\n",
      "Epoch: 294, loss 0.013560546082752409\n",
      "Epoch: 295, loss 0.013562916164944757\n",
      "Epoch: 296, loss 0.013565341452297488\n",
      "Epoch: 297, loss 0.013568166400661335\n",
      "Epoch: 298, loss 0.013569892155689605\n",
      "Epoch: 299, loss 0.013570892301215776\n",
      "Epoch: 300, loss 0.013571407951509086\n",
      "Epoch: 301, loss 0.013571590546374279\n",
      "Epoch: 302, loss 0.01357153525401449\n",
      "Epoch: 303, loss 0.013571301966396585\n",
      "Epoch: 304, loss 0.01357092846913204\n",
      "Epoch: 305, loss 0.01357043869894237\n",
      "Epoch: 306, loss 0.013569847918983973\n",
      "Epoch: 307, loss 0.013569165961227238\n",
      "Epoch: 308, loss 0.013568399257089234\n",
      "Epoch: 309, loss 0.013567552108723449\n",
      "Epoch: 310, loss 0.01356662748464304\n",
      "Epoch: 311, loss 0.013658212694467585\n",
      "Epoch: 312, loss 0.01371585291235211\n",
      "Epoch: 313, loss 0.013844483526261955\n",
      "Epoch: 314, loss 0.01392699387657116\n",
      "Epoch: 315, loss 0.013978122941571273\n",
      "Epoch: 316, loss 0.014009869541485875\n",
      "Epoch: 317, loss 0.014029662700574182\n",
      "Epoch: 318, loss 0.014042080537370952\n",
      "Epoch: 319, loss 0.014049939268306404\n",
      "Epoch: 320, loss 0.014054969239769273\n",
      "Epoch: 321, loss 0.014058232778790043\n",
      "Epoch: 322, loss 0.014060381764539066\n",
      "Epoch: 323, loss 0.014061816115403587\n",
      "Epoch: 324, loss 0.014062781186831259\n",
      "Epoch: 325, loss 0.01406342757261721\n",
      "Epoch: 326, loss 0.014063847796520588\n",
      "Epoch: 327, loss 0.014064098811781344\n",
      "Epoch: 328, loss 0.01406421579109006\n",
      "Epoch: 329, loss 0.014064220574673169\n",
      "Epoch: 330, loss 0.014064126843633923\n",
      "Epoch: 331, loss 0.014063943286677927\n",
      "Epoch: 332, loss 0.014063675537791833\n",
      "Epoch: 333, loss 0.014063327361437323\n",
      "Epoch: 334, loss 0.014062901377216065\n",
      "Epoch: 335, loss 0.014039503212619979\n",
      "Epoch: 336, loss 0.014010944599769721\n",
      "Epoch: 337, loss 0.01399273037954106\n",
      "Epoch: 338, loss 0.013980801760747152\n",
      "Epoch: 339, loss 0.013972642702060183\n",
      "Epoch: 340, loss 0.013966735060717557\n",
      "Epoch: 341, loss 0.01396216303507149\n",
      "Epoch: 342, loss 0.013958374330344412\n",
      "Epoch: 343, loss 0.013955036041760986\n",
      "Epoch: 344, loss 0.01395194766223969\n",
      "Epoch: 345, loss 0.013948988567945233\n",
      "Epoch: 346, loss 0.013946086320022198\n",
      "Epoch: 347, loss 0.013943197534951672\n",
      "Epoch: 348, loss 0.013940296342943023\n",
      "Epoch: 349, loss 0.01393736742651058\n",
      "Epoch: 350, loss 0.01393440182295902\n",
      "Epoch: 351, loss 0.013931394394251138\n",
      "Epoch: 352, loss 0.013928342302440593\n",
      "Epoch: 353, loss 0.01392524409133959\n",
      "Epoch: 354, loss 0.013729937859223185\n",
      "Epoch: 355, loss 0.013738886181896696\n",
      "Epoch: 356, loss 0.013806502743031898\n",
      "Epoch: 357, loss 0.013653905948844284\n",
      "Epoch: 358, loss 0.013689174307051844\n",
      "Epoch: 359, loss 0.013580440231591509\n",
      "Epoch: 360, loss 0.013641786680849389\n",
      "Epoch: 361, loss 0.013547978257705248\n",
      "Epoch: 362, loss 0.013430389129401626\n",
      "Epoch: 363, loss 0.013545541635852996\n",
      "Epoch: 364, loss 0.01348427463301131\n",
      "Epoch: 365, loss 0.013386274137940793\n",
      "Epoch: 366, loss 0.013325654674485295\n",
      "Epoch: 367, loss 0.013287403807104607\n",
      "Epoch: 368, loss 0.013262379341639428\n",
      "Epoch: 369, loss 0.013245178583134938\n",
      "Epoch: 370, loss 0.013232605296510106\n",
      "Epoch: 371, loss 0.013222767303145005\n",
      "Epoch: 372, loss 0.01321454412199766\n",
      "Epoch: 373, loss 0.013207272232992338\n",
      "Epoch: 374, loss 0.01320055877968963\n",
      "Epoch: 375, loss 0.013194171231162065\n",
      "Epoch: 376, loss 0.013187972016168226\n",
      "Epoch: 377, loss 0.01318187979820244\n",
      "Epoch: 378, loss 0.013175846535577199\n",
      "Epoch: 379, loss 0.01316984389448102\n",
      "Epoch: 380, loss 0.013163855203321419\n",
      "Epoch: 381, loss 0.013157870689510692\n",
      "Epoch: 382, loss 0.01315188466023425\n",
      "Epoch: 383, loss 0.013145893834245708\n",
      "Epoch: 384, loss 0.013139896355012006\n",
      "Epoch: 385, loss 0.013133891207081054\n",
      "Epoch: 386, loss 0.013127877871012075\n",
      "Epoch: 387, loss 0.013121856119413852\n",
      "Epoch: 388, loss 0.013115825896423908\n",
      "Epoch: 389, loss 0.013109787246516012\n",
      "Epoch: 390, loss 0.01311803243810317\n",
      "Epoch: 391, loss 0.013224074898129351\n",
      "Epoch: 392, loss 0.013284144180279173\n",
      "Epoch: 393, loss 0.013316656148644574\n",
      "Epoch: 394, loss 0.013333001554728649\n",
      "Epoch: 395, loss 0.013339876954191983\n",
      "Epoch: 396, loss 0.013480932816945995\n",
      "Epoch: 397, loss 0.013563492461929007\n",
      "Epoch: 398, loss 0.013609106491188508\n",
      "Epoch: 399, loss 0.013633187662703036\n",
      "Epoch: 400, loss 0.01364474788669274\n",
      "Epoch: 401, loss 0.013649032338112748\n",
      "Epoch: 402, loss 0.013431253000063625\n",
      "Epoch: 403, loss 0.013515975427682797\n",
      "Epoch: 404, loss 0.013566626604006718\n",
      "Epoch: 405, loss 0.013376270010774627\n",
      "Epoch: 406, loss 0.01347636323226432\n",
      "Epoch: 407, loss 0.013319277128818606\n",
      "Epoch: 408, loss 0.013437928233666838\n",
      "Epoch: 409, loss 0.01329181895659067\n",
      "Epoch: 410, loss 0.013416638740279053\n",
      "Epoch: 411, loss 0.013274117288699785\n",
      "Epoch: 412, loss 0.013186091177674577\n",
      "Epoch: 413, loss 0.013346893617770084\n",
      "Epoch: 414, loss 0.013225757650614255\n",
      "Epoch: 415, loss 0.013149977392131447\n",
      "Epoch: 416, loss 0.013103657126027957\n",
      "Epoch: 417, loss 0.013074180648333917\n",
      "Epoch: 418, loss 0.013054334499470126\n",
      "Epoch: 419, loss 0.013039997539635318\n",
      "Epoch: 420, loss 0.013028813170909441\n",
      "Epoch: 421, loss 0.013019433258679922\n",
      "Epoch: 422, loss 0.01301108649388787\n",
      "Epoch: 423, loss 0.013003331620676786\n",
      "Epoch: 424, loss 0.0129959162529568\n",
      "Epoch: 425, loss 0.012988696076627348\n",
      "Epoch: 426, loss 0.01298158860731998\n",
      "Epoch: 427, loss 0.012974546727178413\n",
      "Epoch: 428, loss 0.012967543543106728\n",
      "Epoch: 429, loss 0.012960563724737159\n",
      "Epoch: 430, loss 0.01295359855035612\n",
      "Epoch: 431, loss 0.012946643074222497\n",
      "Epoch: 432, loss 0.012939694507287311\n",
      "Epoch: 433, loss 0.012932751291787219\n",
      "Epoch: 434, loss 0.012925812572522141\n",
      "Epoch: 435, loss 0.012918877894856212\n",
      "Epoch: 436, loss 0.012911947032268072\n",
      "Epoch: 437, loss 0.012905019887905546\n",
      "Epoch: 438, loss 0.012898096438403745\n",
      "Epoch: 439, loss 0.012891176701833842\n",
      "Epoch: 440, loss 0.012884260719425182\n",
      "Epoch: 441, loss 0.012877348545146597\n",
      "Epoch: 442, loss 0.012870440239772307\n",
      "Epoch: 443, loss 0.012863535867504439\n",
      "Epoch: 444, loss 0.012856635494054245\n",
      "Epoch: 445, loss 0.012849739185553986\n",
      "Epoch: 446, loss 0.012842847007942746\n",
      "Epoch: 447, loss 0.012835959026621921\n",
      "Epoch: 448, loss 0.012829075306263964\n",
      "Epoch: 449, loss 0.01282219591070943\n",
      "Epoch: 450, loss 0.012815320902912786\n",
      "Epoch: 451, loss 0.012808450344916776\n",
      "Epoch: 452, loss 0.012801584297842995\n",
      "Epoch: 453, loss 0.01279472282189156\n",
      "Epoch: 454, loss 0.01278786597634534\n",
      "Epoch: 455, loss 0.01278101381957756\n",
      "Epoch: 456, loss 0.012774166409060712\n",
      "Epoch: 457, loss 0.012767323801376277\n",
      "Epoch: 458, loss 0.012760486052225193\n",
      "Epoch: 459, loss 0.012753653216437927\n",
      "Epoch: 460, loss 0.012746825347985248\n",
      "Epoch: 461, loss 0.012740002499988343\n",
      "Epoch: 462, loss 0.01273318472472948\n",
      "Epoch: 463, loss 0.01272637207366193\n",
      "Epoch: 464, loss 0.012719564597420525\n",
      "Epoch: 465, loss 0.012712762345831527\n",
      "Epoch: 466, loss 0.01270596536792246\n",
      "Epoch: 467, loss 0.01269917371193223\n",
      "Epoch: 468, loss 0.012692387425320754\n",
      "Epoch: 469, loss 0.01268560655477828\n",
      "Epoch: 470, loss 0.0126788311462355\n",
      "Epoch: 471, loss 0.012672061244872421\n",
      "Epoch: 472, loss 0.012665296895127772\n",
      "Epoch: 473, loss 0.012658538140708433\n",
      "Epoch: 474, loss 0.012651785024598216\n",
      "Epoch: 475, loss 0.01264503758906693\n",
      "Epoch: 476, loss 0.012638295875679311\n",
      "Epoch: 477, loss 0.012631559925303772\n",
      "Epoch: 478, loss 0.012624829778121018\n",
      "Epoch: 479, loss 0.01261810547363239\n",
      "Epoch: 480, loss 0.012611387050668868\n",
      "Epoch: 481, loss 0.012604674547398909\n",
      "Epoch: 482, loss 0.012597968001337013\n",
      "Epoch: 483, loss 0.012591267449351703\n",
      "Epoch: 484, loss 0.01258457292767367\n",
      "Epoch: 485, loss 0.012577884471903824\n",
      "Epoch: 486, loss 0.012571202117021034\n",
      "Epoch: 487, loss 0.012564525897390029\n",
      "Epoch: 488, loss 0.012557855846769054\n",
      "Epoch: 489, loss 0.012551191998317517\n",
      "Epoch: 490, loss 0.012544534384603434\n",
      "Epoch: 491, loss 0.012537883037610801\n",
      "Epoch: 492, loss 0.012531237988747227\n",
      "Epoch: 493, loss 0.012524599268850892\n",
      "Epoch: 494, loss 0.012517966908197831\n",
      "Epoch: 495, loss 0.012511340936508936\n",
      "Epoch: 496, loss 0.012504721382957275\n",
      "Epoch: 497, loss 0.012498108276174497\n",
      "Epoch: 498, loss 0.012491501644258177\n",
      "Epoch: 499, loss 0.012484901514778258\n",
      "Epoch: 500, loss 0.01247830791478379\n",
      "Epoch: 501, loss 0.01247172087080969\n",
      "Epoch: 502, loss 0.012465140408883002\n",
      "Epoch: 503, loss 0.012458566554529591\n",
      "Epoch: 504, loss 0.01245199933278038\n",
      "Epoch: 505, loss 0.012445438768177712\n",
      "Epoch: 506, loss 0.012438884884781419\n",
      "Epoch: 507, loss 0.012431876968986134\n",
      "Epoch: 508, loss 0.012424787227835656\n",
      "Epoch: 509, loss 0.012417666076146168\n",
      "Epoch: 510, loss 0.01241053240284296\n",
      "Epoch: 511, loss 0.01240339665854478\n",
      "Epoch: 512, loss 0.012396264646703743\n",
      "Epoch: 513, loss 0.012389139586278318\n",
      "Epoch: 514, loss 0.012382023259072884\n",
      "Epoch: 515, loss 0.012374916647916616\n",
      "Epoch: 516, loss 0.012367820291552499\n",
      "Epoch: 517, loss 0.01236073448194378\n",
      "Epoch: 518, loss 0.012353659373944916\n",
      "Epoch: 519, loss 0.012346595046246403\n",
      "Epoch: 520, loss 0.012334333085556653\n",
      "Epoch: 521, loss 0.01232396390791759\n",
      "Epoch: 522, loss 0.012315040482227972\n",
      "Epoch: 523, loss 0.012306920087296211\n",
      "Epoch: 524, loss 0.012299247095912888\n",
      "Epoch: 525, loss 0.012291825916287488\n",
      "Epoch: 526, loss 0.012284548989897818\n",
      "Epoch: 527, loss 0.012277357171211247\n",
      "Epoch: 528, loss 0.012270217934162844\n",
      "Epoch: 529, loss 0.012263113386285255\n",
      "Epoch: 530, loss 0.012256033678028714\n",
      "Epoch: 531, loss 0.012248973379486038\n",
      "Epoch: 532, loss 0.012241929488900438\n",
      "Epoch: 533, loss 0.012234900338307257\n",
      "Epoch: 534, loss 0.012227884992300025\n",
      "Epoch: 535, loss 0.01222088291778983\n",
      "Epoch: 536, loss 0.012213893802654675\n",
      "Epoch: 537, loss 0.012206917456172466\n",
      "Epoch: 538, loss 0.012199953754367708\n",
      "Epoch: 539, loss 0.012193002610018343\n",
      "Epoch: 540, loss 0.012186063956197608\n",
      "Epoch: 541, loss 0.012179137737245818\n",
      "Epoch: 542, loss 0.012172223903817033\n",
      "Epoch: 543, loss 0.012165322410162181\n",
      "Epoch: 544, loss 0.012158433212638058\n",
      "Epoch: 545, loss 0.01215155626888875\n",
      "Epoch: 546, loss 0.012144691537395823\n",
      "Epoch: 547, loss 0.012137838977230192\n",
      "Epoch: 548, loss 0.012130998547915125\n",
      "Epoch: 549, loss 0.012124170209349399\n",
      "Epoch: 550, loss 0.012117353921764294\n",
      "Epoch: 551, loss 0.012110549645698165\n",
      "Epoch: 552, loss 0.012103757341981677\n",
      "Epoch: 553, loss 0.012096976971727822\n",
      "Epoch: 554, loss 0.012090208496325763\n",
      "Epoch: 555, loss 0.012083451877435517\n",
      "Epoch: 556, loss 0.012076707076984163\n",
      "Epoch: 557, loss 0.012069974057162312\n",
      "Epoch: 558, loss 0.012063252780420924\n",
      "Epoch: 559, loss 0.012056543209468435\n",
      "Epoch: 560, loss 0.01204984530726772\n",
      "Epoch: 561, loss 0.01204315903703344\n",
      "Epoch: 562, loss 0.01203648436222914\n",
      "Epoch: 563, loss 0.012029821246564792\n",
      "Epoch: 564, loss 0.01202316965399411\n",
      "Epoch: 565, loss 0.012016529548711755\n",
      "Epoch: 566, loss 0.012009900895150855\n",
      "Epoch: 567, loss 0.012003283657980509\n",
      "Epoch: 568, loss 0.01199667780210353\n",
      "Epoch: 569, loss 0.01199008329265338\n",
      "Epoch: 570, loss 0.011983500094992218\n",
      "Epoch: 571, loss 0.011976928174708372\n",
      "Epoch: 572, loss 0.011970367497613855\n",
      "Epoch: 573, loss 0.01196381802974223\n",
      "Epoch: 574, loss 0.011957279737346024\n",
      "Epoch: 575, loss 0.01195075258689459\n",
      "Epoch: 576, loss 0.01194423654507191\n",
      "Epoch: 577, loss 0.011937731578774329\n",
      "Epoch: 578, loss 0.011931237655108092\n",
      "Epoch: 579, loss 0.011924754741387766\n",
      "Epoch: 580, loss 0.011918282805133296\n",
      "Epoch: 581, loss 0.011911821814068597\n",
      "Epoch: 582, loss 0.0119053717361191\n",
      "Epoch: 583, loss 0.011898932539409764\n",
      "Epoch: 584, loss 0.011892504192262968\n",
      "Epoch: 585, loss 0.011886086663196675\n",
      "Epoch: 586, loss 0.01187967992092221\n",
      "Epoch: 587, loss 0.011873283934342562\n",
      "Epoch: 588, loss 0.01186689867255014\n",
      "Epoch: 589, loss 0.011860524104825172\n",
      "Epoch: 590, loss 0.011854160200633438\n",
      "Epoch: 591, loss 0.01184780692962493\n",
      "Epoch: 592, loss 0.011841464261631351\n",
      "Epoch: 593, loss 0.01183513216666494\n",
      "Epoch: 594, loss 0.011828810614916231\n",
      "Epoch: 595, loss 0.011822499576752455\n",
      "Epoch: 596, loss 0.01181619902271577\n",
      "Epoch: 597, loss 0.011809908923521503\n",
      "Epoch: 598, loss 0.011803629250056446\n",
      "Epoch: 599, loss 0.0117973599733772\n",
      "Epoch: 600, loss 0.011791101064708386\n",
      "Epoch: 601, loss 0.011784852495441223\n",
      "Epoch: 602, loss 0.011778614237131751\n",
      "Epoch: 603, loss 0.011772386261499119\n",
      "Epoch: 604, loss 0.011766168540424275\n",
      "Epoch: 605, loss 0.011759961045948192\n",
      "Epoch: 606, loss 0.011753763750270395\n",
      "Epoch: 607, loss 0.011747576625747389\n",
      "Epoch: 608, loss 0.011741399644891218\n",
      "Epoch: 609, loss 0.01173523278036785\n",
      "Epoch: 610, loss 0.01172907600499579\n",
      "Epoch: 611, loss 0.011722929291744797\n",
      "Epoch: 612, loss 0.01171679261373407\n",
      "Epoch: 613, loss 0.011710665944231142\n",
      "Epoch: 614, loss 0.011704549256650355\n",
      "Epoch: 615, loss 0.011698442524551404\n",
      "Epoch: 616, loss 0.011692345721638226\n",
      "Epoch: 617, loss 0.01168625882175729\n",
      "Epoch: 618, loss 0.011680181798896572\n",
      "Epoch: 619, loss 0.011674114627184148\n",
      "Epoch: 620, loss 0.011668057280886709\n",
      "Epoch: 621, loss 0.011662009734408663\n",
      "Epoch: 622, loss 0.011655971962290339\n",
      "Epoch: 623, loss 0.011649943939207293\n",
      "Epoch: 624, loss 0.011643925639968597\n",
      "Epoch: 625, loss 0.011637917039516006\n",
      "Epoch: 626, loss 0.011631918112922383\n",
      "Epoch: 627, loss 0.011625928835390745\n",
      "Epoch: 628, loss 0.011619949182253042\n",
      "Epoch: 629, loss 0.01161397912896897\n",
      "Epoch: 630, loss 0.011608018651124834\n",
      "Epoch: 631, loss 0.011602067724432239\n",
      "Epoch: 632, loss 0.011596126324727387\n",
      "Epoch: 633, loss 0.011590194427969444\n",
      "Epoch: 634, loss 0.011584272010239746\n",
      "Epoch: 635, loss 0.01157835904774091\n",
      "Epoch: 636, loss 0.011572455516795074\n",
      "Epoch: 637, loss 0.011566561393843641\n",
      "Epoch: 638, loss 0.011560676655445789\n",
      "Epoch: 639, loss 0.011554801278277476\n",
      "Epoch: 640, loss 0.011548935239130424\n",
      "Epoch: 641, loss 0.011543078514911157\n",
      "Epoch: 642, loss 0.011430000277234603\n",
      "Epoch: 643, loss 0.011431821199044843\n",
      "Epoch: 644, loss 0.011471778743374711\n",
      "Epoch: 645, loss 0.011383756048553234\n",
      "Epoch: 646, loss 0.011398908305065434\n",
      "Epoch: 647, loss 0.011445928565991558\n",
      "Epoch: 648, loss 0.011361216187355324\n",
      "Epoch: 649, loss 0.011379005900193686\n",
      "Epoch: 650, loss 0.011320169948111762\n",
      "Epoch: 651, loss 0.011351476761577235\n",
      "Epoch: 652, loss 0.011299851019318213\n",
      "Epoch: 653, loss 0.011335299735586803\n",
      "Epoch: 654, loss 0.011285474166491049\n",
      "Epoch: 655, loss 0.011322398746552862\n",
      "Epoch: 656, loss 0.01127283592130477\n",
      "Epoch: 657, loss 0.011204602987272841\n",
      "Epoch: 658, loss 0.011270370232136905\n",
      "Epoch: 659, loss 0.011236925711886182\n",
      "Epoch: 660, loss 0.011177224191058328\n",
      "Epoch: 661, loss 0.011247858844334387\n",
      "Epoch: 662, loss 0.011216453104816471\n",
      "Epoch: 663, loss 0.01115809890880091\n",
      "Epoch: 664, loss 0.011124215241926617\n",
      "Epoch: 665, loss 0.011103494163764736\n",
      "Epoch: 666, loss 0.011089754961509896\n",
      "Epoch: 667, loss 0.011079725664011273\n",
      "Epoch: 668, loss 0.0110716731544118\n",
      "Epoch: 669, loss 0.01106467901462213\n",
      "Epoch: 670, loss 0.011058256437339726\n",
      "Epoch: 671, loss 0.011052147341770789\n",
      "Epoch: 672, loss 0.011046214868725179\n",
      "Epoch: 673, loss 0.011040386403590107\n",
      "Epoch: 674, loss 0.011034623377556644\n",
      "Epoch: 675, loss 0.01102890526253828\n",
      "Epoch: 676, loss 0.011023221089432138\n",
      "Epoch: 677, loss 0.011017564953843073\n",
      "Epoch: 678, loss 0.011011933634986443\n",
      "Epoch: 679, loss 0.011006325334339968\n",
      "Epoch: 680, loss 0.011000739007554747\n",
      "Epoch: 681, loss 0.010995174010640405\n",
      "Epoch: 682, loss 0.010989629912607792\n",
      "Epoch: 683, loss 0.010984106396265802\n",
      "Epoch: 684, loss 0.01097860320569576\n",
      "Epoch: 685, loss 0.010973120118438942\n",
      "Epoch: 686, loss 0.010967656930764897\n",
      "Epoch: 687, loss 0.01096221344986447\n",
      "Epoch: 688, loss 0.010956789489706884\n",
      "Epoch: 689, loss 0.010951384868834933\n",
      "Epoch: 690, loss 0.010945999409187482\n",
      "Epoch: 691, loss 0.010940632935463534\n",
      "Epoch: 692, loss 0.0109352852747742\n",
      "Epoch: 693, loss 0.010929956256446728\n",
      "Epoch: 694, loss 0.010924645711908845\n",
      "Epoch: 695, loss 0.0109193534746162\n",
      "Epoch: 696, loss 0.010914079380001947\n",
      "Epoch: 697, loss 0.010908823265439278\n",
      "Epoch: 698, loss 0.010903584970210137\n",
      "Epoch: 699, loss 0.010898364335477447\n",
      "Epoch: 700, loss 0.01089316120425961\n",
      "Epoch: 701, loss 0.010887975421406473\n",
      "Epoch: 702, loss 0.01088280683357548\n",
      "Epoch: 703, loss 0.010877655289209014\n",
      "Epoch: 704, loss 0.010872520638511799\n",
      "Epoch: 705, loss 0.010867402733428802\n",
      "Epoch: 706, loss 0.01086230142762364\n",
      "Epoch: 707, loss 0.01085721657645676\n",
      "Epoch: 708, loss 0.01085214803696464\n",
      "Epoch: 709, loss 0.010847095667838666\n",
      "Epoch: 710, loss 0.010842059329404748\n",
      "Epoch: 711, loss 0.010837038883602856\n",
      "Epoch: 712, loss 0.010832034193967017\n",
      "Epoch: 713, loss 0.010827045125605693\n",
      "Epoch: 714, loss 0.010822071545181857\n",
      "Epoch: 715, loss 0.01081711332089434\n",
      "Epoch: 716, loss 0.010812170322458401\n",
      "Epoch: 717, loss 0.010807242421087094\n",
      "Epoch: 718, loss 0.010802329489472838\n",
      "Epoch: 719, loss 0.010797431401769031\n",
      "Epoch: 720, loss 0.010792548033572356\n",
      "Epoch: 721, loss 0.010787679261904572\n",
      "Epoch: 722, loss 0.010782824965195361\n",
      "Epoch: 723, loss 0.010777985023264703\n",
      "Epoch: 724, loss 0.010773159317306154\n",
      "Epoch: 725, loss 0.010768347729869702\n",
      "Epoch: 726, loss 0.010763550144845167\n",
      "Epoch: 727, loss 0.010758766447446054\n",
      "Epoch: 728, loss 0.010753996524193012\n",
      "Epoch: 729, loss 0.010749240262897851\n",
      "Epoch: 730, loss 0.010744497552648263\n",
      "Epoch: 731, loss 0.010739768283791608\n",
      "Epoch: 732, loss 0.010735052347919882\n",
      "Epoch: 733, loss 0.010730349637854478\n",
      "Epoch: 734, loss 0.010725660047631284\n",
      "Epoch: 735, loss 0.01072098347248584\n",
      "Epoch: 736, loss 0.01071631980883856\n",
      "Epoch: 737, loss 0.010711668954280716\n",
      "Epoch: 738, loss 0.010573023130742406\n",
      "Epoch: 739, loss 0.01061556882730585\n",
      "Epoch: 740, loss 0.010651889029860926\n",
      "Epoch: 741, loss 0.010668876594102164\n",
      "Epoch: 742, loss 0.01054179222405246\n",
      "Epoch: 743, loss 0.010590301965878492\n",
      "Epoch: 744, loss 0.010629748490268208\n",
      "Epoch: 745, loss 0.010514806198071451\n",
      "Epoch: 746, loss 0.010569475708094446\n",
      "Epoch: 747, loss 0.010612179263145994\n",
      "Epoch: 748, loss 0.010498941772669189\n",
      "Epoch: 749, loss 0.010554620572726265\n",
      "Epoch: 750, loss 0.010597818441526158\n",
      "Epoch: 751, loss 0.010484794963280072\n",
      "Epoch: 752, loss 0.010540747410482111\n",
      "Epoch: 753, loss 0.010584044018230016\n",
      "Epoch: 754, loss 0.010471020394373842\n",
      "Epoch: 755, loss 0.010527140871122226\n",
      "Epoch: 756, loss 0.01043730087866664\n",
      "Epoch: 757, loss 0.010505016728489231\n",
      "Epoch: 758, loss 0.01042150530636875\n",
      "Epoch: 759, loss 0.010492422063896709\n",
      "Epoch: 760, loss 0.01041062632320188\n",
      "Epoch: 761, loss 0.010482466404972066\n",
      "Epoch: 762, loss 0.01040113361783577\n",
      "Epoch: 763, loss 0.010473278666881851\n",
      "Epoch: 764, loss 0.010392068016664605\n",
      "Epoch: 765, loss 0.010464350054272598\n",
      "Epoch: 766, loss 0.010383168272121778\n",
      "Epoch: 767, loss 0.01045554155477124\n",
      "Epoch: 768, loss 0.010374362567097837\n",
      "Epoch: 769, loss 0.0104468145142789\n",
      "Epoch: 770, loss 0.01036563049489414\n",
      "Epoch: 771, loss 0.010438157519792015\n",
      "Epoch: 772, loss 0.010356965610635866\n",
      "Epoch: 773, loss 0.010429566564837865\n",
      "Epoch: 774, loss 0.010348365274714236\n",
      "Epoch: 775, loss 0.01042103967107977\n",
      "Epoch: 776, loss 0.010339827899129793\n",
      "Epoch: 777, loss 0.010412575430221055\n",
      "Epoch: 778, loss 0.010331352200909265\n",
      "Epoch: 779, loss 0.010404172608534278\n",
      "Epoch: 780, loss 0.010322936999426793\n",
      "Epoch: 781, loss 0.010263921395919747\n",
      "Epoch: 782, loss 0.010362460563892074\n",
      "Epoch: 783, loss 0.010295402858870496\n",
      "Epoch: 784, loss 0.010243623947627794\n",
      "Epoch: 785, loss 0.010345833573220739\n",
      "Epoch: 786, loss 0.01028076096500213\n",
      "Epoch: 787, loss 0.010230053177415791\n",
      "Epoch: 788, loss 0.010332836727384262\n",
      "Epoch: 789, loss 0.010268045863110852\n",
      "Epoch: 790, loss 0.010217547757230242\n",
      "Epoch: 791, loss 0.010320472900209857\n",
      "Epoch: 792, loss 0.010255723810466313\n",
      "Epoch: 793, loss 0.010205314148782523\n",
      "Epoch: 794, loss 0.010308320344122435\n",
      "Epoch: 795, loss 0.010243577333444838\n",
      "Epoch: 796, loss 0.01019323817192404\n",
      "Epoch: 797, loss 0.010296316729487593\n",
      "Epoch: 798, loss 0.010231572666596314\n",
      "Epoch: 799, loss 0.010181300492673196\n",
      "Epoch: 800, loss 0.010153323154004742\n",
      "Epoch: 801, loss 0.010267736404830741\n",
      "Epoch: 802, loss 0.010209188424170781\n",
      "Epoch: 803, loss 0.010162126104365347\n",
      "Epoch: 804, loss 0.010135840549638122\n",
      "Epoch: 805, loss 0.010251127283128197\n",
      "Epoch: 806, loss 0.010193031277504648\n",
      "Epoch: 807, loss 0.010146277806595622\n",
      "Epoch: 808, loss 0.010120190156122269\n",
      "Epoch: 809, loss 0.01010483428214387\n",
      "Epoch: 810, loss 0.010095016986454557\n",
      "Epoch: 811, loss 0.010218577979818575\n",
      "Epoch: 812, loss 0.010165025509663132\n",
      "Epoch: 813, loss 0.010120673428277362\n",
      "Epoch: 814, loss 0.010095879160212904\n",
      "Epoch: 815, loss 0.010081240932821032\n",
      "Epoch: 816, loss 0.010071841200678475\n",
      "Epoch: 817, loss 0.010065148472525763\n",
      "Epoch: 818, loss 0.010059858800047873\n",
      "Epoch: 819, loss 0.010055300262363414\n",
      "Epoch: 820, loss 0.010051126536072649\n",
      "Epoch: 821, loss 0.010047159075490598\n",
      "Epoch: 822, loss 0.010043305796725589\n",
      "Epoch: 823, loss 0.01003951917284999\n",
      "Epoch: 824, loss 0.010035774638056012\n",
      "Epoch: 825, loss 0.010032059458436007\n",
      "Epoch: 826, loss 0.01002836699699077\n",
      "Epoch: 827, loss 0.010024693758565713\n",
      "Epoch: 828, loss 0.010021037867327668\n",
      "Epoch: 829, loss 0.010017398282391928\n",
      "Epoch: 830, loss 0.010013774393762553\n",
      "Epoch: 831, loss 0.010010165814201385\n",
      "Epoch: 832, loss 0.0100065722720258\n",
      "Epoch: 833, loss 0.01000299355589189\n",
      "Epoch: 834, loss 0.009999429486352348\n",
      "Epoch: 835, loss 0.009995879901201625\n",
      "Epoch: 836, loss 0.009992344647922034\n",
      "Epoch: 837, loss 0.009988823579784993\n",
      "Epoch: 838, loss 0.009985316553834165\n",
      "Epoch: 839, loss 0.009981823429838466\n",
      "Epoch: 840, loss 0.009978344069743253\n",
      "Epoch: 841, loss 0.009974878337378496\n",
      "Epoch: 842, loss 0.009971426098299674\n",
      "Epoch: 843, loss 0.009967987219696304\n",
      "Epoch: 844, loss 0.00996456157033629\n",
      "Epoch: 845, loss 0.009961149020527903\n",
      "Epoch: 846, loss 0.009957749442091773\n",
      "Epoch: 847, loss 0.009954362708337593\n",
      "Epoch: 848, loss 0.009950988694043629\n",
      "Epoch: 849, loss 0.009947627275437547\n",
      "Epoch: 850, loss 0.009944278330178339\n",
      "Epoch: 851, loss 0.009940941737338867\n",
      "Epoch: 852, loss 0.009937617377388318\n",
      "Epoch: 853, loss 0.009934305132175528\n",
      "Epoch: 854, loss 0.009931004884912352\n",
      "Epoch: 855, loss 0.009927716520157169\n",
      "Epoch: 856, loss 0.009924439923798722\n",
      "Epoch: 857, loss 0.009921174983040185\n",
      "Epoch: 858, loss 0.009917921586383433\n",
      "Epoch: 859, loss 0.009914679623613355\n",
      "Epoch: 860, loss 0.009911448985782684\n",
      "Epoch: 861, loss 0.009908229565196618\n",
      "Epoch: 862, loss 0.009905021255398175\n",
      "Epoch: 863, loss 0.009901823951153153\n",
      "Epoch: 864, loss 0.009898637548435547\n",
      "Epoch: 865, loss 0.009895461944413434\n",
      "Epoch: 866, loss 0.009892297037434442\n",
      "Epoch: 867, loss 0.009889142727011748\n",
      "Epoch: 868, loss 0.009885998913810546\n",
      "Epoch: 869, loss 0.009882865499633922\n",
      "Epoch: 870, loss 0.009879742387409648\n",
      "Epoch: 871, loss 0.00987662948117689\n",
      "Epoch: 872, loss 0.009873526686072612\n",
      "Epoch: 873, loss 0.009870433908319152\n",
      "Epoch: 874, loss 0.00986735105521094\n",
      "Epoch: 875, loss 0.009864278035102094\n",
      "Epoch: 876, loss 0.009861214757393766\n",
      "Epoch: 877, loss 0.009858161132521682\n",
      "Epoch: 878, loss 0.009855117071944192\n",
      "Epoch: 879, loss 0.009852082488130106\n",
      "Epoch: 880, loss 0.00984905729454656\n",
      "Epoch: 881, loss 0.009846041405647765\n",
      "Epoch: 882, loss 0.009843034736862856\n",
      "Epoch: 883, loss 0.00984003720458484\n",
      "Epoch: 884, loss 0.009837048726158986\n",
      "Epoch: 885, loss 0.009834069219871802\n",
      "Epoch: 886, loss 0.009831098604939945\n",
      "Epoch: 887, loss 0.009828136801499337\n",
      "Epoch: 888, loss 0.009825183730594341\n",
      "Epoch: 889, loss 0.009822239314167284\n",
      "Epoch: 890, loss 0.009819303475047652\n",
      "Epoch: 891, loss 0.009816376136941906\n",
      "Epoch: 892, loss 0.009813457224423342\n",
      "Epoch: 893, loss 0.009810546662921684\n",
      "Epoch: 894, loss 0.00980764437871335\n",
      "Epoch: 895, loss 0.009804750298911327\n",
      "Epoch: 896, loss 0.0098018643514556\n",
      "Epoch: 897, loss 0.00979898646510349\n",
      "Epoch: 898, loss 0.009796116569420088\n",
      "Epoch: 899, loss 0.009793254594768827\n",
      "Epoch: 900, loss 0.009790400472302301\n",
      "Epoch: 901, loss 0.009787554133952903\n",
      "Epoch: 902, loss 0.009784715512423941\n",
      "Epoch: 903, loss 0.009781884541180647\n",
      "Epoch: 904, loss 0.00977906115444122\n",
      "Epoch: 905, loss 0.00977624528716828\n",
      "Epoch: 906, loss 0.009773436875060042\n",
      "Epoch: 907, loss 0.009770635854541864\n",
      "Epoch: 908, loss 0.009767842162757924\n",
      "Epoch: 909, loss 0.009765055737562681\n",
      "Epoch: 910, loss 0.009762276517512937\n",
      "Epoch: 911, loss 0.009759504441859385\n",
      "Epoch: 912, loss 0.009756739450538794\n",
      "Epoch: 913, loss 0.009753981484166152\n",
      "Epoch: 914, loss 0.009751230484026499\n",
      "Epoch: 915, loss 0.009748486392067657\n",
      "Epoch: 916, loss 0.009745749150892128\n",
      "Epoch: 917, loss 0.009743018703749901\n",
      "Epoch: 918, loss 0.00974029499453068\n",
      "Epoch: 919, loss 0.00973757796775681\n",
      "Epoch: 920, loss 0.009734867568575747\n",
      "Epoch: 921, loss 0.009732163742752922\n",
      "Epoch: 922, loss 0.009729466436664773\n",
      "Epoch: 923, loss 0.009726775597291496\n",
      "Epoch: 924, loss 0.009724091172210259\n",
      "Epoch: 925, loss 0.009721413109588219\n",
      "Epoch: 926, loss 0.009718741358175827\n",
      "Epoch: 927, loss 0.009716075867300165\n",
      "Epoch: 928, loss 0.009713416586858196\n",
      "Epoch: 929, loss 0.00971076346731035\n",
      "Epoch: 930, loss 0.009708116459673887\n",
      "Epoch: 931, loss 0.009705475515516733\n",
      "Epoch: 932, loss 0.009702840586951022\n",
      "Epoch: 933, loss 0.009700211626626942\n",
      "Epoch: 934, loss 0.00969758858772635\n",
      "Epoch: 935, loss 0.00969497142395702\n",
      "Epoch: 936, loss 0.009692360089546383\n",
      "Epoch: 937, loss 0.009689754539235585\n",
      "Epoch: 938, loss 0.00968715472827384\n",
      "Epoch: 939, loss 0.009684560612412275\n",
      "Epoch: 940, loss 0.009681972147898442\n",
      "Epoch: 941, loss 0.009679389291470613\n",
      "Epoch: 942, loss 0.009676812000351994\n",
      "Epoch: 943, loss 0.009674240232245437\n",
      "Epoch: 944, loss 0.009671673945327773\n",
      "Epoch: 945, loss 0.009669113098244543\n",
      "Epoch: 946, loss 0.009666557650104407\n",
      "Epoch: 947, loss 0.009664007560474246\n",
      "Epoch: 948, loss 0.009661462789373531\n",
      "Epoch: 949, loss 0.009658923297269402\n",
      "Epoch: 950, loss 0.009656389045071516\n",
      "Epoch: 951, loss 0.00965385999412703\n",
      "Epoch: 952, loss 0.009651336106215522\n",
      "Epoch: 953, loss 0.009648817343544117\n",
      "Epoch: 954, loss 0.009646303668742586\n",
      "Epoch: 955, loss 0.009643795044858742\n",
      "Epoch: 956, loss 0.009641291435353246\n",
      "Epoch: 957, loss 0.00963879280409538\n",
      "Epoch: 958, loss 0.009636299115358068\n",
      "Epoch: 959, loss 0.009633810333813435\n",
      "Epoch: 960, loss 0.0096313264245283\n",
      "Epoch: 961, loss 0.00962884735295946\n",
      "Epoch: 962, loss 0.009626373084949496\n",
      "Epoch: 963, loss 0.009623903586722401\n",
      "Epoch: 964, loss 0.009621438824879052\n",
      "Epoch: 965, loss 0.00961897876639293\n",
      "Epoch: 966, loss 0.009616523378606261\n",
      "Epoch: 967, loss 0.009614072629225272\n",
      "Epoch: 968, loss 0.009611626486316496\n",
      "Epoch: 969, loss 0.009609184918302498\n",
      "Epoch: 970, loss 0.009606747893957789\n",
      "Epoch: 971, loss 0.009604315382404924\n",
      "Epoch: 972, loss 0.009601887353110586\n",
      "Epoch: 973, loss 0.009599463775881575\n",
      "Epoch: 974, loss 0.009597044620861007\n",
      "Epoch: 975, loss 0.009594629858524514\n",
      "Epoch: 976, loss 0.009592219459676357\n",
      "Epoch: 977, loss 0.009589813395445878\n",
      "Epoch: 978, loss 0.009587411637283662\n",
      "Epoch: 979, loss 0.009585014156958071\n",
      "Epoch: 980, loss 0.009582620926551377\n",
      "Epoch: 981, loss 0.009580231918456528\n",
      "Epoch: 982, loss 0.009577847105373306\n",
      "Epoch: 983, loss 0.009575466460305012\n",
      "Epoch: 984, loss 0.00957308995655511\n",
      "Epoch: 985, loss 0.009570717567723666\n",
      "Epoch: 986, loss 0.00956834926770409\n",
      "Epoch: 987, loss 0.009565985030679739\n",
      "Epoch: 988, loss 0.009563624831120663\n",
      "Epoch: 989, loss 0.009561268643780397\n",
      "Epoch: 990, loss 0.009558916443692738\n",
      "Epoch: 991, loss 0.009556568206168566\n",
      "Epoch: 992, loss 0.009554223906792562\n",
      "Epoch: 993, loss 0.009551883521420411\n",
      "Epoch: 994, loss 0.009549547026175399\n",
      "Epoch: 995, loss 0.009547214397445719\n",
      "Epoch: 996, loss 0.00954488561188123\n",
      "Epoch: 997, loss 0.009542560646390526\n",
      "Epoch: 998, loss 0.00954023947813809\n",
      "Epoch: 999, loss 0.009537922084541316\n",
      "Epoch: 1000, loss 0.009535608443267764\n"
     ]
    }
   ],
   "source": [
    "# we'll start building our neural network training app here\n",
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# we'll save these for future\n",
    "# se we can compare results to the final weights\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    " \n",
    "# learning rate and epochs\n",
    "LR = 0.005\n",
    "epochs = 1000\n",
    "\n",
    "# DataFrame data values as list\n",
    "data = list(df.values)\n",
    "\n",
    "# use min/max-scaling to make the values in the range 0 - 1\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# let's initialize a list for loss points\n",
    "loss_points = []\n",
    "\n",
    "# START THE TRAINING PROCESS\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # let's also monitor epoch-wise losses\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # for example with first row\n",
    "        # [1, 0, 2] => assign input1 = 1, input2 = 0, true_value = 2\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the the previous weights\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # probably used later, we might want to have error metrics (MSE)\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current training data row loss to epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solve w5 and update the new value\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # solve w6 and update the new value\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # solve bias3 and update the new value\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT FORWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES -> CHAIN RULE\n",
    "\n",
    "        # see the materials and the math experiment notebook for more details\n",
    "        # start with weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS!\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it the loss_points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the average loss of this epoch into the overall loss list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.8241086996324927\n",
      "w2: 1.699613599282119\n",
      "w3: 0.7741475622753586\n",
      "w4: -0.16670742338578337\n",
      "w5: 1.5977528347745031\n",
      "w6: 1.8307169997666117\n",
      "b1: -0.001311810449009308\n",
      "b2: -0.11110157137283395\n",
      "b3: -0.03222246751527733\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the the previous weights\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1    1\n",
       "x2    6\n",
       "y     9\n",
       "Name: 15, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can see the expected value is 9\n",
    "df.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03846154, 0.23076923, 0.34615385])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the first two values into the predict()-function below\n",
    "data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3632144186430953"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.03846154, 0.23076923)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.443574884720478"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert scaled value back to actual y-value \n",
    "# basically multiply the predicted result with max-value of y =>\n",
    "# get the corresponding y-value\n",
    "df['y'].max() * result\n",
    "\n",
    "# the predicted is 9.4, real value is 9\n",
    "# so the model works fairly ok with x1/x2/y -type of equation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

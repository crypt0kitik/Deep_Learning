{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5071aa3b-e75a-4e5c-8f0f-5b1e3e18c801",
   "metadata": {},
   "source": [
    "I noticed that when I used EarlyStop, it actually stopped the improvements of metrics.\n",
    "In this notebook I will try the combination of two techniques:\n",
    "- Dropout-layers\n",
    "- ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bda7e-a490-4d6e-853c-48f62cc83e46",
   "metadata": {},
   "source": [
    "## Uploading data, X-y variables, Train/test/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa87a3bd-b718-4220-b6a4-9ed9f8eea425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057ad201-930c-4d1d-88f6-0ebf7f7195c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data\n",
    "df = pd.read_csv(\"balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2aede19-8838-438b-a671-f60ca82c982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave out the target variable! (dependent variable)\n",
    "X = df[['Year', 'Mileage', 'City', 'State', 'Make', 'Model']]\n",
    "\n",
    "# have only the target variable here (dependent variable)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd94c44-f805-4bd5-8466-e2b58d64e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, train/test split => 70% for training, 30% for other purposes (temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# now, split the 30% for other purposes by 50% (resulting in 2 x 15%)\n",
    "# so finally, we have:\n",
    "# 70% for training\n",
    "# 15% for testing\n",
    "# 15% for validation\n",
    "# => 70 + 15 +15 = 100%\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbcdab-5e8c-4fbe-83ac-b984206bcde2",
   "metadata": {},
   "source": [
    "## Development of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4a270-360f-4c47-9e05-1d37ef24c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiliiazemskova/anaconda3/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">281</span> (1.10 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m281\u001b[0m (1.10 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269</span> (1.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269\u001b[0m (1.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> (48.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12\u001b[0m (48.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 238970688.0000 - val_loss: 33088866.0000 - learning_rate: 0.0010\n",
      "Epoch 2/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 36208860.0000 - val_loss: 32980088.0000 - learning_rate: 0.0010\n",
      "Epoch 3/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35973392.0000 - val_loss: 32976336.0000 - learning_rate: 0.0010\n",
      "Epoch 4/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 36180184.0000 - val_loss: 33043142.0000 - learning_rate: 0.0010\n",
      "Epoch 5/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 36116280.0000 - val_loss: 32972166.0000 - learning_rate: 0.0010\n",
      "Epoch 6/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 36021608.0000 - val_loss: 32974010.0000 - learning_rate: 0.0010\n",
      "Epoch 7/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36066248.0000 - val_loss: 32976988.0000 - learning_rate: 0.0010\n",
      "Epoch 8/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 36100348.0000 - val_loss: 33063978.0000 - learning_rate: 0.0010\n",
      "Epoch 9/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 366us/step - loss: 36018716.0000 - val_loss: 33023760.0000 - learning_rate: 0.0010\n",
      "Epoch 10/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 36126060.0000 - val_loss: 33029362.0000 - learning_rate: 0.0010\n",
      "Epoch 11/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36043204.0000 - val_loss: 32993236.0000 - learning_rate: 0.0010\n",
      "Epoch 12/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36109672.0000 - val_loss: 33015454.0000 - learning_rate: 0.0010\n",
      "Epoch 13/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36097012.0000 - val_loss: 32987628.0000 - learning_rate: 0.0010\n",
      "Epoch 14/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 36072924.0000 - val_loss: 33107924.0000 - learning_rate: 0.0010\n",
      "Epoch 15/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36017368.0000 - val_loss: 32986106.0000 - learning_rate: 0.0010\n",
      "Epoch 16/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35981204.0000 - val_loss: 32985780.0000 - learning_rate: 0.0010\n",
      "Epoch 17/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 416us/step - loss: 36030352.0000 - val_loss: 32976774.0000 - learning_rate: 0.0010\n",
      "Epoch 18/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 36173704.0000 - val_loss: 32986658.0000 - learning_rate: 0.0010\n",
      "Epoch 19/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35956828.0000 - val_loss: 32978160.0000 - learning_rate: 0.0010\n",
      "Epoch 20/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35875420.0000 - val_loss: 32966450.0000 - learning_rate: 0.0010\n",
      "Epoch 21/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36091224.0000 - val_loss: 33098452.0000 - learning_rate: 0.0010\n",
      "Epoch 22/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35898096.0000 - val_loss: 33013082.0000 - learning_rate: 0.0010\n",
      "Epoch 23/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 36066496.0000 - val_loss: 33006104.0000 - learning_rate: 0.0010\n",
      "Epoch 24/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 36075788.0000 - val_loss: 32984146.0000 - learning_rate: 0.0010\n",
      "Epoch 25/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35953572.0000 - val_loss: 32975838.0000 - learning_rate: 0.0010\n",
      "Epoch 26/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 36090744.0000 - val_loss: 32978200.0000 - learning_rate: 0.0010\n",
      "Epoch 27/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35896800.0000 - val_loss: 32978216.0000 - learning_rate: 0.0010\n",
      "Epoch 28/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 36000704.0000 - val_loss: 32959574.0000 - learning_rate: 0.0010\n",
      "Epoch 29/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 35952112.0000 - val_loss: 32998124.0000 - learning_rate: 0.0010\n",
      "Epoch 30/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35806012.0000 - val_loss: 32982640.0000 - learning_rate: 0.0010\n",
      "Epoch 31/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35964800.0000 - val_loss: 32982712.0000 - learning_rate: 0.0010\n",
      "Epoch 32/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35889804.0000 - val_loss: 32993846.0000 - learning_rate: 0.0010\n",
      "Epoch 33/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36266772.0000 - val_loss: 33110222.0000 - learning_rate: 0.0010\n",
      "Epoch 34/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35955312.0000 - val_loss: 33058252.0000 - learning_rate: 0.0010\n",
      "Epoch 35/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 36122420.0000 - val_loss: 33004806.0000 - learning_rate: 0.0010\n",
      "Epoch 36/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 415us/step - loss: 35926400.0000 - val_loss: 33011580.0000 - learning_rate: 0.0010\n",
      "Epoch 37/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 35935948.0000 - val_loss: 33012952.0000 - learning_rate: 0.0010\n",
      "Epoch 38/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 36100924.0000 - val_loss: 33037798.0000 - learning_rate: 0.0010\n",
      "Epoch 39/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35986968.0000 - val_loss: 32997374.0000 - learning_rate: 0.0010\n",
      "Epoch 40/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35881084.0000 - val_loss: 33001250.0000 - learning_rate: 0.0010\n",
      "Epoch 41/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36159948.0000 - val_loss: 33088024.0000 - learning_rate: 0.0010\n",
      "Epoch 42/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35952852.0000 - val_loss: 33029172.0000 - learning_rate: 0.0010\n",
      "Epoch 43/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36003716.0000 - val_loss: 32981886.0000 - learning_rate: 0.0010\n",
      "Epoch 44/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 409us/step - loss: 35916072.0000 - val_loss: 32991540.0000 - learning_rate: 0.0010\n",
      "Epoch 45/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36069652.0000 - val_loss: 32980550.0000 - learning_rate: 0.0010\n",
      "Epoch 46/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35959108.0000 - val_loss: 33019604.0000 - learning_rate: 0.0010\n",
      "Epoch 47/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36079480.0000 - val_loss: 32986650.0000 - learning_rate: 0.0010\n",
      "Epoch 48/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 36073044.0000 - val_loss: 32993426.0000 - learning_rate: 0.0010\n",
      "Epoch 49/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35955948.0000 - val_loss: 33018996.0000 - learning_rate: 0.0010\n",
      "Epoch 50/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35813868.0000 - val_loss: 33071948.0000 - learning_rate: 0.0010\n",
      "Epoch 51/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35795316.0000 - val_loss: 32983080.0000 - learning_rate: 0.0010\n",
      "Epoch 52/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35903324.0000 - val_loss: 32970096.0000 - learning_rate: 0.0010\n",
      "Epoch 53/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35911144.0000 - val_loss: 33009662.0000 - learning_rate: 0.0010\n",
      "Epoch 54/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35983608.0000 - val_loss: 32976596.0000 - learning_rate: 0.0010\n",
      "Epoch 55/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35756240.0000 - val_loss: 32982296.0000 - learning_rate: 0.0010\n",
      "Epoch 56/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35782452.0000 - val_loss: 32994224.0000 - learning_rate: 0.0010\n",
      "Epoch 57/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35770368.0000 - val_loss: 32979638.0000 - learning_rate: 0.0010\n",
      "Epoch 58/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35807164.0000 - val_loss: 33004966.0000 - learning_rate: 0.0010\n",
      "Epoch 59/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36013488.0000 - val_loss: 32981588.0000 - learning_rate: 0.0010\n",
      "Epoch 60/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35824720.0000 - val_loss: 32973958.0000 - learning_rate: 0.0010\n",
      "Epoch 61/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 36113060.0000 - val_loss: 33042820.0000 - learning_rate: 0.0010\n",
      "Epoch 62/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35877060.0000 - val_loss: 33023426.0000 - learning_rate: 0.0010\n",
      "Epoch 63/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35866240.0000 - val_loss: 32974224.0000 - learning_rate: 0.0010\n",
      "Epoch 64/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35987160.0000 - val_loss: 32995790.0000 - learning_rate: 0.0010\n",
      "Epoch 65/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36007728.0000 - val_loss: 33061462.0000 - learning_rate: 0.0010\n",
      "Epoch 66/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35891268.0000 - val_loss: 33033054.0000 - learning_rate: 0.0010\n",
      "Epoch 67/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36084548.0000 - val_loss: 32973692.0000 - learning_rate: 0.0010\n",
      "Epoch 68/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 36081320.0000 - val_loss: 33063272.0000 - learning_rate: 0.0010\n",
      "Epoch 69/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 36114316.0000 - val_loss: 32986548.0000 - learning_rate: 0.0010\n",
      "Epoch 70/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35891716.0000 - val_loss: 33006894.0000 - learning_rate: 0.0010\n",
      "Epoch 71/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35709672.0000 - val_loss: 32990804.0000 - learning_rate: 0.0010\n",
      "Epoch 72/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35968828.0000 - val_loss: 33069334.0000 - learning_rate: 0.0010\n",
      "Epoch 73/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436us/step - loss: 36144956.0000 - val_loss: 33035784.0000 - learning_rate: 0.0010\n",
      "Epoch 74/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 36015708.0000 - val_loss: 32981156.0000 - learning_rate: 0.0010\n",
      "Epoch 75/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35880280.0000 - val_loss: 32976168.0000 - learning_rate: 0.0010\n",
      "Epoch 76/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35831384.0000 - val_loss: 32971352.0000 - learning_rate: 0.0010\n",
      "Epoch 77/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35657888.0000 - val_loss: 32992062.0000 - learning_rate: 0.0010\n",
      "Epoch 78/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 36070280.0000 - val_loss: 32973322.0000 - learning_rate: 0.0010\n",
      "Epoch 79/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 36104916.0000 - val_loss: 32992274.0000 - learning_rate: 0.0010\n",
      "Epoch 80/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 35711008.0000 - val_loss: 33015318.0000 - learning_rate: 0.0010\n",
      "Epoch 81/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35706072.0000 - val_loss: 32985112.0000 - learning_rate: 0.0010\n",
      "Epoch 82/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35909892.0000 - val_loss: 33014090.0000 - learning_rate: 0.0010\n",
      "Epoch 83/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35736144.0000 - val_loss: 32983886.0000 - learning_rate: 0.0010\n",
      "Epoch 84/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35925208.0000 - val_loss: 33046304.0000 - learning_rate: 0.0010\n",
      "Epoch 85/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35843224.0000 - val_loss: 32992150.0000 - learning_rate: 0.0010\n",
      "Epoch 86/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35676348.0000 - val_loss: 33072648.0000 - learning_rate: 0.0010\n",
      "Epoch 87/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35985244.0000 - val_loss: 32990720.0000 - learning_rate: 0.0010\n",
      "Epoch 88/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35821728.0000 - val_loss: 33056382.0000 - learning_rate: 0.0010\n",
      "Epoch 89/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35763160.0000 - val_loss: 33018362.0000 - learning_rate: 0.0010\n",
      "Epoch 90/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35911528.0000 - val_loss: 32968502.0000 - learning_rate: 0.0010\n",
      "Epoch 91/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35724108.0000 - val_loss: 32974646.0000 - learning_rate: 0.0010\n",
      "Epoch 92/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35854636.0000 - val_loss: 32999902.0000 - learning_rate: 0.0010\n",
      "Epoch 93/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35769988.0000 - val_loss: 32969756.0000 - learning_rate: 0.0010\n",
      "Epoch 94/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35810604.0000 - val_loss: 33084174.0000 - learning_rate: 0.0010\n",
      "Epoch 95/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 36067828.0000 - val_loss: 33002314.0000 - learning_rate: 0.0010\n",
      "Epoch 96/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35831700.0000 - val_loss: 32974552.0000 - learning_rate: 0.0010\n",
      "Epoch 97/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35817164.0000 - val_loss: 32987038.0000 - learning_rate: 0.0010\n",
      "Epoch 98/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35662628.0000 - val_loss: 32981714.0000 - learning_rate: 0.0010\n",
      "Epoch 99/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35896060.0000 - val_loss: 33073844.0000 - learning_rate: 0.0010\n",
      "Epoch 100/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35764224.0000 - val_loss: 33020666.0000 - learning_rate: 0.0010\n",
      "Epoch 101/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35725620.0000 - val_loss: 32969278.0000 - learning_rate: 0.0010\n",
      "Epoch 102/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35977992.0000 - val_loss: 32976878.0000 - learning_rate: 0.0010\n",
      "Epoch 103/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35766144.0000 - val_loss: 32973456.0000 - learning_rate: 0.0010\n",
      "Epoch 104/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35740876.0000 - val_loss: 33005944.0000 - learning_rate: 0.0010\n",
      "Epoch 105/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35981060.0000 - val_loss: 33016542.0000 - learning_rate: 0.0010\n",
      "Epoch 106/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35765916.0000 - val_loss: 32973178.0000 - learning_rate: 0.0010\n",
      "Epoch 107/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 443us/step - loss: 35789832.0000 - val_loss: 32978506.0000 - learning_rate: 0.0010\n",
      "Epoch 108/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35705140.0000 - val_loss: 32978354.0000 - learning_rate: 0.0010\n",
      "Epoch 109/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35665332.0000 - val_loss: 33211174.0000 - learning_rate: 0.0010\n",
      "Epoch 110/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35883048.0000 - val_loss: 33073346.0000 - learning_rate: 0.0010\n",
      "Epoch 111/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35748276.0000 - val_loss: 33149392.0000 - learning_rate: 0.0010\n",
      "Epoch 112/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35729708.0000 - val_loss: 32999694.0000 - learning_rate: 0.0010\n",
      "Epoch 113/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35788220.0000 - val_loss: 33086510.0000 - learning_rate: 0.0010\n",
      "Epoch 114/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35834260.0000 - val_loss: 32982468.0000 - learning_rate: 0.0010\n",
      "Epoch 115/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35796240.0000 - val_loss: 32971124.0000 - learning_rate: 0.0010\n",
      "Epoch 116/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35915808.0000 - val_loss: 32970750.0000 - learning_rate: 0.0010\n",
      "Epoch 117/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35813508.0000 - val_loss: 32976560.0000 - learning_rate: 0.0010\n",
      "Epoch 118/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35795900.0000 - val_loss: 32977176.0000 - learning_rate: 0.0010\n",
      "Epoch 119/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35612016.0000 - val_loss: 33067598.0000 - learning_rate: 0.0010\n",
      "Epoch 120/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35986876.0000 - val_loss: 32983790.0000 - learning_rate: 0.0010\n",
      "Epoch 121/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35624700.0000 - val_loss: 33114766.0000 - learning_rate: 0.0010\n",
      "Epoch 122/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 36051272.0000 - val_loss: 32984828.0000 - learning_rate: 0.0010\n",
      "Epoch 123/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35699052.0000 - val_loss: 32980600.0000 - learning_rate: 0.0010\n",
      "Epoch 124/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35956540.0000 - val_loss: 32977730.0000 - learning_rate: 0.0010\n",
      "Epoch 125/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 35948060.0000 - val_loss: 32998118.0000 - learning_rate: 0.0010\n",
      "Epoch 126/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35726468.0000 - val_loss: 33070562.0000 - learning_rate: 0.0010\n",
      "Epoch 127/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35796416.0000 - val_loss: 32977372.0000 - learning_rate: 0.0010\n",
      "Epoch 128/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35794680.0000 - val_loss: 32979298.0000 - learning_rate: 0.0010\n",
      "Epoch 129/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35688600.0000 - val_loss: 32987490.0000 - learning_rate: 0.0010\n",
      "Epoch 130/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35810896.0000 - val_loss: 32992286.0000 - learning_rate: 0.0010\n",
      "Epoch 131/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35717556.0000 - val_loss: 32971378.0000 - learning_rate: 0.0010\n",
      "Epoch 132/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35651036.0000 - val_loss: 33120078.0000 - learning_rate: 0.0010\n",
      "Epoch 133/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35881980.0000 - val_loss: 33029424.0000 - learning_rate: 0.0010\n",
      "Epoch 134/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35796592.0000 - val_loss: 32961398.0000 - learning_rate: 0.0010\n",
      "Epoch 135/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35809708.0000 - val_loss: 33075964.0000 - learning_rate: 0.0010\n",
      "Epoch 136/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35865360.0000 - val_loss: 32996520.0000 - learning_rate: 0.0010\n",
      "Epoch 137/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35576704.0000 - val_loss: 33036834.0000 - learning_rate: 0.0010\n",
      "Epoch 138/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35606192.0000 - val_loss: 33012416.0000 - learning_rate: 0.0010\n",
      "Epoch 139/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35770284.0000 - val_loss: 33009508.0000 - learning_rate: 0.0010\n",
      "Epoch 140/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35576200.0000 - val_loss: 33082080.0000 - learning_rate: 0.0010\n",
      "Epoch 141/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35777476.0000 - val_loss: 32993926.0000 - learning_rate: 0.0010\n",
      "Epoch 142/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35646808.0000 - val_loss: 32980690.0000 - learning_rate: 0.0010\n",
      "Epoch 143/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35587912.0000 - val_loss: 33010854.0000 - learning_rate: 0.0010\n",
      "Epoch 144/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35711808.0000 - val_loss: 32972706.0000 - learning_rate: 0.0010\n",
      "Epoch 145/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35856828.0000 - val_loss: 32969596.0000 - learning_rate: 0.0010\n",
      "Epoch 146/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35649928.0000 - val_loss: 32978348.0000 - learning_rate: 0.0010\n",
      "Epoch 147/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35606460.0000 - val_loss: 32992590.0000 - learning_rate: 0.0010\n",
      "Epoch 148/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35723764.0000 - val_loss: 33002488.0000 - learning_rate: 0.0010\n",
      "Epoch 149/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35461836.0000 - val_loss: 33006728.0000 - learning_rate: 0.0010\n",
      "Epoch 150/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35461292.0000 - val_loss: 32973644.0000 - learning_rate: 0.0010\n",
      "Epoch 151/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35818680.0000 - val_loss: 33039358.0000 - learning_rate: 0.0010\n",
      "Epoch 152/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35530424.0000 - val_loss: 32993164.0000 - learning_rate: 0.0010\n",
      "Epoch 153/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35706508.0000 - val_loss: 32966648.0000 - learning_rate: 0.0010\n",
      "Epoch 154/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35700884.0000 - val_loss: 33113958.0000 - learning_rate: 0.0010\n",
      "Epoch 155/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35797748.0000 - val_loss: 32982324.0000 - learning_rate: 0.0010\n",
      "Epoch 156/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35639312.0000 - val_loss: 33039652.0000 - learning_rate: 0.0010\n",
      "Epoch 157/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35872160.0000 - val_loss: 33017472.0000 - learning_rate: 0.0010\n",
      "Epoch 158/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35458168.0000 - val_loss: 32987552.0000 - learning_rate: 0.0010\n",
      "Epoch 159/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35545820.0000 - val_loss: 33011746.0000 - learning_rate: 0.0010\n",
      "Epoch 160/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35404892.0000 - val_loss: 32962452.0000 - learning_rate: 0.0010\n",
      "Epoch 161/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35501172.0000 - val_loss: 33000714.0000 - learning_rate: 0.0010\n",
      "Epoch 162/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35435304.0000 - val_loss: 33009788.0000 - learning_rate: 0.0010\n",
      "Epoch 163/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35489120.0000 - val_loss: 32978942.0000 - learning_rate: 0.0010\n",
      "Epoch 164/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35403488.0000 - val_loss: 33069434.0000 - learning_rate: 0.0010\n",
      "Epoch 165/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35413752.0000 - val_loss: 33061742.0000 - learning_rate: 0.0010\n",
      "Epoch 166/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35565932.0000 - val_loss: 33046598.0000 - learning_rate: 0.0010\n",
      "Epoch 167/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35729768.0000 - val_loss: 33094236.0000 - learning_rate: 0.0010\n",
      "Epoch 168/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35585320.0000 - val_loss: 32966738.0000 - learning_rate: 0.0010\n",
      "Epoch 169/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 35437184.0000 - val_loss: 32994812.0000 - learning_rate: 0.0010\n",
      "Epoch 170/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35360512.0000 - val_loss: 32976190.0000 - learning_rate: 0.0010\n",
      "Epoch 171/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35423476.0000 - val_loss: 32975280.0000 - learning_rate: 0.0010\n",
      "Epoch 172/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35602140.0000 - val_loss: 32987134.0000 - learning_rate: 0.0010\n",
      "Epoch 173/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35510012.0000 - val_loss: 32970072.0000 - learning_rate: 0.0010\n",
      "Epoch 174/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35433488.0000 - val_loss: 32975880.0000 - learning_rate: 0.0010\n",
      "Epoch 175/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35410992.0000 - val_loss: 33081688.0000 - learning_rate: 0.0010\n",
      "Epoch 176/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35505380.0000 - val_loss: 33050906.0000 - learning_rate: 0.0010\n",
      "Epoch 177/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35458408.0000 - val_loss: 33052056.0000 - learning_rate: 0.0010\n",
      "Epoch 178/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35409044.0000 - val_loss: 33042818.0000 - learning_rate: 0.0010\n",
      "Epoch 179/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35431792.0000 - val_loss: 32981502.0000 - learning_rate: 0.0010\n",
      "Epoch 180/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35511816.0000 - val_loss: 32965754.0000 - learning_rate: 0.0010\n",
      "Epoch 181/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35388680.0000 - val_loss: 32969100.0000 - learning_rate: 0.0010\n",
      "Epoch 182/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35409284.0000 - val_loss: 32988222.0000 - learning_rate: 0.0010\n",
      "Epoch 183/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35350508.0000 - val_loss: 33041054.0000 - learning_rate: 0.0010\n",
      "Epoch 184/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35439220.0000 - val_loss: 33032142.0000 - learning_rate: 0.0010\n",
      "Epoch 185/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35358232.0000 - val_loss: 32974944.0000 - learning_rate: 0.0010\n",
      "Epoch 186/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 35197440.0000 - val_loss: 33033528.0000 - learning_rate: 0.0010\n",
      "Epoch 187/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35131528.0000 - val_loss: 33128650.0000 - learning_rate: 0.0010\n",
      "Epoch 188/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35275588.0000 - val_loss: 32985092.0000 - learning_rate: 0.0010\n",
      "Epoch 189/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35173132.0000 - val_loss: 32979824.0000 - learning_rate: 0.0010\n",
      "Epoch 190/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35348272.0000 - val_loss: 32988264.0000 - learning_rate: 0.0010\n",
      "Epoch 191/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35293508.0000 - val_loss: 33176786.0000 - learning_rate: 0.0010\n",
      "Epoch 192/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35473244.0000 - val_loss: 33022754.0000 - learning_rate: 0.0010\n",
      "Epoch 193/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35303380.0000 - val_loss: 32988072.0000 - learning_rate: 0.0010\n",
      "Epoch 194/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35359884.0000 - val_loss: 33020450.0000 - learning_rate: 0.0010\n",
      "Epoch 195/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35334472.0000 - val_loss: 32981044.0000 - learning_rate: 0.0010\n",
      "Epoch 196/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35324584.0000 - val_loss: 32996498.0000 - learning_rate: 0.0010\n",
      "Epoch 197/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35089788.0000 - val_loss: 33000730.0000 - learning_rate: 0.0010\n",
      "Epoch 198/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35370404.0000 - val_loss: 33048248.0000 - learning_rate: 0.0010\n",
      "Epoch 199/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 35275856.0000 - val_loss: 33033280.0000 - learning_rate: 0.0010\n",
      "Epoch 200/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35229820.0000 - val_loss: 33031414.0000 - learning_rate: 0.0010\n",
      "Epoch 201/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35277424.0000 - val_loss: 33238928.0000 - learning_rate: 0.0010\n",
      "Epoch 202/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35014716.0000 - val_loss: 33123118.0000 - learning_rate: 0.0010\n",
      "Epoch 203/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35110004.0000 - val_loss: 32987680.0000 - learning_rate: 0.0010\n",
      "Epoch 204/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34990228.0000 - val_loss: 33141688.0000 - learning_rate: 0.0010\n",
      "Epoch 205/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 34948656.0000 - val_loss: 33160500.0000 - learning_rate: 0.0010\n",
      "Epoch 206/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 34896800.0000 - val_loss: 33068372.0000 - learning_rate: 0.0010\n",
      "Epoch 207/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 34711352.0000 - val_loss: 33328576.0000 - learning_rate: 0.0010\n",
      "Epoch 208/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34643368.0000 - val_loss: 33155762.0000 - learning_rate: 0.0010\n",
      "Epoch 209/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 34591072.0000 - val_loss: 33416528.0000 - learning_rate: 0.0010\n",
      "Epoch 210/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34456472.0000 - val_loss: 33301958.0000 - learning_rate: 0.0010\n",
      "Epoch 211/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34602748.0000 - val_loss: 33087054.0000 - learning_rate: 0.0010\n",
      "Epoch 212/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 34583548.0000 - val_loss: 33344568.0000 - learning_rate: 0.0010\n",
      "Epoch 213/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 34301840.0000 - val_loss: 33229388.0000 - learning_rate: 0.0010\n",
      "Epoch 214/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 34365800.0000 - val_loss: 33463712.0000 - learning_rate: 0.0010\n",
      "Epoch 215/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 34585032.0000 - val_loss: 33538516.0000 - learning_rate: 0.0010\n",
      "Epoch 216/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34246164.0000 - val_loss: 33813408.0000 - learning_rate: 0.0010\n",
      "Epoch 217/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 34073628.0000 - val_loss: 33535752.0000 - learning_rate: 0.0010\n",
      "Epoch 218/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 34111784.0000 - val_loss: 34164948.0000 - learning_rate: 0.0010\n",
      "Epoch 219/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 34275560.0000 - val_loss: 33998904.0000 - learning_rate: 0.0010\n",
      "Epoch 220/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 34138708.0000 - val_loss: 33648276.0000 - learning_rate: 0.0010\n",
      "Epoch 221/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 34329980.0000 - val_loss: 34044160.0000 - learning_rate: 0.0010\n",
      "Epoch 222/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 34002312.0000 - val_loss: 33814912.0000 - learning_rate: 0.0010\n",
      "Epoch 223/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33830892.0000 - val_loss: 33734892.0000 - learning_rate: 0.0010\n",
      "Epoch 224/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 34043608.0000 - val_loss: 33910988.0000 - learning_rate: 0.0010\n",
      "Epoch 225/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 34031572.0000 - val_loss: 34769680.0000 - learning_rate: 0.0010\n",
      "Epoch 226/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34017776.0000 - val_loss: 34518228.0000 - learning_rate: 0.0010\n",
      "Epoch 227/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33993620.0000 - val_loss: 34111788.0000 - learning_rate: 0.0010\n",
      "Epoch 228/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 33893188.0000 - val_loss: 34344604.0000 - learning_rate: 0.0010\n",
      "Epoch 229/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441us/step - loss: 33781984.0000 - val_loss: 34369484.0000 - learning_rate: 0.0010\n",
      "Epoch 230/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33757724.0000 - val_loss: 34832376.0000 - learning_rate: 0.0010\n",
      "Epoch 231/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33890792.0000 - val_loss: 34200236.0000 - learning_rate: 0.0010\n",
      "Epoch 232/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 34021216.0000 - val_loss: 34229280.0000 - learning_rate: 0.0010\n",
      "Epoch 233/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33927584.0000 - val_loss: 34393340.0000 - learning_rate: 0.0010\n",
      "Epoch 234/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33853208.0000 - val_loss: 34164836.0000 - learning_rate: 0.0010\n",
      "Epoch 235/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 33868948.0000 - val_loss: 33894476.0000 - learning_rate: 0.0010\n",
      "Epoch 236/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 34015400.0000 - val_loss: 34454396.0000 - learning_rate: 0.0010\n",
      "Epoch 237/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33737848.0000 - val_loss: 34794080.0000 - learning_rate: 0.0010\n",
      "Epoch 238/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33906028.0000 - val_loss: 34675440.0000 - learning_rate: 0.0010\n",
      "Epoch 239/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33969204.0000 - val_loss: 34833676.0000 - learning_rate: 0.0010\n",
      "Epoch 240/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 34006272.0000 - val_loss: 35076228.0000 - learning_rate: 0.0010\n",
      "Epoch 241/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33984332.0000 - val_loss: 34103216.0000 - learning_rate: 0.0010\n",
      "Epoch 242/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33741000.0000 - val_loss: 33893676.0000 - learning_rate: 0.0010\n",
      "Epoch 243/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33701648.0000 - val_loss: 34618584.0000 - learning_rate: 0.0010\n",
      "Epoch 244/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33851756.0000 - val_loss: 34628256.0000 - learning_rate: 0.0010\n",
      "Epoch 245/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33794648.0000 - val_loss: 35017752.0000 - learning_rate: 0.0010\n",
      "Epoch 246/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 33743780.0000 - val_loss: 34092840.0000 - learning_rate: 0.0010\n",
      "Epoch 247/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33772140.0000 - val_loss: 35195780.0000 - learning_rate: 0.0010\n",
      "Epoch 248/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33862448.0000 - val_loss: 34518304.0000 - learning_rate: 0.0010\n",
      "Epoch 249/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33851044.0000 - val_loss: 35520008.0000 - learning_rate: 0.0010\n",
      "Epoch 250/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33834572.0000 - val_loss: 34687984.0000 - learning_rate: 0.0010\n",
      "Epoch 251/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33828608.0000 - val_loss: 34885636.0000 - learning_rate: 0.0010\n",
      "Epoch 252/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33923328.0000 - val_loss: 34243324.0000 - learning_rate: 0.0010\n",
      "Epoch 253/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 33785644.0000 - val_loss: 34704168.0000 - learning_rate: 0.0010\n",
      "Epoch 254/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33687052.0000 - val_loss: 35394624.0000 - learning_rate: 0.0010\n",
      "Epoch 255/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33729552.0000 - val_loss: 34851768.0000 - learning_rate: 0.0010\n",
      "Epoch 256/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33809356.0000 - val_loss: 34444440.0000 - learning_rate: 0.0010\n",
      "Epoch 257/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 33759416.0000 - val_loss: 34693332.0000 - learning_rate: 0.0010\n",
      "Epoch 258/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 33758184.0000 - val_loss: 34582904.0000 - learning_rate: 0.0010\n",
      "Epoch 259/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 33876204.0000 - val_loss: 34451508.0000 - learning_rate: 0.0010\n",
      "Epoch 260/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33901720.0000 - val_loss: 34785268.0000 - learning_rate: 0.0010\n",
      "Epoch 261/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33461018.0000 - val_loss: 34974560.0000 - learning_rate: 0.0010\n",
      "Epoch 262/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33899280.0000 - val_loss: 34924068.0000 - learning_rate: 0.0010\n",
      "Epoch 263/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33808996.0000 - val_loss: 34173484.0000 - learning_rate: 0.0010\n",
      "Epoch 264/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33883864.0000 - val_loss: 34719956.0000 - learning_rate: 0.0010\n",
      "Epoch 265/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 33646028.0000 - val_loss: 35103720.0000 - learning_rate: 0.0010\n",
      "Epoch 266/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33842924.0000 - val_loss: 35138852.0000 - learning_rate: 0.0010\n",
      "Epoch 267/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33721052.0000 - val_loss: 34823336.0000 - learning_rate: 0.0010\n",
      "Epoch 268/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 33870616.0000 - val_loss: 34834248.0000 - learning_rate: 0.0010\n",
      "Epoch 269/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33833196.0000 - val_loss: 33671876.0000 - learning_rate: 0.0010\n",
      "Epoch 270/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33851360.0000 - val_loss: 34222300.0000 - learning_rate: 0.0010\n",
      "Epoch 271/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33980044.0000 - val_loss: 34835796.0000 - learning_rate: 0.0010\n",
      "Epoch 272/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33667400.0000 - val_loss: 34522888.0000 - learning_rate: 0.0010\n",
      "Epoch 273/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33857752.0000 - val_loss: 35568824.0000 - learning_rate: 0.0010\n",
      "Epoch 274/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33894784.0000 - val_loss: 34742304.0000 - learning_rate: 0.0010\n",
      "Epoch 275/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33685192.0000 - val_loss: 34394744.0000 - learning_rate: 0.0010\n",
      "Epoch 276/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33879728.0000 - val_loss: 34965296.0000 - learning_rate: 0.0010\n",
      "Epoch 277/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 33696296.0000 - val_loss: 34622220.0000 - learning_rate: 0.0010\n",
      "Epoch 278/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33823440.0000 - val_loss: 34085468.0000 - learning_rate: 0.0010\n",
      "Epoch 279/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33733660.0000 - val_loss: 34466744.0000 - learning_rate: 0.0010\n",
      "Epoch 280/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33624044.0000 - val_loss: 34402744.0000 - learning_rate: 0.0010\n",
      "Epoch 281/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33731588.0000 - val_loss: 34347072.0000 - learning_rate: 0.0010\n",
      "Epoch 282/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 33726436.0000 - val_loss: 34761372.0000 - learning_rate: 0.0010\n",
      "Epoch 283/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33766876.0000 - val_loss: 34509048.0000 - learning_rate: 0.0010\n",
      "Epoch 284/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33724456.0000 - val_loss: 34590156.0000 - learning_rate: 0.0010\n",
      "Epoch 285/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33819936.0000 - val_loss: 33908688.0000 - learning_rate: 0.0010\n",
      "Epoch 286/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33834052.0000 - val_loss: 34187432.0000 - learning_rate: 0.0010\n",
      "Epoch 287/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33807196.0000 - val_loss: 34417220.0000 - learning_rate: 0.0010\n",
      "Epoch 288/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33702976.0000 - val_loss: 34334600.0000 - learning_rate: 0.0010\n",
      "Epoch 289/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 33870160.0000 - val_loss: 35099540.0000 - learning_rate: 0.0010\n",
      "Epoch 290/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33702452.0000 - val_loss: 34625100.0000 - learning_rate: 0.0010\n",
      "Epoch 291/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33931448.0000 - val_loss: 35191916.0000 - learning_rate: 0.0010\n",
      "Epoch 292/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33817784.0000 - val_loss: 34992344.0000 - learning_rate: 0.0010\n",
      "Epoch 293/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 33863204.0000 - val_loss: 34257076.0000 - learning_rate: 0.0010\n",
      "Epoch 294/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33725976.0000 - val_loss: 34605088.0000 - learning_rate: 0.0010\n",
      "Epoch 295/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33670660.0000 - val_loss: 34035716.0000 - learning_rate: 0.0010\n",
      "Epoch 296/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 33860736.0000 - val_loss: 34284824.0000 - learning_rate: 0.0010\n",
      "Epoch 297/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33765416.0000 - val_loss: 34542236.0000 - learning_rate: 0.0010\n",
      "Epoch 298/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33702612.0000 - val_loss: 35126316.0000 - learning_rate: 0.0010\n",
      "Epoch 299/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33644968.0000 - val_loss: 34226976.0000 - learning_rate: 0.0010\n",
      "Epoch 300/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33660100.0000 - val_loss: 35195316.0000 - learning_rate: 0.0010\n",
      "Epoch 301/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33891772.0000 - val_loss: 34322644.0000 - learning_rate: 0.0010\n",
      "Epoch 302/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 33921368.0000 - val_loss: 34337404.0000 - learning_rate: 0.0010\n",
      "Epoch 303/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33896836.0000 - val_loss: 35128768.0000 - learning_rate: 0.0010\n",
      "Epoch 304/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 33663536.0000 - val_loss: 34437508.0000 - learning_rate: 0.0010\n",
      "Epoch 305/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33733144.0000 - val_loss: 34153264.0000 - learning_rate: 0.0010\n",
      "Epoch 306/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 33816568.0000 - val_loss: 34267792.0000 - learning_rate: 0.0010\n",
      "Epoch 307/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33614280.0000 - val_loss: 34312176.0000 - learning_rate: 0.0010\n",
      "Epoch 308/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33979772.0000 - val_loss: 34388772.0000 - learning_rate: 0.0010\n",
      "Epoch 309/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33772960.0000 - val_loss: 34067064.0000 - learning_rate: 0.0010\n",
      "Epoch 310/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33662912.0000 - val_loss: 34006344.0000 - learning_rate: 0.0010\n",
      "Epoch 311/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33874636.0000 - val_loss: 34589412.0000 - learning_rate: 0.0010\n",
      "Epoch 312/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33809732.0000 - val_loss: 34548124.0000 - learning_rate: 0.0010\n",
      "Epoch 313/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 33668092.0000 - val_loss: 34272956.0000 - learning_rate: 0.0010\n",
      "Epoch 314/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33790172.0000 - val_loss: 34496548.0000 - learning_rate: 0.0010\n",
      "Epoch 315/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33812440.0000 - val_loss: 34319544.0000 - learning_rate: 0.0010\n",
      "Epoch 316/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33866308.0000 - val_loss: 34602660.0000 - learning_rate: 0.0010\n",
      "Epoch 317/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33731812.0000 - val_loss: 34493512.0000 - learning_rate: 0.0010\n",
      "Epoch 318/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33478370.0000 - val_loss: 34193448.0000 - learning_rate: 0.0010\n",
      "Epoch 319/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33723532.0000 - val_loss: 34041428.0000 - learning_rate: 0.0010\n",
      "Epoch 320/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33766100.0000 - val_loss: 34471848.0000 - learning_rate: 0.0010\n",
      "Epoch 321/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33629536.0000 - val_loss: 34949528.0000 - learning_rate: 0.0010\n",
      "Epoch 322/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33560736.0000 - val_loss: 34720240.0000 - learning_rate: 0.0010\n",
      "Epoch 323/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33763564.0000 - val_loss: 34587452.0000 - learning_rate: 0.0010\n",
      "Epoch 324/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33767980.0000 - val_loss: 34539648.0000 - learning_rate: 0.0010\n",
      "Epoch 325/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33599904.0000 - val_loss: 34729248.0000 - learning_rate: 0.0010\n",
      "Epoch 326/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33604520.0000 - val_loss: 34863140.0000 - learning_rate: 0.0010\n",
      "Epoch 327/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33706788.0000 - val_loss: 33865148.0000 - learning_rate: 0.0010\n",
      "Epoch 328/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33760936.0000 - val_loss: 34337920.0000 - learning_rate: 0.0010\n",
      "Epoch 329/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33766096.0000 - val_loss: 33989960.0000 - learning_rate: 0.0010\n",
      "Epoch 330/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33798556.0000 - val_loss: 34324568.0000 - learning_rate: 0.0010\n",
      "Epoch 331/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33632228.0000 - val_loss: 34313340.0000 - learning_rate: 0.0010\n",
      "Epoch 332/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33805256.0000 - val_loss: 34501332.0000 - learning_rate: 0.0010\n",
      "Epoch 333/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33722720.0000 - val_loss: 33635080.0000 - learning_rate: 0.0010\n",
      "Epoch 334/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33621832.0000 - val_loss: 34081544.0000 - learning_rate: 0.0010\n",
      "Epoch 335/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33898292.0000 - val_loss: 34755312.0000 - learning_rate: 0.0010\n",
      "Epoch 336/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33694232.0000 - val_loss: 34202592.0000 - learning_rate: 0.0010\n",
      "Epoch 337/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33568828.0000 - val_loss: 34463592.0000 - learning_rate: 0.0010\n",
      "Epoch 338/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 33561504.0000 - val_loss: 33723636.0000 - learning_rate: 0.0010\n",
      "Epoch 339/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33618184.0000 - val_loss: 34551232.0000 - learning_rate: 0.0010\n",
      "Epoch 340/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33617692.0000 - val_loss: 34382620.0000 - learning_rate: 0.0010\n",
      "Epoch 341/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33716320.0000 - val_loss: 34367368.0000 - learning_rate: 0.0010\n",
      "Epoch 342/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33559164.0000 - val_loss: 34108712.0000 - learning_rate: 0.0010\n",
      "Epoch 343/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33795184.0000 - val_loss: 34247800.0000 - learning_rate: 0.0010\n",
      "Epoch 344/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33693564.0000 - val_loss: 34518272.0000 - learning_rate: 0.0010\n",
      "Epoch 345/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33682768.0000 - val_loss: 33925352.0000 - learning_rate: 0.0010\n",
      "Epoch 346/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33818904.0000 - val_loss: 34597348.0000 - learning_rate: 0.0010\n",
      "Epoch 347/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33625236.0000 - val_loss: 34177880.0000 - learning_rate: 0.0010\n",
      "Epoch 348/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 33611168.0000 - val_loss: 34141912.0000 - learning_rate: 0.0010\n",
      "Epoch 349/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33843656.0000 - val_loss: 34353644.0000 - learning_rate: 0.0010\n",
      "Epoch 350/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33711952.0000 - val_loss: 34870016.0000 - learning_rate: 0.0010\n",
      "Epoch 351/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33770228.0000 - val_loss: 34976332.0000 - learning_rate: 0.0010\n",
      "Epoch 352/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33508490.0000 - val_loss: 34227472.0000 - learning_rate: 0.0010\n",
      "Epoch 353/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 33649708.0000 - val_loss: 34125396.0000 - learning_rate: 0.0010\n",
      "Epoch 354/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33600856.0000 - val_loss: 34030556.0000 - learning_rate: 0.0010\n",
      "Epoch 355/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33546106.0000 - val_loss: 33989244.0000 - learning_rate: 0.0010\n",
      "Epoch 356/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33791164.0000 - val_loss: 34155520.0000 - learning_rate: 0.0010\n",
      "Epoch 357/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33675164.0000 - val_loss: 34686552.0000 - learning_rate: 0.0010\n",
      "Epoch 358/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33405624.0000 - val_loss: 33940080.0000 - learning_rate: 0.0010\n",
      "Epoch 359/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33755500.0000 - val_loss: 33959752.0000 - learning_rate: 0.0010\n",
      "Epoch 360/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33789612.0000 - val_loss: 34135552.0000 - learning_rate: 0.0010\n",
      "Epoch 361/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33655160.0000 - val_loss: 34186116.0000 - learning_rate: 0.0010\n",
      "Epoch 362/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33783736.0000 - val_loss: 34306776.0000 - learning_rate: 0.0010\n",
      "Epoch 363/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33753536.0000 - val_loss: 34034896.0000 - learning_rate: 0.0010\n",
      "Epoch 364/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33576660.0000 - val_loss: 34357596.0000 - learning_rate: 0.0010\n",
      "Epoch 365/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33554072.0000 - val_loss: 34503336.0000 - learning_rate: 0.0010\n",
      "Epoch 366/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33716872.0000 - val_loss: 33893816.0000 - learning_rate: 0.0010\n",
      "Epoch 367/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33757364.0000 - val_loss: 34500500.0000 - learning_rate: 0.0010\n",
      "Epoch 368/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33723032.0000 - val_loss: 34145348.0000 - learning_rate: 0.0010\n",
      "Epoch 369/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33753884.0000 - val_loss: 34032156.0000 - learning_rate: 0.0010\n",
      "Epoch 370/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33750156.0000 - val_loss: 34419140.0000 - learning_rate: 0.0010\n",
      "Epoch 371/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33794400.0000 - val_loss: 34827244.0000 - learning_rate: 0.0010\n",
      "Epoch 372/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33613176.0000 - val_loss: 33827892.0000 - learning_rate: 0.0010\n",
      "Epoch 373/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33600412.0000 - val_loss: 33800792.0000 - learning_rate: 0.0010\n",
      "Epoch 374/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33745888.0000 - val_loss: 33983420.0000 - learning_rate: 0.0010\n",
      "Epoch 375/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33682952.0000 - val_loss: 34626512.0000 - learning_rate: 0.0010\n",
      "Epoch 376/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33623600.0000 - val_loss: 33977428.0000 - learning_rate: 0.0010\n",
      "Epoch 377/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33657064.0000 - val_loss: 34256056.0000 - learning_rate: 0.0010\n",
      "Epoch 378/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33669072.0000 - val_loss: 33741268.0000 - learning_rate: 0.0010\n",
      "Epoch 379/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 33720000.0000 - val_loss: 33769280.0000 - learning_rate: 0.0010\n",
      "Epoch 380/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33864204.0000 - val_loss: 34273896.0000 - learning_rate: 0.0010\n",
      "Epoch 381/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33606900.0000 - val_loss: 34115444.0000 - learning_rate: 0.0010\n",
      "Epoch 382/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33728856.0000 - val_loss: 34395240.0000 - learning_rate: 0.0010\n",
      "Epoch 383/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33769304.0000 - val_loss: 34246336.0000 - learning_rate: 0.0010\n",
      "Epoch 384/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33785680.0000 - val_loss: 34449288.0000 - learning_rate: 0.0010\n",
      "Epoch 385/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33562404.0000 - val_loss: 34126128.0000 - learning_rate: 0.0010\n",
      "Epoch 386/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 33635160.0000 - val_loss: 34531280.0000 - learning_rate: 0.0010\n",
      "Epoch 387/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33622504.0000 - val_loss: 34561900.0000 - learning_rate: 0.0010\n",
      "Epoch 388/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33686868.0000 - val_loss: 34092792.0000 - learning_rate: 0.0010\n",
      "Epoch 389/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33700872.0000 - val_loss: 34005968.0000 - learning_rate: 0.0010\n",
      "Epoch 390/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33677648.0000 - val_loss: 34499992.0000 - learning_rate: 0.0010\n",
      "Epoch 391/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33599824.0000 - val_loss: 33511328.0000 - learning_rate: 0.0010\n",
      "Epoch 392/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33765940.0000 - val_loss: 33987076.0000 - learning_rate: 0.0010\n",
      "Epoch 393/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33559924.0000 - val_loss: 34194740.0000 - learning_rate: 0.0010\n",
      "Epoch 394/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33602276.0000 - val_loss: 34160624.0000 - learning_rate: 0.0010\n",
      "Epoch 395/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407us/step - loss: 33620504.0000 - val_loss: 33606720.0000 - learning_rate: 0.0010\n",
      "Epoch 396/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33660960.0000 - val_loss: 34228044.0000 - learning_rate: 0.0010\n",
      "Epoch 397/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33778116.0000 - val_loss: 34284360.0000 - learning_rate: 0.0010\n",
      "Epoch 398/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 33657700.0000 - val_loss: 33578548.0000 - learning_rate: 0.0010\n",
      "Epoch 399/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33596776.0000 - val_loss: 34261396.0000 - learning_rate: 0.0010\n",
      "Epoch 400/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 33549196.0000 - val_loss: 34261348.0000 - learning_rate: 0.0010\n",
      "Epoch 401/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 33590516.0000 - val_loss: 34168456.0000 - learning_rate: 0.0010\n",
      "Epoch 402/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33582356.0000 - val_loss: 33611740.0000 - learning_rate: 0.0010\n",
      "Epoch 403/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33563304.0000 - val_loss: 33545122.0000 - learning_rate: 0.0010\n",
      "Epoch 404/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33819288.0000 - val_loss: 33553046.0000 - learning_rate: 0.0010\n",
      "Epoch 405/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33823848.0000 - val_loss: 34285988.0000 - learning_rate: 0.0010\n",
      "Epoch 406/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33671296.0000 - val_loss: 34110376.0000 - learning_rate: 0.0010\n",
      "Epoch 407/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33552848.0000 - val_loss: 33933796.0000 - learning_rate: 0.0010\n",
      "Epoch 408/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33665872.0000 - val_loss: 34036948.0000 - learning_rate: 0.0010\n",
      "Epoch 409/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33772900.0000 - val_loss: 33768880.0000 - learning_rate: 0.0010\n",
      "Epoch 410/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33656488.0000 - val_loss: 33738492.0000 - learning_rate: 0.0010\n",
      "Epoch 411/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33556996.0000 - val_loss: 33771152.0000 - learning_rate: 0.0010\n",
      "Epoch 412/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33653568.0000 - val_loss: 34646452.0000 - learning_rate: 0.0010\n",
      "Epoch 413/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33783528.0000 - val_loss: 33855412.0000 - learning_rate: 0.0010\n",
      "Epoch 414/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33603148.0000 - val_loss: 33575100.0000 - learning_rate: 0.0010\n",
      "Epoch 415/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33611172.0000 - val_loss: 34053620.0000 - learning_rate: 0.0010\n",
      "Epoch 416/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33527344.0000 - val_loss: 34212276.0000 - learning_rate: 0.0010\n",
      "Epoch 417/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 33751796.0000 - val_loss: 33651148.0000 - learning_rate: 0.0010\n",
      "Epoch 418/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33597288.0000 - val_loss: 33942632.0000 - learning_rate: 0.0010\n",
      "Epoch 419/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33726436.0000 - val_loss: 33665212.0000 - learning_rate: 0.0010\n",
      "Epoch 420/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33468952.0000 - val_loss: 33648368.0000 - learning_rate: 0.0010\n",
      "Epoch 421/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407us/step - loss: 33737824.0000 - val_loss: 33792684.0000 - learning_rate: 0.0010\n",
      "Epoch 422/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 33792568.0000 - val_loss: 33877324.0000 - learning_rate: 0.0010\n",
      "Epoch 423/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418us/step - loss: 33491246.0000 - val_loss: 33940140.0000 - learning_rate: 0.0010\n",
      "Epoch 424/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 33527824.0000 - val_loss: 33878636.0000 - learning_rate: 0.0010\n",
      "Epoch 425/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33647284.0000 - val_loss: 33421226.0000 - learning_rate: 0.0010\n",
      "Epoch 426/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 33567316.0000 - val_loss: 33770752.0000 - learning_rate: 0.0010\n",
      "Epoch 427/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 421us/step - loss: 33586588.0000 - val_loss: 33608392.0000 - learning_rate: 0.0010\n",
      "Epoch 428/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 425us/step - loss: 33644752.0000 - val_loss: 33730120.0000 - learning_rate: 0.0010\n",
      "Epoch 429/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 33664872.0000 - val_loss: 34232748.0000 - learning_rate: 0.0010\n",
      "Epoch 430/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 33479014.0000 - val_loss: 33757972.0000 - learning_rate: 0.0010\n",
      "Epoch 431/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417us/step - loss: 33666804.0000 - val_loss: 34613980.0000 - learning_rate: 0.0010\n",
      "Epoch 432/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33499820.0000 - val_loss: 33934576.0000 - learning_rate: 0.0010\n",
      "Epoch 433/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33610492.0000 - val_loss: 34016132.0000 - learning_rate: 0.0010\n",
      "Epoch 434/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 33567408.0000 - val_loss: 33561136.0000 - learning_rate: 0.0010\n",
      "Epoch 435/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33582404.0000 - val_loss: 33679580.0000 - learning_rate: 0.0010\n",
      "Epoch 436/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 33633632.0000 - val_loss: 33841980.0000 - learning_rate: 0.0010\n",
      "Epoch 437/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33524792.0000 - val_loss: 33800068.0000 - learning_rate: 0.0010\n",
      "Epoch 438/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33599884.0000 - val_loss: 33654580.0000 - learning_rate: 0.0010\n",
      "Epoch 439/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33698192.0000 - val_loss: 33790996.0000 - learning_rate: 0.0010\n",
      "Epoch 440/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33575824.0000 - val_loss: 34031176.0000 - learning_rate: 0.0010\n",
      "Epoch 441/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33705820.0000 - val_loss: 33479340.0000 - learning_rate: 0.0010\n",
      "Epoch 442/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 33615704.0000 - val_loss: 33808284.0000 - learning_rate: 0.0010\n",
      "Epoch 443/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 33520704.0000 - val_loss: 33770260.0000 - learning_rate: 0.0010\n",
      "Epoch 444/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33552172.0000 - val_loss: 33866088.0000 - learning_rate: 0.0010\n",
      "Epoch 445/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33543570.0000 - val_loss: 33529092.0000 - learning_rate: 0.0010\n",
      "Epoch 446/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33560284.0000 - val_loss: 33520964.0000 - learning_rate: 0.0010\n",
      "Epoch 447/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33603952.0000 - val_loss: 34116300.0000 - learning_rate: 0.0010\n",
      "Epoch 448/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33592452.0000 - val_loss: 33430298.0000 - learning_rate: 0.0010\n",
      "Epoch 449/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33528298.0000 - val_loss: 33470586.0000 - learning_rate: 0.0010\n",
      "Epoch 450/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33412678.0000 - val_loss: 33314436.0000 - learning_rate: 0.0010\n",
      "Epoch 451/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 33588156.0000 - val_loss: 33813608.0000 - learning_rate: 0.0010\n",
      "Epoch 452/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33625960.0000 - val_loss: 34023672.0000 - learning_rate: 0.0010\n",
      "Epoch 453/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33625400.0000 - val_loss: 33410652.0000 - learning_rate: 0.0010\n",
      "Epoch 454/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33605544.0000 - val_loss: 33706056.0000 - learning_rate: 0.0010\n",
      "Epoch 455/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33574784.0000 - val_loss: 33746864.0000 - learning_rate: 0.0010\n",
      "Epoch 456/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 33488890.0000 - val_loss: 33796696.0000 - learning_rate: 0.0010\n",
      "Epoch 457/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33708708.0000 - val_loss: 34001452.0000 - learning_rate: 0.0010\n",
      "Epoch 458/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33413726.0000 - val_loss: 33584780.0000 - learning_rate: 0.0010\n",
      "Epoch 459/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33650380.0000 - val_loss: 33576864.0000 - learning_rate: 0.0010\n",
      "Epoch 460/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 33529942.0000 - val_loss: 33891520.0000 - learning_rate: 0.0010\n",
      "Epoch 461/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33450716.0000 - val_loss: 33977896.0000 - learning_rate: 0.0010\n",
      "Epoch 462/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33483986.0000 - val_loss: 33641248.0000 - learning_rate: 0.0010\n",
      "Epoch 463/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33337100.0000 - val_loss: 33681332.0000 - learning_rate: 0.0010\n",
      "Epoch 464/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 33349812.0000 - val_loss: 34039512.0000 - learning_rate: 0.0010\n",
      "Epoch 465/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33239636.0000 - val_loss: 34081084.0000 - learning_rate: 0.0010\n",
      "Epoch 466/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 33406364.0000 - val_loss: 33534586.0000 - learning_rate: 0.0010\n",
      "Epoch 467/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33514998.0000 - val_loss: 34283040.0000 - learning_rate: 0.0010\n",
      "Epoch 468/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33523564.0000 - val_loss: 34284380.0000 - learning_rate: 0.0010\n",
      "Epoch 469/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33344896.0000 - val_loss: 33634004.0000 - learning_rate: 0.0010\n",
      "Epoch 470/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33323536.0000 - val_loss: 33723664.0000 - learning_rate: 0.0010\n",
      "Epoch 471/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33415346.0000 - val_loss: 33567544.0000 - learning_rate: 0.0010\n",
      "Epoch 472/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 33324786.0000 - val_loss: 33861840.0000 - learning_rate: 0.0010\n",
      "Epoch 473/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33364668.0000 - val_loss: 33493356.0000 - learning_rate: 0.0010\n",
      "Epoch 474/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33273440.0000 - val_loss: 33193988.0000 - learning_rate: 0.0010\n",
      "Epoch 475/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33188144.0000 - val_loss: 33220284.0000 - learning_rate: 0.0010\n",
      "Epoch 476/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 33372416.0000 - val_loss: 33281514.0000 - learning_rate: 0.0010\n",
      "Epoch 477/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33325810.0000 - val_loss: 32825420.0000 - learning_rate: 0.0010\n",
      "Epoch 478/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33094388.0000 - val_loss: 33060300.0000 - learning_rate: 0.0010\n",
      "Epoch 479/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 33062908.0000 - val_loss: 32788836.0000 - learning_rate: 0.0010\n",
      "Epoch 480/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33041006.0000 - val_loss: 32971030.0000 - learning_rate: 0.0010\n",
      "Epoch 481/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33195600.0000 - val_loss: 32418350.0000 - learning_rate: 0.0010\n",
      "Epoch 482/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 33005164.0000 - val_loss: 32686778.0000 - learning_rate: 0.0010\n",
      "Epoch 483/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 33063452.0000 - val_loss: 32957390.0000 - learning_rate: 0.0010\n",
      "Epoch 484/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 32915760.0000 - val_loss: 32711412.0000 - learning_rate: 0.0010\n",
      "Epoch 485/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 33154616.0000 - val_loss: 32642402.0000 - learning_rate: 0.0010\n",
      "Epoch 486/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 33082122.0000 - val_loss: 32604530.0000 - learning_rate: 0.0010\n",
      "Epoch 487/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 32961760.0000 - val_loss: 32874882.0000 - learning_rate: 0.0010\n",
      "Epoch 488/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33040582.0000 - val_loss: 32969606.0000 - learning_rate: 0.0010\n",
      "Epoch 489/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33156528.0000 - val_loss: 32990814.0000 - learning_rate: 0.0010\n",
      "Epoch 490/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 33096442.0000 - val_loss: 32600130.0000 - learning_rate: 0.0010\n",
      "Epoch 491/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 33016116.0000 - val_loss: 32566036.0000 - learning_rate: 0.0010\n",
      "Epoch 492/800\n",
      "\u001b[1m3011/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 32936302.0000"
     ]
    }
   ],
   "source": [
    "# Number of input features (after dropping the target column)\n",
    "variable_amount = len(X.columns)\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "mc = ModelCheckpoint('best_model_regression1.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# ReduceLROnPlateau to reduce learning rate when the validation loss has stopped improving\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)\n",
    "\n",
    "# Combine all active callbacks into a list (excluding EarlyStopping)\n",
    "callback_list = [mc, rlr]\n",
    "\n",
    "# Define Sequential neural network model with Dropout layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(variable_amount,)),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1)  # Output layer for regression (1 node, no activation function)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model with Adam optimizer and Mean Squared Error loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print out the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model with the dataset and apply the callbacks\n",
    "history = model.fit(x=X_train, y=y_train, epochs=800, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520168d0-4995-4c49-ac40-b9aa20e6227b",
   "metadata": {},
   "source": [
    "## Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21e55e-1def-4103-94b3-35f1010f39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loss over epochs \n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()\n",
    "\n",
    "# insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1399850-0217-4fff-aec2-75ad85a7c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the final model loss/evaluation values\n",
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))\n",
    "\n",
    "# the model is often good when these error values are similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ecb2b-d848-4d1f-a1c7-da799d71bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df\n",
    "\n",
    "# insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b1ef-8a24-4721-b0c4-c394936dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values follow a linear line = good predictions\n",
    "# we basically compare the predicted values \n",
    "# to true test values and see the differences\n",
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)\n",
    "\n",
    "# the same picture as in the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16462457-2dca-48f4-a391-2714d2310e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean average error\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"$\")\n",
    "\n",
    "# MSE - Mean square error\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"$^2\")\n",
    "\n",
    "# RMSE - Root mean square error\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"$\")\n",
    "\n",
    "# R-squared. 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "# Explained Variance Score => 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "# high variance score = model is a good fit for the data \n",
    "# low variance score = model is not a good fit for the data\n",
    "# the higher the score, the model is more able to explain the variation in the data\n",
    "# if score is low, we might need more and better data\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))\n",
    "\n",
    "\n",
    "# MAE\n",
    "# Lower is better\n",
    "# Original is 4841.92 $\n",
    "# Now is \n",
    "# almost the same result\n",
    "\n",
    "# MSE\n",
    "# Lower is better\n",
    "# Original is 34666769.92 $^2\n",
    "# Now is \n",
    "# almost the same\n",
    "\n",
    "# RMSE\n",
    "# Lower is better\n",
    "# Original is 5887.85 $\n",
    "# Now is \n",
    "# a little difference\n",
    "\n",
    "# R-squared\n",
    "# Closer to 1 is better\n",
    "# Original is 0.07\n",
    "# Now is\n",
    "# here we have a bit better results\n",
    "\n",
    "\n",
    "# Explained variance score\n",
    "# Closer to 1 is better\n",
    "# Original is 0.1\n",
    "# Now is \n",
    "\n",
    "# To sum up,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529c354-1706-4a69-ba5e-4509ab186122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5071aa3b-e75a-4e5c-8f0f-5b1e3e18c801",
   "metadata": {},
   "source": [
    "I noticed that when I used EarlyStop, it actually stopped the improvements of metrics.\n",
    "In this notebook I will try the combination of two techniques:\n",
    "- Dropout-layers\n",
    "- ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bda7e-a490-4d6e-853c-48f62cc83e46",
   "metadata": {},
   "source": [
    "## Uploading data, X-y variables, Train/test/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa87a3bd-b718-4220-b6a4-9ed9f8eea425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "057ad201-930c-4d1d-88f6-0ebf7f7195c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data\n",
    "df = pd.read_csv(\"balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2aede19-8838-438b-a671-f60ca82c982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave out the target variable! (dependent variable)\n",
    "X = df[['Year', 'Mileage', 'City', 'State', 'Make', 'Model']]\n",
    "\n",
    "# have only the target variable here (dependent variable)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd94c44-f805-4bd5-8466-e2b58d64e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, train/test split => 70% for training, 30% for other purposes (temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# now, split the 30% for other purposes by 50% (resulting in 2 x 15%)\n",
    "# so finally, we have:\n",
    "# 70% for training\n",
    "# 15% for testing\n",
    "# 15% for validation\n",
    "# => 70 + 15 +15 = 100%\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbcdab-5e8c-4fbe-83ac-b984206bcde2",
   "metadata": {},
   "source": [
    "## Development of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4a270-360f-4c47-9e05-1d37ef24c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiliiazemskova/anaconda3/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">281</span> (1.10 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m281\u001b[0m (1.10 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269</span> (1.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269\u001b[0m (1.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> (48.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12\u001b[0m (48.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 237022704.0000 - val_loss: 33026518.0000 - learning_rate: 0.0010\n",
      "Epoch 2/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36213196.0000 - val_loss: 33036956.0000 - learning_rate: 0.0010\n",
      "Epoch 3/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36198932.0000 - val_loss: 32978012.0000 - learning_rate: 0.0010\n",
      "Epoch 4/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 36196884.0000 - val_loss: 33102598.0000 - learning_rate: 0.0010\n",
      "Epoch 5/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 36161128.0000 - val_loss: 32988440.0000 - learning_rate: 0.0010\n",
      "Epoch 6/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 36047304.0000 - val_loss: 32991632.0000 - learning_rate: 0.0010\n",
      "Epoch 7/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35901868.0000 - val_loss: 32994414.0000 - learning_rate: 0.0010\n",
      "Epoch 8/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36048164.0000 - val_loss: 32999574.0000 - learning_rate: 0.0010\n",
      "Epoch 9/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36301036.0000 - val_loss: 33050346.0000 - learning_rate: 0.0010\n",
      "Epoch 10/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36407812.0000 - val_loss: 32994990.0000 - learning_rate: 0.0010\n",
      "Epoch 11/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 36250616.0000 - val_loss: 32977260.0000 - learning_rate: 0.0010\n",
      "Epoch 12/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36098148.0000 - val_loss: 33011758.0000 - learning_rate: 0.0010\n",
      "Epoch 13/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36261636.0000 - val_loss: 33011672.0000 - learning_rate: 0.0010\n",
      "Epoch 14/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36308424.0000 - val_loss: 33011560.0000 - learning_rate: 0.0010\n",
      "Epoch 15/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36149704.0000 - val_loss: 33052754.0000 - learning_rate: 0.0010\n",
      "Epoch 16/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35978900.0000 - val_loss: 33022986.0000 - learning_rate: 0.0010\n",
      "Epoch 17/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 36008984.0000 - val_loss: 33012542.0000 - learning_rate: 0.0010\n",
      "Epoch 18/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36156184.0000 - val_loss: 33015498.0000 - learning_rate: 0.0010\n",
      "Epoch 19/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35991928.0000 - val_loss: 33041410.0000 - learning_rate: 0.0010\n",
      "Epoch 20/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36121644.0000 - val_loss: 33084618.0000 - learning_rate: 0.0010\n",
      "Epoch 21/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36181408.0000 - val_loss: 33091678.0000 - learning_rate: 0.0010\n",
      "Epoch 22/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 36188364.0000 - val_loss: 32982812.0000 - learning_rate: 0.0010\n",
      "Epoch 23/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 35834868.0000 - val_loss: 32978042.0000 - learning_rate: 0.0010\n",
      "Epoch 24/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 35868700.0000 - val_loss: 32980390.0000 - learning_rate: 0.0010\n",
      "Epoch 25/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 36004780.0000 - val_loss: 33001046.0000 - learning_rate: 0.0010\n",
      "Epoch 26/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35925808.0000 - val_loss: 32995026.0000 - learning_rate: 0.0010\n",
      "Epoch 27/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 36070816.0000 - val_loss: 33000220.0000 - learning_rate: 0.0010\n",
      "Epoch 28/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 36004944.0000 - val_loss: 32967196.0000 - learning_rate: 0.0010\n",
      "Epoch 29/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36056256.0000 - val_loss: 32983396.0000 - learning_rate: 0.0010\n",
      "Epoch 30/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 36129620.0000 - val_loss: 32998218.0000 - learning_rate: 0.0010\n",
      "Epoch 31/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 36021764.0000 - val_loss: 32979580.0000 - learning_rate: 0.0010\n",
      "Epoch 32/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36034964.0000 - val_loss: 32994704.0000 - learning_rate: 0.0010\n",
      "Epoch 33/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36045080.0000 - val_loss: 33082718.0000 - learning_rate: 0.0010\n",
      "Epoch 34/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36038268.0000 - val_loss: 32988554.0000 - learning_rate: 0.0010\n",
      "Epoch 35/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36076288.0000 - val_loss: 33083420.0000 - learning_rate: 0.0010\n",
      "Epoch 36/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35960196.0000 - val_loss: 33064146.0000 - learning_rate: 0.0010\n",
      "Epoch 37/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35825748.0000 - val_loss: 33068144.0000 - learning_rate: 0.0010\n",
      "Epoch 38/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36085924.0000 - val_loss: 32985634.0000 - learning_rate: 0.0010\n",
      "Epoch 39/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36069888.0000 - val_loss: 33034360.0000 - learning_rate: 0.0010\n",
      "Epoch 40/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35913548.0000 - val_loss: 33004584.0000 - learning_rate: 0.0010\n",
      "Epoch 41/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35911600.0000 - val_loss: 33000444.0000 - learning_rate: 0.0010\n",
      "Epoch 42/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 36020264.0000 - val_loss: 32984634.0000 - learning_rate: 0.0010\n",
      "Epoch 43/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 35981316.0000 - val_loss: 32960994.0000 - learning_rate: 0.0010\n",
      "Epoch 44/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35717396.0000 - val_loss: 32969772.0000 - learning_rate: 0.0010\n",
      "Epoch 45/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 35874160.0000 - val_loss: 32979648.0000 - learning_rate: 0.0010\n",
      "Epoch 46/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36012024.0000 - val_loss: 32977792.0000 - learning_rate: 0.0010\n",
      "Epoch 47/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35997084.0000 - val_loss: 33002256.0000 - learning_rate: 0.0010\n",
      "Epoch 48/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35901600.0000 - val_loss: 32972688.0000 - learning_rate: 0.0010\n",
      "Epoch 49/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 36033112.0000 - val_loss: 32970760.0000 - learning_rate: 0.0010\n",
      "Epoch 50/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35779032.0000 - val_loss: 32969090.0000 - learning_rate: 0.0010\n",
      "Epoch 51/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35914780.0000 - val_loss: 32982104.0000 - learning_rate: 0.0010\n",
      "Epoch 52/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35997180.0000 - val_loss: 33050700.0000 - learning_rate: 0.0010\n",
      "Epoch 53/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35909152.0000 - val_loss: 33046084.0000 - learning_rate: 0.0010\n",
      "Epoch 54/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 36005992.0000 - val_loss: 33013108.0000 - learning_rate: 0.0010\n",
      "Epoch 55/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 409us/step - loss: 35976532.0000 - val_loss: 32976602.0000 - learning_rate: 0.0010\n",
      "Epoch 56/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35778656.0000 - val_loss: 33107752.0000 - learning_rate: 0.0010\n",
      "Epoch 57/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35985836.0000 - val_loss: 32999500.0000 - learning_rate: 0.0010\n",
      "Epoch 58/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36059804.0000 - val_loss: 33003940.0000 - learning_rate: 0.0010\n",
      "Epoch 59/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36174944.0000 - val_loss: 32984658.0000 - learning_rate: 0.0010\n",
      "Epoch 60/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 35985568.0000 - val_loss: 32964288.0000 - learning_rate: 0.0010\n",
      "Epoch 61/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 35958784.0000 - val_loss: 32993920.0000 - learning_rate: 0.0010\n",
      "Epoch 62/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 35888644.0000 - val_loss: 33115672.0000 - learning_rate: 0.0010\n",
      "Epoch 63/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 36192804.0000 - val_loss: 33178400.0000 - learning_rate: 0.0010\n",
      "Epoch 64/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 35760560.0000 - val_loss: 32981190.0000 - learning_rate: 0.0010\n",
      "Epoch 65/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35993204.0000 - val_loss: 33018912.0000 - learning_rate: 0.0010\n",
      "Epoch 66/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 36080228.0000 - val_loss: 33072388.0000 - learning_rate: 0.0010\n",
      "Epoch 67/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35646852.0000 - val_loss: 33020430.0000 - learning_rate: 0.0010\n",
      "Epoch 68/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 36060040.0000 - val_loss: 33050890.0000 - learning_rate: 0.0010\n",
      "Epoch 69/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 35994116.0000 - val_loss: 33067892.0000 - learning_rate: 0.0010\n",
      "Epoch 70/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 35636364.0000 - val_loss: 33016496.0000 - learning_rate: 0.0010\n",
      "Epoch 71/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35854408.0000 - val_loss: 32979778.0000 - learning_rate: 0.0010\n",
      "Epoch 72/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35821288.0000 - val_loss: 32983930.0000 - learning_rate: 0.0010\n",
      "Epoch 73/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 36040276.0000 - val_loss: 33024380.0000 - learning_rate: 0.0010\n",
      "Epoch 74/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 35957204.0000 - val_loss: 32974872.0000 - learning_rate: 0.0010\n",
      "Epoch 75/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35932336.0000 - val_loss: 33115638.0000 - learning_rate: 0.0010\n",
      "Epoch 76/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35736748.0000 - val_loss: 32990254.0000 - learning_rate: 0.0010\n",
      "Epoch 77/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35932484.0000 - val_loss: 32990524.0000 - learning_rate: 0.0010\n",
      "Epoch 78/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35718700.0000 - val_loss: 33011424.0000 - learning_rate: 0.0010\n",
      "Epoch 79/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35955084.0000 - val_loss: 32973520.0000 - learning_rate: 0.0010\n",
      "Epoch 80/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35988032.0000 - val_loss: 33022108.0000 - learning_rate: 0.0010\n",
      "Epoch 81/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35945016.0000 - val_loss: 33113838.0000 - learning_rate: 0.0010\n",
      "Epoch 82/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35947352.0000 - val_loss: 33020276.0000 - learning_rate: 0.0010\n",
      "Epoch 83/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35838308.0000 - val_loss: 32975666.0000 - learning_rate: 0.0010\n",
      "Epoch 84/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35873908.0000 - val_loss: 32973122.0000 - learning_rate: 0.0010\n",
      "Epoch 85/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35755772.0000 - val_loss: 32991964.0000 - learning_rate: 0.0010\n",
      "Epoch 86/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 36028672.0000 - val_loss: 32987192.0000 - learning_rate: 0.0010\n",
      "Epoch 87/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35885184.0000 - val_loss: 33013362.0000 - learning_rate: 0.0010\n",
      "Epoch 88/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35892524.0000 - val_loss: 32990292.0000 - learning_rate: 0.0010\n",
      "Epoch 89/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35847452.0000 - val_loss: 32972578.0000 - learning_rate: 0.0010\n",
      "Epoch 90/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35764000.0000 - val_loss: 32962024.0000 - learning_rate: 0.0010\n",
      "Epoch 91/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35792224.0000 - val_loss: 33162468.0000 - learning_rate: 0.0010\n",
      "Epoch 92/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35862416.0000 - val_loss: 33017266.0000 - learning_rate: 0.0010\n",
      "Epoch 93/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 36054068.0000 - val_loss: 33016046.0000 - learning_rate: 0.0010\n",
      "Epoch 94/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 36068612.0000 - val_loss: 32977210.0000 - learning_rate: 0.0010\n",
      "Epoch 95/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35831260.0000 - val_loss: 33030042.0000 - learning_rate: 0.0010\n",
      "Epoch 96/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35845280.0000 - val_loss: 32989516.0000 - learning_rate: 0.0010\n",
      "Epoch 97/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35906336.0000 - val_loss: 32988842.0000 - learning_rate: 0.0010\n",
      "Epoch 98/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35875636.0000 - val_loss: 32992392.0000 - learning_rate: 0.0010\n",
      "Epoch 99/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35919476.0000 - val_loss: 32985644.0000 - learning_rate: 0.0010\n",
      "Epoch 100/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 36030132.0000 - val_loss: 32976686.0000 - learning_rate: 0.0010\n",
      "Epoch 101/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35846164.0000 - val_loss: 32963074.0000 - learning_rate: 0.0010\n",
      "Epoch 102/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 35987828.0000 - val_loss: 32967324.0000 - learning_rate: 0.0010\n",
      "Epoch 103/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 413us/step - loss: 35928372.0000 - val_loss: 32968602.0000 - learning_rate: 0.0010\n",
      "Epoch 104/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 35882684.0000 - val_loss: 32967172.0000 - learning_rate: 0.0010\n",
      "Epoch 105/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35960696.0000 - val_loss: 33092698.0000 - learning_rate: 0.0010\n",
      "Epoch 106/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 36023120.0000 - val_loss: 32986922.0000 - learning_rate: 0.0010\n",
      "Epoch 107/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 36013360.0000 - val_loss: 32968390.0000 - learning_rate: 0.0010\n",
      "Epoch 108/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35768732.0000 - val_loss: 33018832.0000 - learning_rate: 0.0010\n",
      "Epoch 109/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35904144.0000 - val_loss: 33064358.0000 - learning_rate: 0.0010\n",
      "Epoch 110/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35641860.0000 - val_loss: 32997282.0000 - learning_rate: 0.0010\n",
      "Epoch 111/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35781404.0000 - val_loss: 33068806.0000 - learning_rate: 0.0010\n",
      "Epoch 112/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 35957024.0000 - val_loss: 32973502.0000 - learning_rate: 0.0010\n",
      "Epoch 113/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35734744.0000 - val_loss: 32979102.0000 - learning_rate: 0.0010\n",
      "Epoch 114/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35803024.0000 - val_loss: 32980080.0000 - learning_rate: 0.0010\n",
      "Epoch 115/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35862556.0000 - val_loss: 33025948.0000 - learning_rate: 0.0010\n",
      "Epoch 116/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35660088.0000 - val_loss: 33003960.0000 - learning_rate: 0.0010\n",
      "Epoch 117/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 413us/step - loss: 35786160.0000 - val_loss: 33006050.0000 - learning_rate: 0.0010\n",
      "Epoch 118/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35708564.0000 - val_loss: 32976492.0000 - learning_rate: 0.0010\n",
      "Epoch 119/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35862520.0000 - val_loss: 32990180.0000 - learning_rate: 0.0010\n",
      "Epoch 120/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35666424.0000 - val_loss: 33026134.0000 - learning_rate: 0.0010\n",
      "Epoch 121/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35751556.0000 - val_loss: 32989044.0000 - learning_rate: 0.0010\n",
      "Epoch 122/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35855624.0000 - val_loss: 33000432.0000 - learning_rate: 0.0010\n",
      "Epoch 123/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 35736924.0000 - val_loss: 32975052.0000 - learning_rate: 0.0010\n",
      "Epoch 124/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35780404.0000 - val_loss: 32975436.0000 - learning_rate: 0.0010\n",
      "Epoch 125/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35810032.0000 - val_loss: 32983054.0000 - learning_rate: 0.0010\n",
      "Epoch 126/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35770684.0000 - val_loss: 32968030.0000 - learning_rate: 0.0010\n",
      "Epoch 127/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35871412.0000 - val_loss: 32970582.0000 - learning_rate: 0.0010\n",
      "Epoch 128/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35758032.0000 - val_loss: 32973700.0000 - learning_rate: 0.0010\n",
      "Epoch 129/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35594788.0000 - val_loss: 32987172.0000 - learning_rate: 0.0010\n",
      "Epoch 130/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35785816.0000 - val_loss: 33017998.0000 - learning_rate: 0.0010\n",
      "Epoch 131/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35662060.0000 - val_loss: 33036522.0000 - learning_rate: 0.0010\n",
      "Epoch 132/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35626064.0000 - val_loss: 32991048.0000 - learning_rate: 0.0010\n",
      "Epoch 133/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 35649032.0000 - val_loss: 32978716.0000 - learning_rate: 0.0010\n",
      "Epoch 134/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 416us/step - loss: 35848280.0000 - val_loss: 32971376.0000 - learning_rate: 0.0010\n",
      "Epoch 135/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35824252.0000 - val_loss: 33028006.0000 - learning_rate: 0.0010\n",
      "Epoch 136/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35884800.0000 - val_loss: 32978426.0000 - learning_rate: 0.0010\n",
      "Epoch 137/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35640244.0000 - val_loss: 32976548.0000 - learning_rate: 0.0010\n",
      "Epoch 138/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35643480.0000 - val_loss: 32979782.0000 - learning_rate: 0.0010\n",
      "Epoch 139/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35735328.0000 - val_loss: 33038282.0000 - learning_rate: 0.0010\n",
      "Epoch 140/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 35734824.0000 - val_loss: 32996004.0000 - learning_rate: 0.0010\n",
      "Epoch 141/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35697056.0000 - val_loss: 32980164.0000 - learning_rate: 0.0010\n",
      "Epoch 142/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35673664.0000 - val_loss: 32985766.0000 - learning_rate: 0.0010\n",
      "Epoch 143/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35655968.0000 - val_loss: 33006172.0000 - learning_rate: 0.0010\n",
      "Epoch 144/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35507240.0000 - val_loss: 32972704.0000 - learning_rate: 0.0010\n",
      "Epoch 145/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35709856.0000 - val_loss: 32974388.0000 - learning_rate: 0.0010\n",
      "Epoch 146/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35614228.0000 - val_loss: 32966774.0000 - learning_rate: 0.0010\n",
      "Epoch 147/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35562556.0000 - val_loss: 32999122.0000 - learning_rate: 0.0010\n",
      "Epoch 148/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 35791032.0000 - val_loss: 32987330.0000 - learning_rate: 0.0010\n",
      "Epoch 149/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 414us/step - loss: 35707920.0000 - val_loss: 33140850.0000 - learning_rate: 0.0010\n",
      "Epoch 150/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35680712.0000 - val_loss: 32967154.0000 - learning_rate: 0.0010\n",
      "Epoch 151/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35756232.0000 - val_loss: 32985140.0000 - learning_rate: 0.0010\n",
      "Epoch 152/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35567720.0000 - val_loss: 32980138.0000 - learning_rate: 0.0010\n",
      "Epoch 153/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 35384160.0000 - val_loss: 32976676.0000 - learning_rate: 0.0010\n",
      "Epoch 154/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424us/step - loss: 35536396.0000 - val_loss: 32978420.0000 - learning_rate: 0.0010\n",
      "Epoch 155/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 35724120.0000 - val_loss: 32980528.0000 - learning_rate: 0.0010\n",
      "Epoch 156/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35750308.0000 - val_loss: 33027594.0000 - learning_rate: 0.0010\n",
      "Epoch 157/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35541916.0000 - val_loss: 32985770.0000 - learning_rate: 0.0010\n",
      "Epoch 158/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35782652.0000 - val_loss: 33028736.0000 - learning_rate: 0.0010\n",
      "Epoch 159/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 35641164.0000 - val_loss: 33054502.0000 - learning_rate: 0.0010\n",
      "Epoch 160/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35815124.0000 - val_loss: 32985926.0000 - learning_rate: 0.0010\n",
      "Epoch 161/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35640180.0000 - val_loss: 33137658.0000 - learning_rate: 0.0010\n",
      "Epoch 162/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35598928.0000 - val_loss: 32991014.0000 - learning_rate: 0.0010\n",
      "Epoch 163/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 35838756.0000 - val_loss: 33044706.0000 - learning_rate: 0.0010\n",
      "Epoch 164/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 35613452.0000 - val_loss: 33006472.0000 - learning_rate: 0.0010\n",
      "Epoch 165/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35677736.0000 - val_loss: 33067264.0000 - learning_rate: 0.0010\n",
      "Epoch 166/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35740284.0000 - val_loss: 32969634.0000 - learning_rate: 0.0010\n",
      "Epoch 167/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35434220.0000 - val_loss: 33043840.0000 - learning_rate: 0.0010\n",
      "Epoch 168/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 35317820.0000 - val_loss: 32983116.0000 - learning_rate: 0.0010\n",
      "Epoch 169/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35555248.0000 - val_loss: 32978696.0000 - learning_rate: 0.0010\n",
      "Epoch 170/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35460468.0000 - val_loss: 32994044.0000 - learning_rate: 0.0010\n",
      "Epoch 171/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35551232.0000 - val_loss: 32979618.0000 - learning_rate: 0.0010\n",
      "Epoch 172/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424us/step - loss: 35620232.0000 - val_loss: 32994772.0000 - learning_rate: 0.0010\n",
      "Epoch 173/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35472428.0000 - val_loss: 32974434.0000 - learning_rate: 0.0010\n",
      "Epoch 174/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35504660.0000 - val_loss: 33007286.0000 - learning_rate: 0.0010\n",
      "Epoch 175/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35298956.0000 - val_loss: 33012112.0000 - learning_rate: 0.0010\n",
      "Epoch 176/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407us/step - loss: 35466600.0000 - val_loss: 33009942.0000 - learning_rate: 0.0010\n",
      "Epoch 177/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35453060.0000 - val_loss: 32989628.0000 - learning_rate: 0.0010\n",
      "Epoch 178/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35107928.0000 - val_loss: 32993722.0000 - learning_rate: 0.0010\n",
      "Epoch 179/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35499996.0000 - val_loss: 33072010.0000 - learning_rate: 0.0010\n",
      "Epoch 180/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 35387392.0000 - val_loss: 33004264.0000 - learning_rate: 0.0010\n",
      "Epoch 181/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35435884.0000 - val_loss: 33119226.0000 - learning_rate: 0.0010\n",
      "Epoch 182/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35505524.0000 - val_loss: 33043110.0000 - learning_rate: 0.0010\n",
      "Epoch 183/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35439552.0000 - val_loss: 32982194.0000 - learning_rate: 0.0010\n",
      "Epoch 184/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 35562796.0000 - val_loss: 32993050.0000 - learning_rate: 0.0010\n",
      "Epoch 185/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35424044.0000 - val_loss: 32999908.0000 - learning_rate: 0.0010\n",
      "Epoch 186/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35335920.0000 - val_loss: 33011946.0000 - learning_rate: 0.0010\n",
      "Epoch 187/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35387312.0000 - val_loss: 33008948.0000 - learning_rate: 0.0010\n",
      "Epoch 188/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417us/step - loss: 35446644.0000 - val_loss: 33095876.0000 - learning_rate: 0.0010\n",
      "Epoch 189/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35396360.0000 - val_loss: 33032000.0000 - learning_rate: 0.0010\n",
      "Epoch 190/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35279132.0000 - val_loss: 32991482.0000 - learning_rate: 0.0010\n",
      "Epoch 191/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35378016.0000 - val_loss: 33081086.0000 - learning_rate: 0.0010\n",
      "Epoch 192/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35253576.0000 - val_loss: 32999154.0000 - learning_rate: 0.0010\n",
      "Epoch 193/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35391468.0000 - val_loss: 33027310.0000 - learning_rate: 0.0010\n",
      "Epoch 194/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35127024.0000 - val_loss: 33035556.0000 - learning_rate: 0.0010\n",
      "Epoch 195/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 35529400.0000 - val_loss: 32971266.0000 - learning_rate: 0.0010\n",
      "Epoch 196/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 35279684.0000 - val_loss: 32971290.0000 - learning_rate: 0.0010\n",
      "Epoch 197/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35309728.0000 - val_loss: 32977736.0000 - learning_rate: 0.0010\n",
      "Epoch 198/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35439188.0000 - val_loss: 32980012.0000 - learning_rate: 0.0010\n",
      "Epoch 199/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 35570212.0000 - val_loss: 33004690.0000 - learning_rate: 0.0010\n",
      "Epoch 200/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35232012.0000 - val_loss: 32966754.0000 - learning_rate: 0.0010\n",
      "Epoch 201/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35321616.0000 - val_loss: 32988680.0000 - learning_rate: 0.0010\n",
      "Epoch 202/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 425us/step - loss: 35136868.0000 - val_loss: 33007492.0000 - learning_rate: 0.0010\n",
      "Epoch 203/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424us/step - loss: 35434140.0000 - val_loss: 32973490.0000 - learning_rate: 0.0010\n",
      "Epoch 204/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 35168824.0000 - val_loss: 32981078.0000 - learning_rate: 0.0010\n",
      "Epoch 205/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35262460.0000 - val_loss: 32997976.0000 - learning_rate: 0.0010\n",
      "Epoch 206/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34940176.0000 - val_loss: 32979414.0000 - learning_rate: 0.0010\n",
      "Epoch 207/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 35274100.0000 - val_loss: 33046126.0000 - learning_rate: 0.0010\n",
      "Epoch 208/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 35360068.0000 - val_loss: 33029686.0000 - learning_rate: 0.0010\n",
      "Epoch 209/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35123428.0000 - val_loss: 32984874.0000 - learning_rate: 0.0010\n",
      "Epoch 210/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 35351368.0000 - val_loss: 32998724.0000 - learning_rate: 0.0010\n",
      "Epoch 211/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35319080.0000 - val_loss: 32983826.0000 - learning_rate: 0.0010\n",
      "Epoch 212/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 35289676.0000 - val_loss: 33003806.0000 - learning_rate: 0.0010\n",
      "Epoch 213/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 35210572.0000 - val_loss: 33000028.0000 - learning_rate: 0.0010\n",
      "Epoch 214/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 421us/step - loss: 35109568.0000 - val_loss: 33018106.0000 - learning_rate: 0.0010\n",
      "Epoch 215/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35213896.0000 - val_loss: 33012702.0000 - learning_rate: 0.0010\n",
      "Epoch 216/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35229080.0000 - val_loss: 33007890.0000 - learning_rate: 0.0010\n",
      "Epoch 217/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 415us/step - loss: 34968708.0000 - val_loss: 32965970.0000 - learning_rate: 0.0010\n",
      "Epoch 218/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 35021992.0000 - val_loss: 32984006.0000 - learning_rate: 0.0010\n",
      "Epoch 219/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34820348.0000 - val_loss: 32971228.0000 - learning_rate: 0.0010\n",
      "Epoch 220/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 35210548.0000 - val_loss: 32993010.0000 - learning_rate: 0.0010\n",
      "Epoch 221/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 35216900.0000 - val_loss: 32997132.0000 - learning_rate: 0.0010\n",
      "Epoch 222/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 420us/step - loss: 35090084.0000 - val_loss: 32967796.0000 - learning_rate: 0.0010\n",
      "Epoch 223/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 34856388.0000 - val_loss: 32978144.0000 - learning_rate: 0.0010\n",
      "Epoch 224/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35005932.0000 - val_loss: 32972008.0000 - learning_rate: 0.0010\n",
      "Epoch 225/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 35212896.0000 - val_loss: 33027666.0000 - learning_rate: 0.0010\n",
      "Epoch 226/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35137040.0000 - val_loss: 32974490.0000 - learning_rate: 0.0010\n",
      "Epoch 227/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34953056.0000 - val_loss: 32978002.0000 - learning_rate: 0.0010\n",
      "Epoch 228/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 34904520.0000 - val_loss: 32987810.0000 - learning_rate: 0.0010\n",
      "Epoch 229/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 34798444.0000 - val_loss: 32999094.0000 - learning_rate: 0.0010\n",
      "Epoch 230/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34844816.0000 - val_loss: 33039234.0000 - learning_rate: 0.0010\n",
      "Epoch 231/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 415us/step - loss: 35017504.0000 - val_loss: 33012720.0000 - learning_rate: 0.0010\n",
      "Epoch 232/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 34910056.0000 - val_loss: 32989968.0000 - learning_rate: 0.0010\n",
      "Epoch 233/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 35022008.0000 - val_loss: 32987360.0000 - learning_rate: 0.0010\n",
      "Epoch 234/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407us/step - loss: 34890820.0000 - val_loss: 33053398.0000 - learning_rate: 0.0010\n",
      "Epoch 235/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 34942312.0000 - val_loss: 33059276.0000 - learning_rate: 0.0010\n",
      "Epoch 236/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 34912480.0000 - val_loss: 33031756.0000 - learning_rate: 0.0010\n",
      "Epoch 237/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 34846760.0000 - val_loss: 33095232.0000 - learning_rate: 0.0010\n",
      "Epoch 238/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 34801048.0000 - val_loss: 32971484.0000 - learning_rate: 0.0010\n",
      "Epoch 239/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34791608.0000 - val_loss: 32980752.0000 - learning_rate: 0.0010\n",
      "Epoch 240/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34978708.0000 - val_loss: 33016948.0000 - learning_rate: 0.0010\n",
      "Epoch 241/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34934548.0000 - val_loss: 33025484.0000 - learning_rate: 0.0010\n",
      "Epoch 242/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34738336.0000 - val_loss: 32984480.0000 - learning_rate: 0.0010\n",
      "Epoch 243/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412us/step - loss: 34891928.0000 - val_loss: 32996364.0000 - learning_rate: 0.0010\n",
      "Epoch 244/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 34695280.0000 - val_loss: 32965708.0000 - learning_rate: 0.0010\n",
      "Epoch 245/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 34588264.0000 - val_loss: 33007710.0000 - learning_rate: 0.0010\n",
      "Epoch 246/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34644184.0000 - val_loss: 32964798.0000 - learning_rate: 0.0010\n",
      "Epoch 247/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 34792968.0000 - val_loss: 32978590.0000 - learning_rate: 0.0010\n",
      "Epoch 248/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 34544608.0000 - val_loss: 32994640.0000 - learning_rate: 0.0010\n",
      "Epoch 249/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 34734348.0000 - val_loss: 32972540.0000 - learning_rate: 0.0010\n",
      "Epoch 250/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407us/step - loss: 34676564.0000 - val_loss: 33110732.0000 - learning_rate: 0.0010\n",
      "Epoch 251/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424us/step - loss: 34612724.0000 - val_loss: 32969650.0000 - learning_rate: 0.0010\n",
      "Epoch 252/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34650760.0000 - val_loss: 32965792.0000 - learning_rate: 0.0010\n",
      "Epoch 253/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 34624128.0000 - val_loss: 32984482.0000 - learning_rate: 0.0010\n",
      "Epoch 254/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 34621760.0000 - val_loss: 32990116.0000 - learning_rate: 0.0010\n",
      "Epoch 255/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34753800.0000 - val_loss: 33023072.0000 - learning_rate: 0.0010\n",
      "Epoch 256/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 34607476.0000 - val_loss: 33010238.0000 - learning_rate: 0.0010\n",
      "Epoch 257/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417us/step - loss: 34590624.0000 - val_loss: 33038736.0000 - learning_rate: 0.0010\n",
      "Epoch 258/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 34575316.0000 - val_loss: 33146564.0000 - learning_rate: 0.0010\n",
      "Epoch 259/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 34676016.0000 - val_loss: 32994376.0000 - learning_rate: 0.0010\n",
      "Epoch 260/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 34672224.0000 - val_loss: 33010064.0000 - learning_rate: 0.0010\n",
      "Epoch 261/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 34542824.0000 - val_loss: 32974968.0000 - learning_rate: 0.0010\n",
      "Epoch 262/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 34560420.0000 - val_loss: 33020584.0000 - learning_rate: 0.0010\n",
      "Epoch 263/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418us/step - loss: 34278680.0000 - val_loss: 32977224.0000 - learning_rate: 0.0010\n",
      "Epoch 264/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 34372580.0000 - val_loss: 32986510.0000 - learning_rate: 0.0010\n",
      "Epoch 265/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34503696.0000 - val_loss: 33040500.0000 - learning_rate: 0.0010\n",
      "Epoch 266/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 34365840.0000 - val_loss: 32975600.0000 - learning_rate: 0.0010\n",
      "Epoch 267/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 34425068.0000 - val_loss: 32981566.0000 - learning_rate: 0.0010\n",
      "Epoch 268/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 34443284.0000 - val_loss: 32968566.0000 - learning_rate: 0.0010\n",
      "Epoch 269/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 34253628.0000 - val_loss: 33067126.0000 - learning_rate: 0.0010\n",
      "Epoch 270/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 34397636.0000 - val_loss: 32964312.0000 - learning_rate: 0.0010\n",
      "Epoch 271/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 34515228.0000 - val_loss: 33011740.0000 - learning_rate: 0.0010\n",
      "Epoch 272/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 34233376.0000 - val_loss: 32977716.0000 - learning_rate: 0.0010\n",
      "Epoch 273/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 416us/step - loss: 34211640.0000 - val_loss: 32971044.0000 - learning_rate: 0.0010\n",
      "Epoch 274/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34338412.0000 - val_loss: 33039602.0000 - learning_rate: 0.0010\n",
      "Epoch 275/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34461768.0000 - val_loss: 32969602.0000 - learning_rate: 0.0010\n",
      "Epoch 276/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34431276.0000 - val_loss: 32974202.0000 - learning_rate: 0.0010\n",
      "Epoch 277/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 429us/step - loss: 34225956.0000 - val_loss: 32987232.0000 - learning_rate: 0.0010\n",
      "Epoch 278/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34162852.0000 - val_loss: 32978106.0000 - learning_rate: 0.0010\n",
      "Epoch 279/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 34273588.0000 - val_loss: 32991600.0000 - learning_rate: 0.0010\n",
      "Epoch 280/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 34258428.0000 - val_loss: 32985786.0000 - learning_rate: 0.0010\n",
      "Epoch 281/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 34045528.0000 - val_loss: 32971194.0000 - learning_rate: 0.0010\n",
      "Epoch 282/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 409us/step - loss: 34278644.0000 - val_loss: 32978850.0000 - learning_rate: 0.0010\n",
      "Epoch 283/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34194092.0000 - val_loss: 32971402.0000 - learning_rate: 0.0010\n",
      "Epoch 284/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 34355524.0000 - val_loss: 32996634.0000 - learning_rate: 0.0010\n",
      "Epoch 285/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 34200564.0000 - val_loss: 32987846.0000 - learning_rate: 0.0010\n",
      "Epoch 286/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 34470964.0000 - val_loss: 32974828.0000 - learning_rate: 0.0010\n",
      "Epoch 287/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34175752.0000 - val_loss: 32980252.0000 - learning_rate: 0.0010\n",
      "Epoch 288/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 34118380.0000 - val_loss: 32970576.0000 - learning_rate: 0.0010\n",
      "Epoch 289/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33943264.0000 - val_loss: 32991316.0000 - learning_rate: 0.0010\n",
      "Epoch 290/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 34209492.0000 - val_loss: 32977050.0000 - learning_rate: 0.0010\n",
      "Epoch 291/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34140324.0000 - val_loss: 32985496.0000 - learning_rate: 0.0010\n",
      "Epoch 292/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 34165636.0000 - val_loss: 33002704.0000 - learning_rate: 0.0010\n",
      "Epoch 293/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 34189512.0000 - val_loss: 32975336.0000 - learning_rate: 0.0010\n",
      "Epoch 294/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 34083336.0000 - val_loss: 32974144.0000 - learning_rate: 0.0010\n",
      "Epoch 295/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 34096704.0000 - val_loss: 32968438.0000 - learning_rate: 0.0010\n",
      "Epoch 296/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 34039524.0000 - val_loss: 32983266.0000 - learning_rate: 0.0010\n",
      "Epoch 297/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 33879380.0000 - val_loss: 32979534.0000 - learning_rate: 0.0010\n",
      "Epoch 298/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 34160420.0000 - val_loss: 32982236.0000 - learning_rate: 0.0010\n",
      "Epoch 299/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33814584.0000 - val_loss: 32982668.0000 - learning_rate: 0.0010\n",
      "Epoch 300/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 33952596.0000 - val_loss: 32991038.0000 - learning_rate: 0.0010\n",
      "Epoch 301/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 34042408.0000 - val_loss: 33123238.0000 - learning_rate: 0.0010\n",
      "Epoch 302/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 34040364.0000 - val_loss: 32977082.0000 - learning_rate: 0.0010\n",
      "Epoch 303/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 33817136.0000 - val_loss: 32976026.0000 - learning_rate: 0.0010\n",
      "Epoch 304/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 34018348.0000 - val_loss: 32981988.0000 - learning_rate: 0.0010\n",
      "Epoch 305/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 438us/step - loss: 34180740.0000 - val_loss: 32999022.0000 - learning_rate: 0.0010\n",
      "Epoch 306/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 33955712.0000 - val_loss: 32968256.0000 - learning_rate: 0.0010\n",
      "Epoch 307/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 34077168.0000 - val_loss: 33015618.0000 - learning_rate: 0.0010\n",
      "Epoch 308/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33835068.0000 - val_loss: 32990644.0000 - learning_rate: 0.0010\n",
      "Epoch 309/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33985632.0000 - val_loss: 32993424.0000 - learning_rate: 0.0010\n",
      "Epoch 310/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33828052.0000 - val_loss: 32989798.0000 - learning_rate: 0.0010\n",
      "Epoch 311/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33783012.0000 - val_loss: 32968546.0000 - learning_rate: 0.0010\n",
      "Epoch 312/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 33854908.0000 - val_loss: 32973920.0000 - learning_rate: 0.0010\n",
      "Epoch 313/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 33641056.0000 - val_loss: 33049648.0000 - learning_rate: 0.0010\n",
      "Epoch 314/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 33859000.0000 - val_loss: 32963064.0000 - learning_rate: 0.0010\n",
      "Epoch 315/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 445us/step - loss: 33983968.0000 - val_loss: 32995204.0000 - learning_rate: 0.0010\n",
      "Epoch 316/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 33769396.0000 - val_loss: 32993238.0000 - learning_rate: 0.0010\n",
      "Epoch 317/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33846416.0000 - val_loss: 32978280.0000 - learning_rate: 0.0010\n",
      "Epoch 318/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33858748.0000 - val_loss: 32995346.0000 - learning_rate: 0.0010\n",
      "Epoch 319/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33789468.0000 - val_loss: 32978020.0000 - learning_rate: 0.0010\n",
      "Epoch 320/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33865788.0000 - val_loss: 32985714.0000 - learning_rate: 0.0010\n",
      "Epoch 321/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 33809820.0000 - val_loss: 32978140.0000 - learning_rate: 0.0010\n",
      "Epoch 322/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33860092.0000 - val_loss: 32990722.0000 - learning_rate: 0.0010\n",
      "Epoch 323/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - loss: 33801964.0000 - val_loss: 32975280.0000 - learning_rate: 0.0010\n",
      "Epoch 324/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 33686156.0000 - val_loss: 32974116.0000 - learning_rate: 0.0010\n",
      "Epoch 325/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33868692.0000 - val_loss: 32977402.0000 - learning_rate: 0.0010\n",
      "Epoch 326/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33898904.0000 - val_loss: 32973728.0000 - learning_rate: 0.0010\n",
      "Epoch 327/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33660352.0000 - val_loss: 33000212.0000 - learning_rate: 0.0010\n",
      "Epoch 328/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418us/step - loss: 33722972.0000 - val_loss: 32990568.0000 - learning_rate: 0.0010\n",
      "Epoch 329/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 356us/step - loss: 33567928.0000 - val_loss: 32982258.0000 - learning_rate: 0.0010\n",
      "Epoch 330/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357us/step - loss: 33696436.0000 - val_loss: 32974312.0000 - learning_rate: 0.0010\n",
      "Epoch 331/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 33934888.0000 - val_loss: 32973898.0000 - learning_rate: 0.0010\n",
      "Epoch 332/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 33713948.0000 - val_loss: 32975492.0000 - learning_rate: 0.0010\n",
      "Epoch 333/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33568152.0000 - val_loss: 33034626.0000 - learning_rate: 0.0010\n",
      "Epoch 334/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33693800.0000 - val_loss: 32981756.0000 - learning_rate: 0.0010\n",
      "Epoch 335/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 447us/step - loss: 33596108.0000 - val_loss: 32991572.0000 - learning_rate: 0.0010\n",
      "Epoch 336/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 33740508.0000 - val_loss: 32985608.0000 - learning_rate: 0.0010\n",
      "Epoch 337/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 428us/step - loss: 33642044.0000 - val_loss: 32967010.0000 - learning_rate: 0.0010\n",
      "Epoch 338/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33667104.0000 - val_loss: 32993212.0000 - learning_rate: 0.0010\n",
      "Epoch 339/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33632472.0000 - val_loss: 32977682.0000 - learning_rate: 0.0010\n",
      "Epoch 340/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 33511522.0000 - val_loss: 32992182.0000 - learning_rate: 0.0010\n",
      "Epoch 341/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 33774216.0000 - val_loss: 32978724.0000 - learning_rate: 0.0010\n",
      "Epoch 342/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 33627104.0000 - val_loss: 33005036.0000 - learning_rate: 0.0010\n",
      "Epoch 343/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 33635100.0000 - val_loss: 32973538.0000 - learning_rate: 0.0010\n",
      "Epoch 344/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 33639540.0000 - val_loss: 32975994.0000 - learning_rate: 0.0010\n",
      "Epoch 345/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 33419016.0000 - val_loss: 32981808.0000 - learning_rate: 0.0010\n",
      "Epoch 346/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 33489250.0000 - val_loss: 33002492.0000 - learning_rate: 0.0010\n",
      "Epoch 347/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 427us/step - loss: 33517898.0000 - val_loss: 32997172.0000 - learning_rate: 0.0010\n",
      "Epoch 348/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 432us/step - loss: 33589220.0000 - val_loss: 32993296.0000 - learning_rate: 0.0010\n",
      "Epoch 349/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 419us/step - loss: 33491208.0000 - val_loss: 32983106.0000 - learning_rate: 0.0010\n",
      "Epoch 350/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436us/step - loss: 33493626.0000 - val_loss: 32982022.0000 - learning_rate: 0.0010\n",
      "Epoch 351/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 422us/step - loss: 33575944.0000 - val_loss: 32984650.0000 - learning_rate: 0.0010\n",
      "Epoch 352/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 467us/step - loss: 33552174.0000 - val_loss: 32978508.0000 - learning_rate: 0.0010\n",
      "Epoch 353/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441us/step - loss: 33540536.0000 - val_loss: 32982140.0000 - learning_rate: 0.0010\n",
      "Epoch 354/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 420us/step - loss: 33507158.0000 - val_loss: 32967360.0000 - learning_rate: 0.0010\n",
      "Epoch 355/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 423us/step - loss: 33584736.0000 - val_loss: 32984660.0000 - learning_rate: 0.0010\n",
      "Epoch 356/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436us/step - loss: 33577764.0000 - val_loss: 32964392.0000 - learning_rate: 0.0010\n",
      "Epoch 357/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 453us/step - loss: 33561336.0000 - val_loss: 32925392.0000 - learning_rate: 0.0010\n",
      "Epoch 358/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 434us/step - loss: 33444576.0000 - val_loss: 32882912.0000 - learning_rate: 0.0010\n",
      "Epoch 359/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441us/step - loss: 33458136.0000 - val_loss: 32834990.0000 - learning_rate: 0.0010\n",
      "Epoch 360/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 451us/step - loss: 33398500.0000 - val_loss: 32794910.0000 - learning_rate: 0.0010\n",
      "Epoch 361/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 469us/step - loss: 33489230.0000 - val_loss: 32745188.0000 - learning_rate: 0.0010\n",
      "Epoch 362/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470us/step - loss: 33343498.0000 - val_loss: 32677800.0000 - learning_rate: 0.0010\n",
      "Epoch 363/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 495us/step - loss: 33203084.0000 - val_loss: 32442480.0000 - learning_rate: 0.0010\n",
      "Epoch 364/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449us/step - loss: 33141424.0000 - val_loss: 32210194.0000 - learning_rate: 0.0010\n",
      "Epoch 365/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 33009394.0000 - val_loss: 32120030.0000 - learning_rate: 0.0010\n",
      "Epoch 366/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449us/step - loss: 32877384.0000 - val_loss: 32075744.0000 - learning_rate: 0.0010\n",
      "Epoch 367/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 480us/step - loss: 32892278.0000 - val_loss: 32061444.0000 - learning_rate: 0.0010\n",
      "Epoch 368/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 469us/step - loss: 32837978.0000 - val_loss: 32002322.0000 - learning_rate: 0.0010\n",
      "Epoch 369/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 446us/step - loss: 32911864.0000 - val_loss: 32003188.0000 - learning_rate: 0.0010\n",
      "Epoch 370/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 469us/step - loss: 32911406.0000 - val_loss: 31983380.0000 - learning_rate: 0.0010\n",
      "Epoch 371/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 481us/step - loss: 32880106.0000 - val_loss: 31934222.0000 - learning_rate: 0.0010\n",
      "Epoch 372/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 32912834.0000 - val_loss: 31909858.0000 - learning_rate: 0.0010\n",
      "Epoch 373/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 458us/step - loss: 32776578.0000 - val_loss: 31875478.0000 - learning_rate: 0.0010\n",
      "Epoch 374/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 453us/step - loss: 32747368.0000 - val_loss: 31924070.0000 - learning_rate: 0.0010\n",
      "Epoch 375/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 474us/step - loss: 32837062.0000 - val_loss: 31889320.0000 - learning_rate: 0.0010\n",
      "Epoch 376/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 501us/step - loss: 32808480.0000 - val_loss: 31873588.0000 - learning_rate: 0.0010\n",
      "Epoch 377/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496us/step - loss: 32790554.0000 - val_loss: 31868594.0000 - learning_rate: 0.0010\n",
      "Epoch 378/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 32751722.0000 - val_loss: 31841736.0000 - learning_rate: 0.0010\n",
      "Epoch 379/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 491us/step - loss: 32956110.0000 - val_loss: 31871554.0000 - learning_rate: 0.0010\n",
      "Epoch 380/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 32758394.0000 - val_loss: 31853038.0000 - learning_rate: 0.0010\n",
      "Epoch 381/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 32637070.0000 - val_loss: 31866132.0000 - learning_rate: 0.0010\n",
      "Epoch 382/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 32797434.0000 - val_loss: 31851436.0000 - learning_rate: 0.0010\n",
      "Epoch 383/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 32776270.0000 - val_loss: 31838424.0000 - learning_rate: 0.0010\n",
      "Epoch 384/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 32553642.0000 - val_loss: 31861334.0000 - learning_rate: 0.0010\n",
      "Epoch 385/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 32718368.0000 - val_loss: 31850600.0000 - learning_rate: 0.0010\n",
      "Epoch 386/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 32795084.0000 - val_loss: 31839204.0000 - learning_rate: 0.0010\n",
      "Epoch 387/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 32649950.0000 - val_loss: 31834696.0000 - learning_rate: 0.0010\n",
      "Epoch 388/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32857342.0000 - val_loss: 31831826.0000 - learning_rate: 0.0010\n",
      "Epoch 389/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 32703362.0000 - val_loss: 31884368.0000 - learning_rate: 0.0010\n",
      "Epoch 390/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 32789050.0000 - val_loss: 31854146.0000 - learning_rate: 0.0010\n",
      "Epoch 391/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 425us/step - loss: 32846024.0000 - val_loss: 31905004.0000 - learning_rate: 0.0010\n",
      "Epoch 392/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 32597250.0000 - val_loss: 31841290.0000 - learning_rate: 0.0010\n",
      "Epoch 393/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 421us/step - loss: 32634444.0000 - val_loss: 31857170.0000 - learning_rate: 0.0010\n",
      "Epoch 394/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 416us/step - loss: 32622678.0000 - val_loss: 31827156.0000 - learning_rate: 0.0010\n",
      "Epoch 395/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 32528104.0000 - val_loss: 31858540.0000 - learning_rate: 0.0010\n",
      "Epoch 396/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 32851244.0000 - val_loss: 31849326.0000 - learning_rate: 0.0010\n",
      "Epoch 397/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 32731830.0000 - val_loss: 31830788.0000 - learning_rate: 0.0010\n",
      "Epoch 398/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 32879150.0000 - val_loss: 31864260.0000 - learning_rate: 0.0010\n",
      "Epoch 399/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 32770714.0000 - val_loss: 31854190.0000 - learning_rate: 0.0010\n",
      "Epoch 400/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 32773146.0000 - val_loss: 31839594.0000 - learning_rate: 0.0010\n",
      "Epoch 401/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32699806.0000 - val_loss: 31845938.0000 - learning_rate: 0.0010\n",
      "Epoch 402/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 32840440.0000 - val_loss: 31833198.0000 - learning_rate: 0.0010\n",
      "Epoch 403/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 32658346.0000 - val_loss: 31835146.0000 - learning_rate: 0.0010\n",
      "Epoch 404/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 32727096.0000 - val_loss: 31831026.0000 - learning_rate: 0.0010\n",
      "Epoch 405/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 32895196.0000 - val_loss: 31839900.0000 - learning_rate: 0.0010\n",
      "Epoch 406/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 32896014.0000 - val_loss: 31849552.0000 - learning_rate: 0.0010\n",
      "Epoch 407/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 32923880.0000 - val_loss: 31836604.0000 - learning_rate: 0.0010\n",
      "Epoch 408/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 32956180.0000 - val_loss: 31815232.0000 - learning_rate: 0.0010\n",
      "Epoch 409/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32684184.0000 - val_loss: 31819598.0000 - learning_rate: 0.0010\n",
      "Epoch 410/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 414us/step - loss: 32818518.0000 - val_loss: 31827844.0000 - learning_rate: 0.0010\n",
      "Epoch 411/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 32827832.0000 - val_loss: 31831852.0000 - learning_rate: 0.0010\n",
      "Epoch 412/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405us/step - loss: 32833204.0000 - val_loss: 31842150.0000 - learning_rate: 0.0010\n",
      "Epoch 413/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384us/step - loss: 32749320.0000 - val_loss: 31849548.0000 - learning_rate: 0.0010\n",
      "Epoch 414/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 32598388.0000 - val_loss: 31821702.0000 - learning_rate: 0.0010\n",
      "Epoch 415/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 32767596.0000 - val_loss: 31853830.0000 - learning_rate: 0.0010\n",
      "Epoch 416/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411us/step - loss: 32976832.0000 - val_loss: 31823816.0000 - learning_rate: 0.0010\n",
      "Epoch 417/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 32713950.0000 - val_loss: 31851376.0000 - learning_rate: 0.0010\n",
      "Epoch 418/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 32695504.0000 - val_loss: 31849320.0000 - learning_rate: 0.0010\n",
      "Epoch 419/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 32623222.0000 - val_loss: 31822946.0000 - learning_rate: 0.0010\n",
      "Epoch 420/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 32576074.0000 - val_loss: 31833638.0000 - learning_rate: 0.0010\n",
      "Epoch 421/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 32684682.0000 - val_loss: 31827106.0000 - learning_rate: 0.0010\n",
      "Epoch 422/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 32711480.0000 - val_loss: 31866106.0000 - learning_rate: 0.0010\n",
      "Epoch 423/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 32610440.0000 - val_loss: 31893994.0000 - learning_rate: 0.0010\n",
      "Epoch 424/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 32746586.0000 - val_loss: 31831606.0000 - learning_rate: 0.0010\n",
      "Epoch 425/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 32745518.0000 - val_loss: 31815144.0000 - learning_rate: 0.0010\n",
      "Epoch 426/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 32728800.0000 - val_loss: 31834432.0000 - learning_rate: 0.0010\n",
      "Epoch 427/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 32910312.0000 - val_loss: 31827102.0000 - learning_rate: 0.0010\n",
      "Epoch 428/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533us/step - loss: 32718020.0000 - val_loss: 31831972.0000 - learning_rate: 0.0010\n",
      "Epoch 429/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 714us/step - loss: 32813088.0000 - val_loss: 31830640.0000 - learning_rate: 0.0010\n",
      "Epoch 430/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 413us/step - loss: 32845370.0000 - val_loss: 31831536.0000 - learning_rate: 0.0010\n",
      "Epoch 431/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 32764990.0000 - val_loss: 31862104.0000 - learning_rate: 0.0010\n",
      "Epoch 432/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418us/step - loss: 32811638.0000 - val_loss: 31918132.0000 - learning_rate: 0.0010\n",
      "Epoch 433/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 32894106.0000 - val_loss: 31824432.0000 - learning_rate: 0.0010\n",
      "Epoch 434/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 32683434.0000 - val_loss: 31816136.0000 - learning_rate: 0.0010\n",
      "Epoch 435/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 32747684.0000 - val_loss: 31844638.0000 - learning_rate: 0.0010\n",
      "Epoch 436/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 32833078.0000 - val_loss: 31833314.0000 - learning_rate: 0.0010\n",
      "Epoch 437/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 32655598.0000 - val_loss: 31825262.0000 - learning_rate: 0.0010\n",
      "Epoch 438/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32742488.0000 - val_loss: 31845784.0000 - learning_rate: 0.0010\n",
      "Epoch 439/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 32740840.0000 - val_loss: 31828824.0000 - learning_rate: 0.0010\n",
      "Epoch 440/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 32697868.0000 - val_loss: 31843374.0000 - learning_rate: 0.0010\n",
      "Epoch 441/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 32718480.0000 - val_loss: 31833658.0000 - learning_rate: 0.0010\n",
      "Epoch 442/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 33003250.0000 - val_loss: 31857632.0000 - learning_rate: 0.0010\n",
      "Epoch 443/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 32735608.0000 - val_loss: 31836386.0000 - learning_rate: 0.0010\n",
      "Epoch 444/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 382us/step - loss: 32753624.0000 - val_loss: 31831614.0000 - learning_rate: 0.0010\n",
      "Epoch 445/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 32662730.0000 - val_loss: 31873082.0000 - learning_rate: 0.0010\n",
      "Epoch 446/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32874346.0000 - val_loss: 31846598.0000 - learning_rate: 0.0010\n",
      "Epoch 447/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 32696658.0000 - val_loss: 31863916.0000 - learning_rate: 0.0010\n",
      "Epoch 448/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 32993092.0000 - val_loss: 31826172.0000 - learning_rate: 0.0010\n",
      "Epoch 449/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 32881338.0000 - val_loss: 31817800.0000 - learning_rate: 0.0010\n",
      "Epoch 450/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 32810648.0000 - val_loss: 31836806.0000 - learning_rate: 0.0010\n",
      "Epoch 451/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 422us/step - loss: 32838790.0000 - val_loss: 31851398.0000 - learning_rate: 0.0010\n",
      "Epoch 452/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385us/step - loss: 32704854.0000 - val_loss: 31829594.0000 - learning_rate: 0.0010\n",
      "Epoch 453/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 32747102.0000 - val_loss: 31868732.0000 - learning_rate: 0.0010\n",
      "Epoch 454/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 32643686.0000 - val_loss: 31837856.0000 - learning_rate: 0.0010\n",
      "Epoch 455/800\n",
      "\u001b[1m4596/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 32881678.0000 - val_loss: 31836572.0000 - learning_rate: 0.0010\n",
      "Epoch 456/800\n",
      "\u001b[1m3385/4596\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 32713178.0000"
     ]
    }
   ],
   "source": [
    "# Number of input features (after dropping the target column)\n",
    "variable_amount = len(X.columns)\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "mc = ModelCheckpoint('best_model_regression1.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# ReduceLROnPlateau to reduce learning rate when the validation loss has stopped improving\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)\n",
    "\n",
    "# Combine all active callbacks into a list (excluding EarlyStopping)\n",
    "callback_list = [mc, rlr]\n",
    "\n",
    "# Define Sequential neural network model with Dropout layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(variable_amount,)),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1)  # Output layer for regression (1 node, no activation function)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model with Adam optimizer and Mean Squared Error loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print out the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model with the dataset and apply the callbacks\n",
    "history = model.fit(x=X_train, y=y_train, epochs=800, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520168d0-4995-4c49-ac40-b9aa20e6227b",
   "metadata": {},
   "source": [
    "## Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21e55e-1def-4103-94b3-35f1010f39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loss over epochs \n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()\n",
    "\n",
    "# insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1399850-0217-4fff-aec2-75ad85a7c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the final model loss/evaluation values\n",
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))\n",
    "\n",
    "# the model is often good when these error values are similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ecb2b-d848-4d1f-a1c7-da799d71bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df\n",
    "\n",
    "# insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b1ef-8a24-4721-b0c4-c394936dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values follow a linear line = good predictions\n",
    "# we basically compare the predicted values \n",
    "# to true test values and see the differences\n",
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)\n",
    "\n",
    "# the same picture as in the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16462457-2dca-48f4-a391-2714d2310e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean average error\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"$\")\n",
    "\n",
    "# MSE - Mean square error\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"$^2\")\n",
    "\n",
    "# RMSE - Root mean square error\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"$\")\n",
    "\n",
    "# R-squared. 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "# Explained Variance Score => 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "# high variance score = model is a good fit for the data \n",
    "# low variance score = model is not a good fit for the data\n",
    "# the higher the score, the model is more able to explain the variation in the data\n",
    "# if score is low, we might need more and better data\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))\n",
    "\n",
    "\n",
    "# MAE\n",
    "# Lower is better\n",
    "# Original is 4841.92 $\n",
    "# Now is \n",
    "# almost the same result\n",
    "\n",
    "# MSE\n",
    "# Lower is better\n",
    "# Original is 34666769.92 $^2\n",
    "# Now is \n",
    "# almost the same\n",
    "\n",
    "# RMSE\n",
    "# Lower is better\n",
    "# Original is 5887.85 $\n",
    "# Now is \n",
    "# a little difference\n",
    "\n",
    "# R-squared\n",
    "# Closer to 1 is better\n",
    "# Original is 0.07\n",
    "# Now is\n",
    "# here we have a bit better results\n",
    "\n",
    "\n",
    "# Explained variance score\n",
    "# Closer to 1 is better\n",
    "# Original is 0.1\n",
    "# Now is \n",
    "\n",
    "# To sum up,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529c354-1706-4a69-ba5e-4509ab186122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

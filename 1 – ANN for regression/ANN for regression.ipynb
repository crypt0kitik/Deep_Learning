{
 "cells": [
  {
   "cell_type": "raw",
   "id": "69d83f0f-f136-4be6-b6af-2b794d8e805d",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/harikrishnareddyb/used-car-price-predictions\n",
    "\n",
    "Price prediction\n",
    "\n",
    "Columns\n",
    "Price - Target Variable.\n",
    "Year - Year of the car purchased.\n",
    "Mileage - The no.of kms drove by the car.\n",
    "City - In which city it was sold.\n",
    "State - In which state it was sold.\n",
    "Vin - a unique number for a car.\n",
    "Make - Manufacturer of the car.\n",
    "Model - The model(name) of the car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6a34e-8c09-4a53-aca5-bdcafb68f5f5",
   "metadata": {},
   "source": [
    "# Part 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c87b1-3cbc-4e5f-adf6-5d9987bf9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalation of neccesary libraries\n",
    "\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install seaborn\n",
    "#!pip install scikit-learn\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808ab6c-c100-48d3-9877-a4799a528261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496ef1e-7ee8-4469-8bd8-74d4c6f396a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data\n",
    "df = pd.read_csv(\"true_car_listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd88d7c-c7dc-49b7-a0db-45aaa17a1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the amount of data\n",
    "df.describe()\n",
    "\n",
    "# this is not a small dataset\n",
    "# we have 852 122 units\n",
    "# which allows us to clean and balance data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad36aff-b408-453e-997a-c759116b7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that we do have\n",
    "# string data which we will need to convert to numeric\n",
    "# to create a neural network later\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9e0df-9662-4f0c-afca-611566e92d5e",
   "metadata": {},
   "source": [
    "# Part 2. Cleaning data and formating into numeric\n",
    "\n",
    "1. Check duplicates\n",
    "2. Check Nan values\n",
    "3. Change the \"City\" column to numeric\n",
    "4. Change the \"State\" column to numeric\n",
    "5. Change the \"Vin\" column to numeric\n",
    "6. Change the \"Make\" column to numeric\n",
    "7. Change the \"Model\" column to numeric\n",
    "8. Remove outliers by checking balance of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c330b-fb8c-4672-82e1-1e51f6b29d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data type to make a plan\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9031d-0fc2-40ad-b1cf-66e3b3a8cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check duplicates\n",
    "# check for duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "# I have 30 which is a quite small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1273377-16ad-4d03-aa5f-ccd587fbed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36f0b4-ef45-423b-83cd-cba7858f46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check Nan values\n",
    "# check for missing values\n",
    "df.isna().sum()\n",
    "\n",
    "# zero missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cb18b-919e-4e9e-bdd1-57982a458eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique values of each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6da9dc-dc2a-45a0-be96-6e3c9463b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I checked that I have a huge amount of unique values\n",
    "# that is why I cannot use very popular and standard \n",
    "# methods like Label encoding or One-Hot encoding\n",
    "# for transfering this data into numeric\n",
    "\n",
    "# For some columns I opted for another \n",
    "# method which is Frequency encoding\n",
    "# it encodes the values based on their frequency\n",
    "\n",
    "# columns for this method\n",
    "# City         2553\n",
    "# State          59\n",
    "# Make           58\n",
    "# Model        2736\n",
    "\n",
    "# 3. Change the \"City\" column to numeric\n",
    "city_freq = df['City'].value_counts().to_dict()\n",
    "df['City'] = df['City'].map(city_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bc7fa-775d-44b0-8eb5-083a69e2482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether everything works correctly\n",
    "# I can implement the same function to other columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf2c85-2cb9-4e15-858d-f805f18b2d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Change the \"State\" column to numeric\n",
    "state_freq = df['State'].value_counts().to_dict()\n",
    "df['State'] = df['State'].map(state_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6332d0-cbcc-47a3-9795-c5c4b8f15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Change the \"Make\" column to numeric\n",
    "make_freq = df['Make'].value_counts().to_dict()\n",
    "df['Make'] = df['Make'].map(make_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455fa50-c645-4221-bebc-b37e8a209c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Change the \"Model\" column to numeric\n",
    "model_freq = df['Model'].value_counts().to_dict()\n",
    "df['Model'] = df['Model'].map(model_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d104f-486c-4270-b321-66bbf88c9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything worked correctly\n",
    "df.head()\n",
    "\n",
    "# we have only one string column left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569736d-43ec-4c1c-9feb-2cc53727369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Change the \"Vin\" column to numeric\n",
    "\n",
    "# for the next \"Vin\" column\n",
    "# I decided to go with Hashing encoding method\n",
    "# because this column has a huge number of values\n",
    "# Vin        852075\n",
    "\n",
    "# import a library\n",
    "import hashlib\n",
    "\n",
    "# apply a hashing function to each Vin\n",
    "df['Vin'] = df['Vin'].apply(lambda x: int(hashlib.sha256(x.encode('utf-8')).hexdigest(), 16) % 10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98582bd6-82a8-490e-ab33-dc5ed7f061cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great, now we have only numeric data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853dda4-f580-42cd-b313-e34a422e05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Remove outliers by checking balance of these columns\n",
    "sns.displot(df, x=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a600471-77d2-4cd1-9756-c86c22c0ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove everything that is considered as outliers\n",
    "\n",
    "# df = df.query(\"Price > Price.quantile(0.05) and Price < Price.quantile(0.95)\")\n",
    "# this is still quite unbalanced\n",
    "\n",
    "# df = df.query(\"Price > Price.quantile(0.05) and Price < Price.quantile(0.90)\")\n",
    "# this one is better but it still has more values on the right side\n",
    "\n",
    "df = df.query(\"Price > Price.quantile(0.05) and Price < Price.quantile(0.85)\")\n",
    "# this variation shows better results and I will keep this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f8108-03d8-401f-9414-2be6cd1bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574eef6f-ed0a-4e66-a128-de02b53a95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check balance for the Year column\n",
    "sns.displot(df, x=\"Year\")\n",
    "\n",
    "# we can see that the majority of outliers\n",
    "# are on the left side that is why\n",
    "# drop only some values from the \"tail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6d4de-b2d2-4af4-8844-3d6c525ee34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the number of unique values\n",
    "unique_value_counts = df['Year'].value_counts()\n",
    "print(unique_value_counts)\n",
    "\n",
    "# if we have all values in thousands\n",
    "# and only 135 for 2018, can we consider them as outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a95fb5-ed3e-408b-936d-905fb8be4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see whether we have correlation between years\n",
    "# it will help us to decide\n",
    "# can we drop some years as outliers\n",
    "# or we should keep them even though they look like outliers\n",
    "\n",
    "# define the bins and corresponding labels\n",
    "bins = [1990, 2000, 2010, 2015, 2020]  # define ´ bin edges\n",
    "labels = ['1990-2000', '2000-2010', '2010-2015', '2015-2020']  # define ´ bin labels\n",
    "\n",
    "# Aápply binning to the 'Year' column\n",
    "df['Year_Binned'] = pd.cut(df['Year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# check the binning by counting the number of entries in each bin\n",
    "bin_counts = df['Year_Binned'].value_counts()\n",
    "print(bin_counts)\n",
    "\n",
    "# insights:\n",
    "# for sure we can drop years 1990\n",
    "# before I thought about 2018 as an outliers\n",
    "# values are in this year\n",
    "# 2018       220\n",
    "# 1998       180\n",
    "# 1997       131\n",
    "\n",
    "# but we can see that it is in a part of a big bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6fb17-237f-4612-b228-2341664b3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"Year > Year.quantile(0.1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0950b6-6e3a-4829-b4a8-54cf3dcb5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quite balanced data\n",
    "sns.displot(df, x=\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c179ff-50cd-4cd5-966a-6cc5b1c075ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column that we created for this analysis\n",
    "df = df.drop(columns='Year_Binned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2cc84-9bc5-4f8d-a00d-ed6b0b7f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I became tired of repeating the same code that is why\n",
    "# i decided to make a graph that shows all balances at the same time\n",
    "# ChatGPT helped me to generate this very quickly\n",
    "\n",
    "# list of columns to plot\n",
    "columns_to_plot = ['Mileage', 'City', 'State', 'Make', 'Model']\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through the columns and create a displot for each\n",
    "for i, column in enumerate(columns_to_plot, 1):\n",
    "    plt.subplot(4, 2, i)  # Adjust the subplot grid based on the number of columns\n",
    "    sns.histplot(df[column], kde=True)  # kde=True adds a Kernel Density Estimate\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# insights:\n",
    "# as we see all of these columns need to be balanced\n",
    "# I deleted the VIN column from this because it is unique\n",
    "# and only now I recognised that I actually do not need this column\n",
    "# it does not affect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e4c86-46d6-403d-b665-7dead1036f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the VIN column\n",
    "df = df.drop(columns='Vin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28b6b1-d893-4907-a548-a6f87cfc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the number of unique values\n",
    "# for Mileage in order to see how many small\n",
    "# but extreme values we have\n",
    "unique_value_counts = df['Mileage'].value_counts()\n",
    "print(unique_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35af8c-4fc1-4d8f-8c80-928ba08cba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"Mileage < Mileage.quantile(0.90)\")\n",
    "\n",
    "# I tried different variants and checked them \n",
    "# by using displot\n",
    "#df = df.query(\"Mileage < Mileage.quantile(0.95)\")\n",
    "#df = df.query(\"Mileage < Mileage.quantile(0.80)\")\n",
    "#df = df.query(\"Mileage < Mileage.quantile(0.77)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f92b5-9334-4a44-866d-f37ae06c64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graph shows that the Mileage column\n",
    "# is quite balanced now\n",
    "sns.displot(df, x=\"Mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67059cc8-82ac-4bf4-bccd-98ce4f793cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"City < City.quantile(0.70)\")\n",
    "\n",
    "# experimented with different quantile\n",
    "#df = df.query(\"City < City.quantile(0.75)\")\n",
    "#df = df.query(\"City < City.quantile(0.80)\")\n",
    "#df = df.query(\"City < City.quantile(0.85)\")\n",
    "#df = df.query(\"City < City.quantile(0.90)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c771f9-f2ea-4165-8dda-fd272660db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though it is not prefeclty balanced\n",
    "# it has less outliers anyway\n",
    "sns.displot(df, x=\"City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a2fc3-2dff-4c6b-be8a-38f417a6b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"State < State.quantile(0.85)\")\n",
    "\n",
    "#df = df.query(\"State < State.quantile(0.95)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ca200-5643-4e12-9dc8-7da8b9682d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there is no extreme outliers \n",
    "sns.displot(df, x=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244bb1e9-daba-42a6-bf26-d6b3b8fac986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"Make < Make.quantile(0.80)\")\n",
    "\n",
    "#df = df.query(\"Make < Make.quantile(0.85)\")\n",
    "#df = df.query(\"Make < Make.quantile(0.90)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaac733-7985-45e6-b725-04183575b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there is no extreme outliers \n",
    "sns.displot(df, x=\"Make\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c3a85-5c81-4f70-9c68-d0de40699ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df.query(\"Model < Model.quantile(0.90)\")\n",
    "\n",
    "#df = df.query(\"Model < Model.quantile(0.95)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393ac39-8b87-4f68-b83b-ee0686a12cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there is no extreme outliers \n",
    "sns.displot(df, x=\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de089f6b-8484-4e39-acee-6e8850644b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the same graphs to see balance now\n",
    "\n",
    "# list of columns to plot\n",
    "columns_to_plot = ['Price', 'Year', 'Mileage', 'City', 'State', 'Make', 'Model']\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through the columns and create a displot for each\n",
    "for i, column in enumerate(columns_to_plot, 1):\n",
    "    plt.subplot(4, 2, i)  # Adjust the subplot grid based on the number of columns\n",
    "    sns.histplot(df[column], kde=True)  # kde=True adds a Kernel Density Estimate\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# insights:\n",
    "# still some columns need some improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f25a41-d339-4f2c-8ea9-ce1040e95d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am curious to see how much data I lost\n",
    "# by removing outliers\n",
    "# before any modification I had 852122 values\n",
    "df.describe()\n",
    "\n",
    "# I have lost approximately 75.35% of values\n",
    "# BUT! (there is a big but! :) \n",
    "# I purposely chose this dataset to avoid worrying \n",
    "# about the number of rows. Of course, in real life, \n",
    "# I wouldn’t be able to drop 75%, but now I have a balanced \n",
    "# and clean dataset for developing a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e6afe-f1b8-4616-8b53-97d24412a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.s. Saving the dataset for other experiments\n",
    "# save the df to a new CSV file\n",
    "df.to_csv('Documents/GitHub/Deep_learning/1 – ANN for regression/balanced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d12c0-ad79-41db-930d-80259be41cd3",
   "metadata": {},
   "source": [
    "# Part 3. X/y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b39871-682b-4edd-9c70-0d20a6eba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86692617-8388-4cf7-8efe-8098ba4e8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out the column names for easier copying for X/y\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c3a68-5e14-47b2-8111-629b87fa245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave out the target variable! (dependent variable)\n",
    "X = df[['Year', 'Mileage', 'City', 'State', 'Make', 'Model']]\n",
    "\n",
    "# have only the target variable here (dependent variable)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a2019-d02e-4249-91e9-f99413e511d7",
   "metadata": {},
   "source": [
    "# Part 4. Inspect the variables if we could remove any of them to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658cb25-45dc-44b8-aed5-eb11f80e7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations are a good starting point\n",
    "# but they often hide connections (because they only measure\n",
    "# linear connections)\n",
    "# for example here, it's implied that Mileage doesn't\n",
    "# correlate with amount paid.... doesn't make sense, does it?\n",
    "correlations = df.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddff4e-6072-4766-9c00-7c0bb88bd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for deciding important variables: SelectKBest -score\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# convert all continuous variables to integer,\n",
    "# and convert all negative numbers to 0\n",
    "X_cat = X.astype(int)\n",
    "X_cat = X_cat.clip(lower=0)\n",
    "\n",
    "# initialize chi2 and SelectKBest\n",
    "# Note: chi2 -test is a very common test\n",
    "# in statistics and quantitative analysis\n",
    "# basically it studies the data whether variables are related\n",
    "# or independent of each other\n",
    "chi_2_features = SelectKBest(chi2, k=len(X_cat.columns))\n",
    "\n",
    "# fit our data to the SelectKBest\n",
    "best_features = chi_2_features.fit(X_cat,y.astype(int))\n",
    "\n",
    "# use decimal format in table print later\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# wrap it up, and show the results\n",
    "# the higher the score, the more effect that column has on price\n",
    "df_features = pd.DataFrame(best_features.scores_)\n",
    "df_columns = pd.DataFrame(X_cat.columns)\n",
    "f_scores = pd.concat([df_columns,df_features],axis=1)\n",
    "f_scores.columns = ['Features','Score']\n",
    "f_scores.sort_values(by='Score',ascending=False)\n",
    "\n",
    "# insights:\n",
    "# Mileage plays a pivotal role and\n",
    "# it goes without saying\n",
    "# the same as Make\n",
    "# but there is an interesting insight regarding to State\n",
    "# it is important because different market demand and supply\n",
    "# also, states impose different taxes\n",
    "# we also need to bear in mind various weather and geography conditions\n",
    "\n",
    "# for some reasons, it shows that the \"Year\" column has zero impact\n",
    "# but the correlation table above shows absolutely different info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eaf7e4-e0c0-4a79-9739-1865d283c9e1",
   "metadata": {},
   "source": [
    "# Part 4 Train/test –split + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a52000-b851-484e-b734-ab7a14745808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, train/test split => 70% for training, 30% for other purposes (temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# now, split the 30% for other purposes by 50% (resulting in 2 x 15%)\n",
    "# so finally, we have:\n",
    "# 70% for training\n",
    "# 15% for testing\n",
    "# 15% for validation\n",
    "# => 70 + 15 +15 = 100%\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c79f4e-a92a-4712-a9f1-2223d9356cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just seeing how much data we have in each\n",
    "print(f\"Train data amount: {len(X_train)}\")\n",
    "print(f\"Test data amount: {len(X_test)}\")\n",
    "print(f\"Validation data amount: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adb1fc-9e4c-4a03-8146-9f4df37f67fd",
   "metadata": {},
   "source": [
    "# Part 5. Creating a neural network structure\n",
    "Almost all layers are Dense-layers and with an output layer of one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb1f17-ad5a-4db4-8ed2-234a9ee0a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this info to a variable so we don't have to change this after\n",
    "# changing the dataset\n",
    "variable_amount = len(X.columns)\n",
    "\n",
    "# needed imports for ModelCheckpoint etc. callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# for EarlyStop/ReduceLROnPlateau, see materials and Moodle\n",
    "# for examples on how to use and when to use (usually more useful with classification)\n",
    "\n",
    "# create a model checkpoint to a file, and only save the best one\n",
    "mc = ModelCheckpoint('best_model_regression1.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# combine all active callbacks into a list\n",
    "# have only those you need, for example only ModelCheckpoint\n",
    "callback_list = [mc]\n",
    "\n",
    "# Define Sequential neural network model\n",
    "# modify the input shape to match your training column count\n",
    "# remember, one of the columns is removed from training columns\n",
    "# to be the target value. so if your data originally had 10 columns\n",
    "# the input shape is 9 ... (10 - 1 => 9)\n",
    "# the input layer itself can have a different number of nodes\n",
    "# Tip: have at least the same number of nodes as in the input shape\n",
    "# output layer in regression is always 1 node without activation function\n",
    "\n",
    "# three most common alternatives for the regularizer\n",
    "# kernel_regularizer=keras.regularizers.l1(l1=0.1)\n",
    "# kernel_regularizer=keras.regularizers.l2(l2=0.1)\n",
    "# kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.1)\n",
    "\n",
    "# typically you can use normalization + regularizer for your model\n",
    "# also try to alter the complexity and size of the neural network\n",
    "# it can greatly affect the performance\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(variable_amount,)),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select the optimizer and loss function\n",
    "# you can try rmsprop also as optimizer, or stochastic gradient descent\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# common tips on how to change neural network structure if your metrics are not good:\n",
    "\n",
    "# make wider (or narrower) layers (for example, 64 or 128 nodes)\n",
    "# make a longer or shorter network (add or remove layers)\n",
    "# use Dropout -layers (e.g. layers.Dropout(0.1))\n",
    "\n",
    "# remember: there's no process or mathematical formula\n",
    "# in order to figure out the optimal neural network structure\n",
    "# it's mostly all about trial and error => EXPERIMENTATION!\n",
    "\n",
    "# remember to have enough \"decision-space\" for your data!\n",
    "# it's highly unlikely a dataset with 20 different variables is going\n",
    "# to work well with only 8 nodes in each layer etc.\n",
    "\n",
    "# print out the summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ceec50-5fc3-4bf6-9e84-96b62b5ad609",
   "metadata": {},
   "source": [
    "# Part 6. Fiting data to the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7638833-47db-47cf-b07c-9b96b38a858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=800, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b6215-7b97-4061-91d1-a3cf685c101a",
   "metadata": {},
   "source": [
    "# Part 6. Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef96b2-f736-4e91-aafb-c6012e512e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loss over epochs \n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()\n",
    "\n",
    "# The training loss quickly drops after first epochs \n",
    "# --> the model is learning\n",
    "\n",
    "# after around 280 epochs there is \n",
    "# a bit of overfitiing\n",
    "# it means I chose too many epochs\n",
    "\n",
    "# they overlapping --> should be good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a0205-f464-4b4f-bfb6-dde5110b64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the final model loss/evaluation values\n",
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))\n",
    "\n",
    "# the model is often good when these error values are similar\n",
    "# mine are almost similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140ea23-6155-45d0-809a-47a229247936",
   "metadata": {},
   "source": [
    "# Part 7.  Make some test predictions to see what kind of mistakes the model makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b8b7-9090-4519-bbcd-3001350a47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df\n",
    "\n",
    "# insights:\n",
    "# we can see that results of predictions\n",
    "# are lower than true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b29848-2430-441b-a200-0fc8e99714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values follow a linear line = good predictions\n",
    "# we basically compare the predicted values \n",
    "# to true test values and see the differences\n",
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)\n",
    "\n",
    "# oh my gosh!! :)\n",
    "# surprise!\n",
    "\n",
    "# maybe becuase I do not have a clear linear regression here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31187443-4897-4095-9925-0791fe4612a9",
   "metadata": {},
   "source": [
    "# Part 7. The inspection of error metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a3121-e395-4f36-b57e-0f2ecab1343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean average error\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"$\")\n",
    "\n",
    "# MSE - Mean square error\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"$^2\")\n",
    "\n",
    "# RMSE - Root mean square error\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"$\")\n",
    "\n",
    "# R-squared. 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "# Explained Variance Score => 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "# high variance score = model is a good fit for the data \n",
    "# low variance score = model is not a good fit for the data\n",
    "# the higher the score, the model is more able to explain the variation in the data\n",
    "# if score is low, we might need more and better data\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))\n",
    "\n",
    "\n",
    "# MAE\n",
    "# Lower is better\n",
    "# on average, the model’s predictions are off by about $4713.47\n",
    "# if we talk about prices that are about 150,000\n",
    "# it is not a huge difference\n",
    "\n",
    "# MSE\n",
    "# Lower is better\n",
    "# probably, because of outliers we still have some MSE \n",
    "\n",
    "# RMSE\n",
    "# Lower is better\n",
    "# the model’s predictions are off by about $5683.16.\n",
    "\n",
    "# R-squared\n",
    "# Closer to 1 is better\n",
    "# it is only 14% but it is not minus as it could be :)\n",
    "\n",
    "# Explained variance score\n",
    "# Closer to 1 is better\n",
    "# A  low score (0.14) further suggests that \n",
    "# the model’s predictions are not closely aligned \n",
    "# with the actual data\n",
    "\n",
    "# To sum up,\n",
    "# it could be worse but there is a room for improvements\n",
    "# probably, there is a problem with the model\n",
    "# I will test in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a16b4e-f628-4aa8-9dac-953baddf3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the prediction distribution are far from normal distribution\n",
    "# then the model is not probably good enough\n",
    "# distplot is deprecating in future pandas-version\n",
    "# unfortunately, there's no exact alternative to do this plot at the moment\n",
    "sns.distplot((y_test - test_predictions))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# here we have a perfect predictions graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1ebf5-24e9-4e79-aafd-917429d4a6ab",
   "metadata": {},
   "source": [
    "# Part 7. Trying the model with some imaginary new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d1000-6986-4e83-b63d-db563813e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e9f34-af57-42f7-b415-7dc18a61a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078be7f8-1855-4350-b5bc-723c8eefadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with some new imaginary data\n",
    "# this example uses the student performance index score dataset\n",
    "# modify this as needed regarding your own dataset\n",
    "tester_row = {\n",
    "    'Year': 2016,\n",
    "    'Mileage': 22140, \n",
    "    'City': 3, \n",
    "    'State': 2,\n",
    "    'Make': 11049, \n",
    "    'Model': 660,\n",
    "}\n",
    "\n",
    "# convert to pandas-format\n",
    "tester_row = pd.DataFrame([tester_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0ac94-ad1b-445b-bdfc-42476b5d81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(tester_row)[0]\n",
    "\n",
    "print()\n",
    "print(f\"Estimated price for this house:\")\n",
    "print(f\"{round(float(result[0]), 2)} $\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec07a0-11c7-4201-8d99-a676481ee5ab",
   "metadata": {},
   "source": [
    "# P.s. Saving the dataset for other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb221b6-6e0a-42e9-9085-e2b766b8c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.s. Saving the dataset for other experiments\n",
    "# save the df to a new CSV file\n",
    "df.to_csv('balanced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeb61-04ba-46e9-8dee-621ea731ebd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
